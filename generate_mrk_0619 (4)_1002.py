# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import re
import io
import json
import time
import html
import datetime
import logging
import sqlite3
import threading
from string import Template
from collections import defaultdict
from dataclasses import dataclass
from functools import lru_cache
from typing import Any, Dict, List, Optional, Set
from urllib.parse import quote_plus, urljoin
import xml.etree.ElementTree as ET

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from pymarc import Record, Field, MARCWriter, Subfield

# Global meta store to avoid NameError
meta_all = {}
OPENAI_CHAT_COMPLETIONS = "https://api.openai.com/v1/chat/completions"
DEFAULT_MODEL = "gpt-4o-mini"

LOGGER_NAME = "isbn2marc"
logger = logging.getLogger(LOGGER_NAME)
if not logger.handlers:
    _handler = logging.StreamHandler()   # Streamlit ì½˜ì†”ì—ë„ ì°íˆì§€ë§Œ, ê¸°ë³¸ì€ WARNING ì´ìƒë§Œ
    _fmt = logging.Formatter("%(levelname)s:%(name)s: %(message)s")
    _handler.setFormatter(_fmt)
    logger.addHandler(_handler)
logger.setLevel(logging.WARNING)  # ê¸°ë³¸ì€ ì¡°ìš©íˆ


# Streamlit ë””ë²„ê·¸ í† ê¸€ (ì—†ìœ¼ë©´ False)
if "debug_mode" not in st.session_state:
    st.session_state["debug_mode"] = False
def _apply_log_level():
    logger.setLevel(logging.DEBUG if st.session_state["debug_mode"] else logging.WARNING)

# === Debug collector ===
CURRENT_DEBUG_LINES: list[str] = []
def dbg(*args):
    """ì¡°ìš©íˆ ë””ë²„ê·¸ ë¼ì¸ì„ ìˆ˜ì§‘ + loggerë¡œë„ ë‚¨ê¹€(ë ˆë²¨=DEBUG)."""
    from datetime import datetime
    msg = " ".join(str(a) for a in args)
    stamp = datetime.now().strftime("%H:%M:%S")
    line = f"[{stamp}] {msg}"
    CURRENT_DEBUG_LINES.append(line)
    logger.debug(msg)

def dbg_err(*args):
    """ì—ëŸ¬ì„± ë¡œê·¸ë„ ìˆ˜ì§‘."""
    from datetime import datetime
    msg = " ".join(str(a) for a in args)
    stamp = datetime.now().strftime("%H:%M:%S")
    line = f"[{stamp}] ERROR: {msg}"
    CURRENT_DEBUG_LINES.append(line)
    logger.debug(msg)



# =========================
# ğŸ”§ HTTP ì„¸ì…˜ (ì¬ì‹œë„/UA/íƒ€ì„ì•„ì›ƒ ê¸°ë³¸ê°’)
# =========================
def _get_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (compatible; isbn2marc/1.0; +https://local)",
        "Accept": "application/json, text/plain, */*",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
    })
    retries = Retry(
        total=4, connect=2, read=3, backoff_factor=0.7,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"], raise_on_status=False
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=10)
    s.mount("https://", adapter)
    s.mount("http://", adapter)
    return s

SESSION = _get_session()

# =========================
# ğŸ” Secrets / Env
# =========================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")
ALADIN_TTB_KEY = os.getenv("ALADIN_TTB_KEY") or st.secrets.get("ALADIN_TTB_KEY", "")
NLK_CERT_KEY   = os.getenv("NLK_CERT_KEY")   or st.secrets.get("NLK_CERT_KEY", "")

# ğŸ” Secrets / Env (í†µí•©)
ALADIN_TTB_KEY = (
    os.getenv("ALADIN_TTB_KEY")
    or st.secrets.get("ALADIN_TTB_KEY")
    or (st.secrets.get("aladin") or {}).get("ttbkey", "")
)

# í˜¸í™˜ìš© ë³„ì¹­(ì—¬ê¸°ì„œ í•œ ë²ˆì— ì •ë¦¬)
aladin_key = ALADIN_TTB_KEY
ALADIN_KEY = ALADIN_TTB_KEY
openai_key = OPENAI_API_KEY
ttbkey     = ALADIN_TTB_KEY
DEFAULT_MODEL = (st.secrets.get("openai", {}) or {}).get("model") or os.getenv("OPENAI_MODEL") or "gpt-4o-mini"
model = DEFAULT_MODEL              # ë³„ì¹­

# ë§¨ ìœ„ ì–´ë”˜ê°€ (OPENAI_API_KEY ì„ ì–¸ ì´í›„)
try:
    from openai import OpenAI
    _client = OpenAI(api_key=OPENAI_API_KEY, timeout=10) if OPENAI_API_KEY else None
except Exception:
    _client = None

# =========================
# 245                      
# =========================

# ì €ìëª…    
INCLUDE_ILLUSTRATOR_AS_AUTHOR = True
USE_WIKIDATA = True
INCLUDE_ORIGINAL_NAME_IN_90010 = True     # ì›ì–´ëª… â†’ 90010ì— ê¸°ë¡
USE_NLK_LOD_AUTH = True                 # NLK LOD ì‚¬ìš©
PREFER_LOD_FIRST = True                 # LOD ë¨¼ì € ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ Wikidata í´ë°±
RECORD_PROVENANCE_META = True           # ì¶œì²˜ ë©”íƒ€ ê¸°ë¡
_KOREAN_ONLY_RX = re.compile(r"^[ê°€-í£\sÂ·\u00B7]$")  # ì™¸êµ­ì¸ ì´ë¦„ íŒì •ìš©(í•œê¸€Â·ì¤‘ì  ì œì™¸)


# ==== Aladin endpoints & HTTP defaults (global) ====
ALADIN_ITEMLOOKUP_URL = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
# ê²€ìƒ‰ í˜ì´ì§€(ìŠ¤í¬ë ˆì´í•‘ ë°±ì—…ìš©): queryì— ISBNì´ë‚˜ ì„œëª… ë„£ì–´ ì‚¬ìš©
ALADIN_SEARCH_URL = "https://www.aladin.co.kr/search/wsearchresult.aspx?SearchTarget=Book&SearchWord={query}"

# ê³µí†µ ìš”ì²­ í—¤ë”(ë´‡ ì°¨ë‹¨ íšŒí”¼ & í•œê¸€ ê²€ìƒ‰ ê²°ê³¼ ì•ˆì •í™”)
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Cache-Control": "no-cache",
}
DEFAULT_TIMEOUT = 10  # seconds





def _has(ch, lo, hi): return lo <= ord(ch) <= hi
def _has_any(s, ranges): return any(any(_has(c,*r) for r in ranges) for c in s)

RANGE_HANGUL  = [(0xAC00,0xD7A3),(0x1100,0x11FF),(0xA960,0xA97F),(0xD7B0,0xD7FF)]
RANGE_CYRIL   = [(0x0400,0x04FF),(0x0500,0x052F)]
RANGE_GREEK   = [(0x0370,0x03FF)]
RANGE_HIRA    = [(0x3040,0x309F)]
RANGE_KATA    = [(0x30A0,0x30FF)]
RANGE_CJK     = [(0x4E00,0x9FFF)]
RANGE_ARABIC  = [(0x0600,0x06FF)]
RANGE_DEVAN   = [(0x0900,0x097F)]
RANGE_LAT_EXT = [(0x00C0,0x024F)]

def _script_rank(s: str) -> int:
    if _has_any(s, RANGE_CYRIL):  return 1
    if _has_any(s, RANGE_HIRA+RANGE_KATA+RANGE_CJK+RANGE_GREEK): return 2
    if _has_any(s, RANGE_ARABIC+RANGE_DEVAN): return 3
    if _has_any(s, RANGE_LAT_EXT): return 4
    if re.search(r"[A-Za-z]", s): return 5
    if _has_any(s, RANGE_HANGUL): return 9
    return 8

def pick_non_hangul_label(labels: list[str]) -> str | None:
    cand = [x.strip() for x in (labels or []) if x and _script_rank(x.strip()) != 9]
    if not cand: return None
    return sorted(cand, key=_script_rank)[0]

SEPS = r"(?:,|Â·|/|ãƒ»|&|\band\b|\bê·¸ë¦¬ê³ \b|\bë°\b)"

ROLE_ALIASES = {
    # author ê³„ì—´
    "ì§€ì€ì´":"author","ì €ì":"author","ê¸€":"author","ê¸€ì“´ì´":"author","ì§‘í•„":"author","ì›ì‘":"author",
    "ì§€ìŒ":"author","ê¸€ì‘ê°€":"author","ìŠ¤í† ë¦¬":"author",
    # translator ê³„ì—´
    "ì˜®ê¸´ì´":"translator","ì˜®ê¹€":"translator","ì—­ì":"translator","ì—­":"translator","ë²ˆì—­":"translator","ì—­ì£¼":"translator","ê³µì—­":"translator",
    # illustrator ê³„ì—´ (ì¶”ì¶œì€ ë³„ë„ë¡œ í•˜ë˜, ë‚˜ì¤‘ì— authorì— í•©ì¹  ì˜ˆì •)
    "ê·¸ë¦¼":"illustrator","ê·¸ë¦°":"illustrator","ì‚½í™”":"illustrator","ì¼ëŸ¬ìŠ¤íŠ¸":"illustrator","ë§Œí™”":"illustrator",
    # editor ë“± (í•„ìš”ì‹œ)
    "ì—®ìŒ":"editor","ì—®ì€ì´":"editor","í¸ì§‘":"editor","í¸":"editor","í¸ì €":"editor","í¸ì§‘ì":"editor",
    # ì˜ë¬¸ í˜¼ì… ëŒ€ë¹„
    "author":"author","writer":"author","story":"author",
    "translator":"translator","trans":"translator","translated":"translator",
    "illustrator":"illustrator","illus.":"illustrator","artist":"illustrator",
    "editor":"editor","ed.":"editor",
}

def normalize_role(token: str) -> str:
    if not token: return "other"
    t = re.sub(r"[()\[\]\s{}]", "", token.strip().lower())
    parts = re.split(r"[Â·/ãƒ»]", t)  # 'ê¸€Â·ê·¸ë¦¼' ê°™ì€ ë³µí•©í‘œê¸°
    cats = {ROLE_ALIASES.get(p, "other") for p in parts if p}
    for pref in ("translator","author","illustrator","editor"):
        if pref in cats: return pref
    return "other"

def strip_tail_role(name: str) -> tuple[str, str]:
    m = re.search(r"\(([^)]+)\)\s*$", name.strip())
    if not m:
        return name.strip(), "other"
    base = name[:m.start()].strip()
    return base, normalize_role(m.group(1))

def split_names(chunk: str) -> list[str]:
    if not chunk: return []
    chunk = re.sub(r"^\s*\([^)]*\)\s*", "", chunk.strip())  # ì•ë¨¸ë¦¬ ê´„í˜¸ ì—­í•  ì œê±°
    parts = re.split(rf"\s*{SEPS}\s*", chunk)
    return [p.strip() for p in parts if p and p.strip()]

def parse_people_flexible(author_str: str) -> dict:
    """
    í•µì‹¬: ì§ì „ ì´ë¦„ ë©ì–´ë¦¬(last_names)ë¥¼ ê¸°ì–µí–ˆë‹¤ê°€,
    ë°”ë¡œ ë‹¤ìŒ í† í°ì´ ì—­í• ì´ë©´ ê·¸ ì´ë¦„ë“¤ì„ ê·¸ ì—­í• ë¡œ 'ì¬í• ë‹¹'í•œë‹¤.
    (ì˜ˆ: 'ê¹€ì—°ê²½ (ì˜®ê¸´ì´)'ê°€ splitë˜ì–´ 'ê¹€ì—°ê²½' ê³¼ '(ì˜®ê¸´ì´)'ë¡œ ë–¨ì–´ì§€ëŠ” ê²½ìš° ì»¤ë²„)
    """
    out = defaultdict(list)
    if not author_str:
        return out

    role_pattern = r"(\([^)]*\)|ì§€ì€ì´|ì €ì|ê¸€|ê¸€ì“´ì´|ì§‘í•„|ì›ì‘|ì—®ìŒ|ì—®ì€ì´|ì§€ìŒ|ê¸€ì‘ê°€|ìŠ¤í† ë¦¬|ì˜®ê¸´ì´|ì˜®ê¹€|ì—­ì|ì—­|ë²ˆì—­|ì—­ì£¼|ê³µì—­|ê·¸ë¦¼|ê·¸ë¦°|ì‚½í™”|ì¼ëŸ¬ìŠ¤íŠ¸|ë§Œí™”|í¸ì§‘|í¸|í¸ì €|í¸ì§‘ì|author|writer|story|translator|trans|translated|editor|ed\.|illustrator|illus\.|artist)"
    tokens = [t.strip() for t in re.split(role_pattern, author_str) if t and t.strip()]

    current = "other"
    pending = []            # ì—­í•  ì—†ëŠ” ì´ë¦„ ëŒ€ê¸°(ì•ì— ì´ë¦„, ë’¤ì— ì—­í•  ë‚˜ì˜¤ëŠ” ì¼€ì´ìŠ¤)
    last_names = []         # ë°©ê¸ˆ ì²˜ë¦¬í•œ ì´ë¦„ë“¤
    last_assigned_to = None # last_namesë¥¼ ì–´ë””ì— ë„£ì—ˆëŠ”ì§€ ê¸°ì–µ

    def _assign(lst, cat):
        for x in lst:
            out[cat].append(x)

    for tok in tokens:
        role_cat = normalize_role(tok)
        if role_cat != "other":
            # 1) ì•ì—ì„œ ì´ë¦„ë§Œ ë‚˜ì˜¤ê³  ì•„ì§ ì—­í• ì´ ì—†ì—ˆë‹¤ë©´ â†’ ì´ë²ˆ ì—­í• ë¡œ ë°°ì •
            if pending:
                _assign(pending, role_cat)
                pending.clear()
                last_names = []  # pendingì€ ê³¼ê±° ë©ì–´ë¦¬ì´ë¯€ë¡œ last_names ì´ˆê¸°í™”
                last_assigned_to = None
            else:
                # 2) ë°”ë¡œ ì§ì „ì— ì´ë¦„ì„ 'í˜„ì¬ current'ë¡œ ë„£ì–´ë‘” ìƒíƒœì—ì„œ
                #    ì´ë²ˆ í† í°ì´ '(ì˜®ê¸´ì´)' ê°™ì€ 'ë’¤ê¼¬ë¦¬ ì—­í• 'ì´ë©´ â†’ ì¬í• ë‹¹
                if last_names and last_assigned_to:
                    # ê¸°ì¡´ ë°°ì •ì—ì„œ ì œê±°
                    for x in last_names:
                        try:
                            out[last_assigned_to].remove(x)
                        except ValueError:
                            pass
                    # ìƒˆ ì—­í• ë¡œ ë°°ì •
                    _assign(last_names, role_cat)
                    # í´ë¦¬ì–´
                    last_names = []
                    last_assigned_to = None

            current = role_cat
            continue

        # ì´ë¦„ ë©ì–´ë¦¬ ì²˜ë¦¬
        names = split_names(tok)
        if not names:
            continue

        # ê° ì´ë¦„ ë‹¨ìœ„ë¡œ 'í™ê¸¸ë™ (ì—­)' ê°™ì€ ë’¤ê¼¬ë¦¬ ê¼¬ë¦¬í‘œê°€ ì§ì ‘ ë¶™ì–´ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ ìš°ì„  ë°°ì •
        direct = []
        for raw in names:
            base, tail = strip_tail_role(raw)
            if tail != "other":
                out[tail].append(base)
                direct.append(base)

        # directë¡œ ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒ ì œì™¸
        remain = [n for n in names if n not in direct]
        if not remain:
            last_names = direct
            last_assigned_to = None
            continue

        if current != "other":
            _assign(remain, current)
            last_names = remain[:]      # ë°©ê¸ˆ ë„£ì€ ê±¸ ê¸°ì–µ (ë‹¤ìŒ í† í°ì´ ì—­í• ì´ë©´ ì¬í• ë‹¹)
            last_assigned_to = current
        else:
            # ì•„ì§ ì—­í• ì´ ì—†ìœ¼ë©´ ë³´ë¥˜ â†’ ë‹¤ìŒ ì—­í•  í† í°ì— ë°°ì •
            pending.extend(remain)
            last_names = remain[:]      # ì§í›„ ì—­í•  í† í°ì´ ì˜¤ë©´ ì´ë“¤ì„ ê·¸ ì—­í• ë¡œ ë°°ì •
            last_assigned_to = None

    # ë£¨í”„ ì¢…ë£Œ í›„ì—ë„ pendingì´ ë‚¨ì•˜ìœ¼ë©´ ì•ˆì „í•˜ê²Œ authorë¡œ
    if pending:
        _assign(pending, "author")

    # ì¤‘ë³µ ì œê±°(ì—­í• ë³„)
    for k, arr in out.items():
        seen = set(); uniq=[]
        for x in arr:
            if x not in seen:
                seen.add(x); uniq.append(x)
        out[k] = uniq

    return out

def _dedup(seq):
    seen=set(); out=[]
    for x in seq:
        if x not in seen:
            seen.add(x); out.append(x)
    return out

def merge_illustrators_into_authors(people: dict, include=True) -> dict:
    if not include: 
        return people
    people["author"] = _dedup(people.get("author", []) + people.get("illustrator", []))
    return people

def extract_people_from_aladin(item: dict) -> dict:
    res = {"author":[], "translator":[], "illustrator":[], "editor":[], "other":[]}
    if not item:
        return res

    sub = (item.get("subInfo") or {})
    arr = sub.get("authors")
    if isinstance(arr, list) and arr:
        for a in arr:
            name = (a.get("authorName") or a.get("name") or "").strip()
            typ  = (a.get("authorTypeName") or a.get("authorType") or "").strip()
            if not name:
                continue
            base, tail = strip_tail_role(name)  # ì´ë¦„ ê¼¬ë¦¬í‘œ ìš°ì„ 
            cat = normalize_role(typ)
            if tail != "other":
                cat = tail
            res.setdefault(cat, []).append(base)
        for k in list(res.keys()):
            res[k] = _dedup(res[k])
    else:
        parsed = parse_people_flexible(item.get("author") or "")
        for k in res:
            res[k] = parsed.get(k, [])

    # âœ… ê·¸ë¦¼(illustrator)ì„ authorì— í•©ì¹˜ê¸° (ì±…ì„í‘œì‹œì—” ì‚¬ìš© ì•ˆ í•¨)
    return merge_illustrators_into_authors(res, INCLUDE_ILLUSTRATOR_AS_AUTHOR)

def build_700_from_people(people: dict, reorder_fn=None, aladin_item=None) -> list[str]:
    seq = people.get("author", []) + people.get("translator", [])
    lines = []
    for nm in seq:
        fixed = reorder_fn(nm, aladin_item=aladin_item) if reorder_fn else nm
        lines.append(f"=700  1\\$a{fixed}")
    return lines


# === [PATCH] JSON ì§ë ¬í™” í—¬í¼ ì¶”ê°€ ===
def _jsonify(obj):
    """dict/list/set ì•ˆì— setì´ ì„ì—¬ ìˆì–´ë„ JSONìœ¼ë¡œ ì €ì¥ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
    if isinstance(obj, set):
        return sorted(obj)
    if isinstance(obj, dict):
        return {k: _jsonify(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_jsonify(v) for v in obj]
    return obj

def _ensure_name_bundle(d):
    if d is None:
        return {"native": set(), "roman": set(), "countries": set()}
    return {
        "native": set(d.get("native", [])),
        "roman": set(d.get("roman", [])),
        "countries": set(d.get("countries", [])),
    }




# CSV ë¡œë“œ
def load_uploaded_csv(uploaded):
    import io
    content = uploaded.getvalue()
    last_err = None
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            text = content.decode(enc)
            return pd.read_csv(io.StringIO(text), engine="python", sep=None, dtype=str)
        except Exception as e:
            last_err = e
    raise RuntimeError(f"CSV ì¸ì½”ë”©/íŒŒì‹± ì‹¤íŒ¨: {last_err}")


# ì™¸êµ­ì¸ ì´ë¦„
_HANGUL_RE = re.compile(r"[ê°€-í£]")

# í•œê¸€ë¡œ ì íŒ 'ì„œì–‘ì‹' ì´ë¦„ì˜ í”í•œ ì²«ì´ë¦„(ìŒì—­) ëª©ë¡
# í•„ìš”í•˜ë©´ ì ì  ë³´íƒœê°€ë©´ ë¨
_WESTERN_GIVEN_KO = (
    "ë§ˆì´í´","ì¡°ì§€","ì œì„ìŠ¤","ì¡´","ë°ì´ë¹„ë“œ","ìŠ¤í‹°ë¸","ìŠ¤í‹°ë¸Œ","ì—ë¦­","ì—ë“œì›Œë“œ",
    "ë¦¬ì²˜ë“œ","ë¡œë²„íŠ¸","ì°°ìŠ¤","ìœŒë¦¬ì—„","ë²¤ìë¯¼","ê°€ë¸Œë¦¬ì—˜","ì¡°ìŠˆì•„","ì•Œë ‰ì‚°ë”",
    "í¬ë¦¬ìŠ¤í† í¼","í¬ë¦¬ìŠ¤ì²œ","ëŒ€ë‹ˆì–¼","ë„ë„ë“œ","ë”ê¸€ëŸ¬ìŠ¤","í”„ë­í¬","í—¨ë¦¬","ì­",
    "ì œì´ìŠ¨","ì œí”„ë¦¬","ì¡°ì…‰","ì¼€ë„¤ìŠ¤","ë˜ë¦¬","ë§ˆí¬","ë§¤íŠœ","ë‹ˆì½œë¼ìŠ¤","í´",
    "í”¼í„°","ì‚¬ë¬´ì—˜","ìŠ¤ì½§","í† ë¨¸ìŠ¤","ì•¤ë“œë¥˜","ì•ˆí† ë‹ˆì˜¤","ì¹´ë¥¼","í”¼ì—ë¥´","ì¥",
    "í”„ë‘ìˆ˜ì•„","ê°€ë¥´ì‹œì•„","ë² ë¥´ë‚˜ë¥´","ê¸°ìš¤","ê°€ë¸Œë¦¬ì—˜"
)

def _looks_western_korean_translit(name: str) -> bool:
    """í•œê¸€ í‘œê¸°ì§€ë§Œ ì„œì–‘ì‹ ê°œì¸ì´ë¦„(ìŒì—­) ê°™ì€ì§€ ê°„ë‹¨ ì¶”ì •"""
    parts = [p for p in name.strip().split() if p]
    if not parts:
        return False
    first = parts[0]
    return first in _WESTERN_GIVEN_KO

def _summarize_name_context_from_aladin(item: dict | None) -> str:
    if not item:
        return ""
    sub  = (item.get("subInfo") or {})
    seri = (item.get("seriesInfo") or {})
    pieces = []
    if (sub.get("originalTitle") or "").strip():
        pieces.append(f"originalTitle={(sub.get('originalTitle') or '').strip()}")
    if (item.get("categoryName") or "").strip():
        pieces.append(f"categoryName={(item.get('categoryName') or '').strip()}")
    if (item.get("publisher") or "").strip():
        pieces.append(f"publisher={(item.get('publisher') or '').strip()}")
    if (item.get("pubDate") or "").strip():
        pieces.append(f"pubDate={(item.get('pubDate') or '').strip()}")
    if (seri.get("seriesName") or "").strip():
        pieces.append(f"seriesName={(seri.get('seriesName') or '').strip()}")
    return " | ".join(pieces)



# =========================
# ğŸ§  OpenAI (ì•„ì‹œì•„ê¶Œ KEEP / ë¹„ì•„ì‹œì•„ê¶Œ 'ì„±, ì´ë¦„')
# =========================

LLM_SCHEMA = {
    "type": "json_schema",
    "json_schema": {
        "name": "NameOrderDecision",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
                "action":  {"type": "string", "enum": ["REORDER", "KEEP"]},
                "result":  {"type": "string"},
                "reason":  {"type": "string"},
                "confidence": {"type": "number"}
            },
            "required": ["action", "result"]
        }
    }
}

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ í•œêµ­ ë„ì„œê´€ KORMARC 700 í•„ë“œìš© ì´ë¦„ ì •ë ¬ ë³´ì¡°ìì…ë‹ˆë‹¤.\n"
    "ì…ë ¥ì€ 'í•œê¸€ í‘œê¸°' ì €ìëª…ê³¼ ì•Œë¼ë”˜/ìœ„í‚¤ë°ì´í„° ë©”íƒ€ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n"
    "ì„ë¬´: ì´ë¦„ì˜ ì„±Â·ì´ë¦„ ìˆœì„œë¥¼ íŒë³„í•˜ê³ , í•„ìš” ì‹œ 'ì„±, ì´ë¦„'ìœ¼ë¡œ ì¬ë°°ì—´í•˜ì—¬ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤.\n"
    "\n"
    "[ê°€ì •/ê·¼ê±° ì‹ í˜¸]\n"
    "- wikidata_country: ìœ„í‚¤ë°ì´í„° P27(ì‹œë¯¼ê¶Œ/êµ­ì )\n"
    "- wikidata_labels: ë‹¤êµ­ì–´ ë¼ë²¨(en/ja/zh/ru ë“±)\n"
    "- originalTitle: ì›ì„œëª…(ë¡œë§ˆì)\n"
    "- categoryName: ì£¼ì œ/ì§€ì—­ íŒíŠ¸(ì˜ˆ: 'ì˜ë¯¸', 'í”„ë‘ìŠ¤ ë¬¸í•™')\n"
    "\n"
    "[íŒë³„ ìš°ì„ ìˆœìœ„]\n"
    "1) í•œê¸€ í‘œê¸° ì´ë¦„ì´ ì„±â€“ì´ë¦„ ê´€ìŠµì¸ ì–¸ì–´ê¶Œ(í•œêµ­/ì¤‘êµ­/ì¼ë³¸ ë“±)ìœ¼ë¡œ ëª…ë°±í•˜ë©´ KEEP.\n"
    "2) ê·¸ ì™¸ì—ëŠ” wikidata_country/labels/originalTitle/categoryNameë¥¼ ê·¼ê±°ë¡œ ì¼ë°˜ì  ê´€ìŠµì„ ì¶”ì •:\n"
    "   - ë‹¤ìˆ˜ ìœ ëŸ½/ë¯¸ì£¼ê¶Œ: ê¸°ë³¸ ì´ë¦„â€“ì„± â†’ 'ì„±, ì´ë¦„'ìœ¼ë¡œ REORDER.\n"
    "   - ëŸ¬ì‹œì•„/ë™ìœ ëŸ½ê¶Œ: ì´ë¦„â€“ì„± ì œê³µì´ í”í•¨ â†’ REORDER.\n"
    "3) ë‹¨ì¼ ì´ë¦„(ëª¨ë…¸ë‹˜)ì€ KEEP.\n"
    "\n"
    "[ì˜ˆì™¸/ì„¸ë¶€ ê·œì¹™]\n"
    "- ìŠ¤í˜ì¸/í¬ë¥´íˆ¬ê°ˆ ë³µì„±(de, da, del, de la, dos, y ë“±) â†’ ì„± ì„±, ì´ë¦„ ìœ ì§€(ì˜ˆ: 'ê°€ë¥´ì‹œì•„ ë§ˆë¥´ì¼€ìŠ¤, ê°€ë¸Œë¦¬ì—˜').\n"
    "- ë„¤ëœë€ë“œ ì ‘ë‘ì‚¬(van, van der, de ë“±)ëŠ” ì„±ì˜ ì¼ë¶€ë¡œ ì²˜ë¦¬(ì˜ˆ: 'ë°˜ ê³ í, ë¹ˆì„¼íŠ¸').\n"
    "- í•˜ì´í”ˆ ì„±/ì´ë¦„ì€ í†µì§¸ë¡œ ìœ ì§€(ì˜ˆ: 'ì¥-í´').\n"
    "- ëŸ¬ì‹œì•„ì‹ ë¶€ì¹­(-ë¹„ì¹˜/-ë¸Œë‚˜/-ì˜¤ë¹„ì¹˜ ë“±)ì€ ì´ë¦„ ë’¤ì— ë‘ê³ , ì„±ì„ ì•ìœ¼ë¡œ(ì˜ˆ: 'ë„ìŠ¤í† ì˜™ìŠ¤í‚¤, í‘œë„ë¥´').\n"
    "- ë² íŠ¸ë‚¨ì‹ì€ í†µìƒ ì„±â€“ì´ë¦„ì´ë¯€ë¡œ KEEP.\n"
    "- ì¸ë¬¼ì´ ë‹¨ì²´/ê¸°ê´€ìœ¼ë¡œ ë³´ì´ë©´ KEEP.\n"
    "\n"
    "[ì¶œë ¥ í˜•ì‹]\n"
    "JSON í•œ ì¤„ë§Œ:\n"
    "{\"action\":\"KEEP|REORDER\",\"result\":\"<ìµœì¢… í‘œê¸°>\",\"reason\":\"<ê·¼ê±°>\",\"confidence\":0.0~1.0}\n"
    "â€» REORDER ì‹œ resultëŠ” ë°˜ë“œì‹œ 'ì„±, ì´ë¦„'ì´ì–´ì•¼ í•¨. ê·¼ê±°ì—ëŠ” ì‚¬ìš© ì‹ í˜¸(country/labels ë“±) ê¸°ì¬.\n"
)



def _is_mononym(h: str) -> bool:
    parts = [p for p in re.split(r"\s+", (h or "").strip()) if p]
    return len(parts) <= 1

@lru_cache(maxsize=4096)
def decide_name_order_via_llm(hangul_name: str, ctx_key: str = "") -> dict:
    """
    hangul_name: 'ì•¤ ë˜ë“œí´ë¦¬í”„' ê°™ì€ í•œê¸€ í‘œê¸°
    ctx_key: ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ë¬¸ìì—´(_summarize_name_context_from_aladin(...) ê²°ê³¼)
    """
    name = (hangul_name or "").strip()
    if not name:
        return {"action":"KEEP","result":"","reason":"empty","confidence":0.0}

    # ëª¨ë…¸ë‹˜ì€ ë°”ë¡œ KEEP
    if len(name.split()) <= 1:
        return {"action":"KEEP","result":name,"reason":"mononym","confidence":0.9}

    # API ì—†ìœ¼ë©´ ê°„ë‹¨ í´ë°±(2ì–´ì ˆë§Œ ë’¤ì§‘ê¸°)
    if not _client or not OPENAI_API_KEY:
        parts = name.split()
        if len(parts) == 2 and _HANGUL_RE.search(name):
            first, last = parts[0], parts[1]
            return {"action":"REORDER","result":f"{last}, {first}","reason":"fallback-no-client","confidence":0.4}
        return {"action":"KEEP","result":name,"reason":"fallback-keep","confidence":0.4}

    try:
        user_msg = f'ì´ë¦„: "{name}"\nì»¨í…ìŠ¤íŠ¸: {ctx_key}'
        resp = _client.responses.create(
            model="gpt-4o-mini",
            instructions=SYSTEM_PROMPT,
            input=user_msg,
            response_format=LLM_SCHEMA,
            temperature=0
        )
        data = json.loads(resp.output_text)
        action = data.get("action","KEEP")
        result = (data.get("result") or name).strip()
        if action == "REORDER" and "," not in result and _HANGUL_RE.search(name):
            parts = name.split()
            if len(parts) == 2:
                first, last = parts[0], parts[1]
                result = f"{last}, {first}"
        return {"action": action, "result": result,
                "reason": data.get("reason",""), "confidence": data.get("confidence",0.75)}
    except Exception as e:
        parts = name.split()
        if len(parts) == 2 and _HANGUL_RE.search(name):
            first, last = parts[0], parts[1]
            return {"action":"REORDER","result":f"{last}, {first}","reason":f"fallback:{e}","confidence":0.4}
        return {"action":"KEEP","result":name,"reason":f"fallback-keep:{e}","confidence":0.4}

def reorder_hangul_name_for_700(name: str, *, aladin_item: dict | None = None) -> str:
    """
    ê°€ëŠ¥í•œ í•œ LLMì´ ì•Œë¼ë”˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  íŒë‹¨.
    LLM ë¶ˆê°€/ì˜¤ë¥˜ ëŒ€ë¹„ í´ë°±ì€ decide_name_order_via_llm ë‚´ë¶€ì—ì„œ ìˆ˜í–‰.
    """
    s = (name or "").strip()
    if not s:
        return s
    ctx = _summarize_name_context_from_aladin(aladin_item)
    return decide_name_order_via_llm(s, ctx_key=ctx)["result"]

def get_anycase(rec: dict, key: str):
    if not rec:
        return None
    key_norm = key.strip().upper()
    for k, v in rec.items():
        if (k or "").strip().upper() == key_norm:
            return v
    return None

# === NLK LOD (SPARQL) ===
_NLK_Lod_Endpoints = ["https://lod.nl.go.kr/sparql", "http://lod.nl.go.kr/sparql"]
_NLK_HEADERS = {
    "Accept": "application/sparql-results+json",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "User-Agent": "isbn2marc/1.0 (+local)"
}

def _nlk_sparql(query: str, timeout=(10, 60), retries=2, backoff=1.6):
    import time, requests
    last = None
    for ep in _NLK_Lod_Endpoints:
        for i in range(retries):
            try:
                r = SESSION.post(ep, data={"query": query, "format": "json"},
                                 headers=_NLK_HEADERS, timeout=timeout)
                r.raise_for_status()
                return ep, r.json()
            except Exception as e:
                last = (ep, e)
                if i < retries - 1:
                    time.sleep(backoff**(i+1))
                else:
                    break
    raise RuntimeError(f"NLK LOD ì‹¤íŒ¨: {last[0]} :: {repr(last[1])}")

def _lod_search_persons_by_name_ko(name_ko: str, limit: int = 10):
    # í•œêµ­ì–´ ì´ë¦„(ë¶€ë¶„ì¼ì¹˜)ìœ¼ë¡œ nlon:Author í›„ë³´ë¥¼ ì°¾ìŒ
    safe = name_ko.replace('"', '\\"').strip()
    q = f"""
PREFIX nlon: <http://lod.nl.go.kr/ontology/>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?person ?name WHERE {{
  ?person a nlon:Author ; foaf:name ?name .
  FILTER(LANG(?name)="ko")
  FILTER(REGEX(STR(?name), "{safe}", "i"))
}}
LIMIT {limit}
"""
    ep, data = _nlk_sparql(q)
    rows = data.get("results", {}).get("bindings", [])
    return ep, [{"person": r["person"]["value"], "name": r["name"]["value"]} for r in rows]

def _lod_get_all_names(person_uri: str):
    q = f"""
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name (LANG(?name) AS ?lang) WHERE {{
  <{person_uri}> foaf:name ?name .
}}
"""
    ep, data = _nlk_sparql(q)
    out = []
    for b in data.get("results", {}).get("bindings", []):
        out.append({"name": b["name"]["value"], "lang": b.get("lang", {}).get("value", "")})
    return ep, out

def get_original_name_via_lod(name_ko: str):
    """
    í•œêµ­ì–´ í‘œê¸° 'í™ê¸¸ë™' â†’ NLK LODë¡œ í›„ë³´ URI ì°¾ê³  â†’ ë¹„í•œê¸€ ì´ë¦„ 1ê°œ ì„ íƒ.
    ë°˜í™˜: (ì›ì–´ëª… ë˜ëŠ” None, provenance_meta)
    """
    if not (USE_NLK_LOD_AUTH and name_ko.strip()):
        return None, None
    try:
        ep1, cands = _lod_search_persons_by_name_ko(name_ko, limit=10)
        if not cands:
            return None, {"source":"NLK LOD", "endpoint": ep1, "reason":"no candidates", "name_ko": name_ko}
        # ì²« í›„ë³´ë¡œ ìƒì„¸ ë¼ë²¨ ì¡°íšŒ
        chosen = cands[0]
        ep2, names = _lod_get_all_names(chosen["person"])
        # ë¹„í•œê¸€ ë¼ë²¨ í•˜ë‚˜ ê³ ë¥´ê¸° (í•¨ìˆ˜ pick_non_hangul_label ì¬ì‚¬ìš©)
        labels = [n["name"] for n in names]
        best = pick_non_hangul_label(labels)
        prov = {
            "source": "NLK LOD",
            "endpoint_search": ep1,
            "endpoint_fetch": ep2,
            "person_uri": chosen["person"],
            "matched_name_ko": chosen["name"],
            "candidates": cands[:3],
            "names_sample": names[:8]
        }
        return best, prov
    except Exception as e:
        return None, {"source":"NLK LOD", "error":repr(e), "name_ko":name_ko}


WD_SPARQL = "https://query.wikidata.org/sparql"

def get_original_name_via_wikidata(name_hint: str) -> str | None:
    """
    í•œê¸€ í‘œê¸°(ì˜ˆ: 'í‘œë„ë¥´ ë„ìŠ¤í† ì˜™ìŠ¤í‚¤')ë¥¼ ë°›ì•„ Wikidataì—ì„œ
    ë¹„í•œê¸€(ì›ì–´) ë¼ë²¨ì„ í•˜ë‚˜ ê³¨ë¼ ë°˜í™˜. ì‹¤íŒ¨ ì‹œ None.
    """
    import re as _re
    name_hint = (name_hint or "").strip()
    if not name_hint:
        return None

    ck = f"wd-orig:{name_hint}"
    try:
        c = cache_get(ck)
        if isinstance(c, dict) and "orig" in c:
            return c["orig"]
    except Exception:
        pass

    qvars = [name_hint]
    if "ì˜™" in name_hint:
        qvars.append(name_hint.replace("ì˜™", "ì˜ˆ"))
    if "ì˜ˆí”„" in name_hint:
        qvars.append(name_hint.replace("ì˜ˆí”„", "ì˜™"))
    qvars.append(_re.sub(r"\s+", "", name_hint))

    hits = []
    for q in qvars:
        try:
            res = wikidata_search_ko(q, limit=10) or []
        except Exception:
            res = []
        hits.extend(res)
        if res:
            break

    if not hits:
        try:
            cache_set(ck, {"orig": None})
        except Exception:
            pass
        return None

    best = hits[0]
    labels = []
    if best.get("native"):
        labels.append(best["native"])
    if best.get("label_ru"):
        labels.append(best["label_ru"])
    if best.get("label_en"):
        labels.append(best["label_en"])

    orig = pick_non_hangul_label(labels)

    try:
        cache_set(ck, {"orig": orig})
    except Exception:
        pass
    return orig

def build_90010_from_wikidata(people: dict, include_translator: bool = True) -> list[str]:
    """
    Wrapper: prefer NLK LOD first, then Wikidata; returns 90010 lines only.
    """
    lines, _prov = build_90010_prefer_lod_then_wikidata_with_meta(people, include_translator=include_translator)
    return lines

def _nlk_sparql(query: str, timeout=(10, 60), retries: int = 2, backoff: float = 1.6):
    import time
    last = None
    for ep in _NLK_LOD_ENDPOINTS:
        for i in range(retries):
            try:
                r = SESSION.post(ep, data={"query": query, "format": "json"}, headers=_NLK_HEADERS, timeout=timeout)
                r.raise_for_status()
                return ep, r.json()
            except Exception as e:
                last = (ep, e)
                if i < retries - 1:
                    time.sleep(backoff ** (i + 1))
                else:
                    break
    raise RuntimeError(f"NLK LOD ì‹¤íŒ¨: {last[0]} :: {repr(last[1])}")

def _lod_search_persons_by_name_ko(name_ko: str, limit: int = 10):
    safe = (name_ko or "").replace('"', '\\"').strip()
    q = f"""
PREFIX nlon: <http://lod.nl.go.kr/ontology/>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?person ?name WHERE {{
  ?person a nlon:Author ; foaf:name ?name .
  FILTER(LANG(?name) = "ko")
  FILTER(REGEX(STR(?name), "{safe}", "i"))
}}
LIMIT {limit}
"""
    ep, data = _nlk_sparql(q)
    rows = data.get("results", {}).get("bindings", [])
    return ep, [{"person": r["person"]["value"], "name": r["name"]["value"]} for r in rows]

def _lod_get_all_names(person_uri: str):
    q = f"""
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name (LANG(?name) AS ?lang) WHERE {{
  <{person_uri}> foaf:name ?name .
}}
"""
    ep, data = _nlk_sparql(q)
    out = []
    for b in data.get("results", {}).get("bindings", []):
        out.append({"name": b["name"]["value"], "lang": b.get("lang", {}).get("value", "")})
    return ep, out

def get_original_name_via_lod(name_ko: str):
    try:
        ep1, cands = _lod_search_persons_by_name_ko(name_ko, limit=10)
        if not cands:
            return None, {"source":"NLK LOD", "endpoint": ep1, "reason":"no candidates", "name_ko": name_ko}
        chosen = cands[0]
        ep2, names = _lod_get_all_names(chosen["person"])
        labels = [n["name"] for n in names]
        best = pick_non_hangul_label(labels)
        prov = {
            "source": "NLK LOD",
            "endpoint_search": ep1,
            "endpoint_fetch": ep2,
            "person_uri": chosen["person"],
            "matched_name_ko": chosen["name"],
            "candidates": cands[:3],
            "names_sample": names[:8]
        }
        return best, prov
    except Exception as e:
        return None, {"source":"NLK LOD", "error":repr(e), "name_ko":name_ko}

_WD_API = "https://www.wikidata.org/w/api.php"
_WD_UA = {"User-Agent": "MARC-Auto/0.1 (edu; test)"}
_KO_WIKI_API = "https://ko.wikipedia.org/w/api.php"

def _get_qid_via_kowiki(title_ko: str):
    """ko.wikipediaì—ì„œ titleë¡œ wikibase_item(QID) ì–»ê¸°"""
    try:
        r = SESSION.get(_KO_WIKI_API, headers=_WD_UA, params={
            "action":"query","titles":title_ko,"prop":"pageprops","ppprop":"wikibase_item","format":"json"
        }, timeout=(10,30))
        r.raise_for_status()
        data = r.json().get("query", {}).get("pages", {})
        for _, page in data.items():
            qid = page.get("pageprops", {}).get("wikibase_item")
            if qid:
                return qid
        return None
    except Exception:
        return None

def _wd_search_qid_ko(name: str, limit=10):
    try:
        r = SESSION.get(_WD_API, headers=_WD_UA, params={
            "action":"wbsearchentities","search":name,"language":"ko","uselang":"ko",
            "type":"item","limit":limit,"format":"json"
        }, timeout=(10,30))
        r.raise_for_status()
        arr = r.json().get("search", [])
        return arr[0]["id"] if arr else None
    except Exception:
        return None

def _wd_get_labels(qid: str, langs=("ru","en","ja","zh","ko")):
    try:
        r = SESSION.get(_WD_API, headers=_WD_UA, params={
            "action":"wbgetentities","ids":qid,"props":"labels|aliases",
            "languages":"|".join(langs),"format":"json"
        }, timeout=(10,30))
        r.raise_for_status()
        ent = r.json().get("entities", {}).get(qid, {})
        return ent.get("labels", {}), ent.get("aliases", {})
    except Exception:
        return {}, {}

def _simple_reorder_family_given(label: str):
    parts = (label or "").strip().split()
    if len(parts) == 2:
        return f"{parts[1]}, {parts[0]}"
    return label

_WD_API = "https://www.wikidata.org/w/api.php"
_WD_UA = {"User-Agent": "MARC-Auto/0.2 (edu; streamlit)"}
_KO_WIKI_API = "https://ko.wikipedia.org/w/api.php"

_WD_COUNTRY_TO_LANG = {
    "Q17": "ja",  # Japan
    "Q148": "zh", # China
    "Q159": "ru", # Russia
    "Q142": "fr", # France
    "Q183": "de", # Germany
    "Q29": "es",  # Spain
    "Q38": "it",  # Italy
    "Q145": "en", # UK
    "Q30": "en",  # USA
}
_DEFAULT_LANGS = ["ja","zh","ru","en","ko"]
_KOREAN_P27_QIDS = {"Q884","Q423","Q180"}
_EAST_ASIAN_P27 = {"Q17","Q148","Q884","Q423","Q865","Q864","Q14773"}

def _wd_get_p27_list(qid: str) -> list[str]:
    if not qid:
        return []
    try:
        r = SESSION.get(_WD_API, headers=_WD_UA, params={
            "action":"wbgetentities","ids":qid,"props":"claims","format":"json"
        }, timeout=(10,30))
        r.raise_for_status()
        ent = r.json().get("entities", {}).get(qid, {})
        out = []
        for stmt in ent.get("claims", {}).get("P27", []):
            try:
                out.append(stmt["mainsnak"]["datavalue"]["value"]["id"])
            except Exception:
                pass
        return out
    except Exception:
        return []

def _wd_is_korean_national(qid: str) -> bool:
    return any(c in _KOREAN_P27_QIDS for c in _wd_get_p27_list(qid))

def _wd_preferred_langs_for_qid(qid: str) -> list[str]:
    prefs = []
    for c in _wd_get_p27_list(qid):
        lang = _WD_COUNTRY_TO_LANG.get(c)
        if lang and lang not in prefs:
            prefs.append(lang)
    for x in _DEFAULT_LANGS:
        if x not in prefs:
            prefs.append(x)
    return prefs

def _wd_get_labels(qid: str, langs: tuple[str, ...] = ("ja","zh","ru","en","ko")):
    """ë¼ë²¨/ë³„ì¹­ ì¡°íšŒ (ì–¸ì–´ ìš°ì„ ìˆœìœ„ ì§€ì • ê°€ëŠ¥)"""
    try:
        r = SESSION.get(_WD_API, headers=_WD_UA, params={
            "action":"wbgetentities","ids":qid,"props":"labels|aliases",
            "languages":"|".join(langs),"format":"json"
        }, timeout=(10,30))
        r.raise_for_status()
        ent = r.json().get("entities", {}).get(qid, {})
        return ent.get("labels", {}), ent.get("aliases", {})
    except Exception:
        return {}, {}

def _get_qid_via_kowiki(title_ko: str):
    """ko.wikipediaì—ì„œ titleë¡œ wikibase_item(QID) ì–»ê¸°"""
    try:
        r = SESSION.get(_KO_WIKI_API, headers=_WD_UA, params={
            "action":"query","titles":title_ko,"prop":"pageprops","ppprop":"wikibase_item","format":"json"
        }, timeout=(10,30))
        r.raise_for_status()
        data = r.json().get("query", {}).get("pages", {})
        for _, page in data.items():
            qid = page.get("pageprops", {}).get("wikibase_item")
            if qid:
                return qid
        return None
    except Exception:
        return None

def get_original_name_via_wikidata_rest(name_ko: str):
    qid = _wd_search_qid_ko(name_ko)
    if not qid:
        qid = _get_qid_via_kowiki(name_ko)
        if not qid:
            return None, {"source":"Wikidata(REST)", "reason":"no qid", "name_ko":name_ko}
        pref_langs = tuple(_wd_preferred_langs_for_qid(qid))
        labels, _ = _wd_get_labels(qid, langs=pref_langs)
        for lang in pref_langs:
            if lang in labels:
                val = labels[lang]["value"]
                if lang in ("en",) and " " in val.strip():
                    val = _simple_reorder_family_given(val)
                return val, {"source":"Wikidata(REST:ko-wiki)", "qid": qid, "lang": lang}
        for lang, obj in labels.items():
            return obj["value"], {"source":"Wikidata(REST:ko-wiki)", "qid": qid, "lang": lang}
        return None, {"source":"Wikidata(REST:ko-wiki)", "qid": qid, "reason":"no labels"}
    pref_langs = tuple(_wd_preferred_langs_for_qid(qid))
    labels, _ = _wd_get_labels(qid, langs=pref_langs)
    for lang in pref_langs:
        if lang in labels:
            val = labels[lang]["value"]
            if lang in ("en",) and " " in val.strip():
                val = _simple_reorder_family_given(val)
            return val, {"source":"Wikidata(REST)", "qid": qid, "lang": lang}
    for lang, obj in labels.items():
        return obj["value"], {"source":"Wikidata(REST)", "qid": qid, "lang": lang}
    return None, {"source":"Wikidata(REST)", "qid": qid, "reason":"no labels"}

def _ko_name_variants(name_ko: str) -> list[str]:
    """ì£¼ì–´ì§„ í•œê¸€ ì¸ëª…ì—ì„œ ê²€ìƒ‰ìš© ë³€ì´(í‘œê¸° ìˆœì„œ/ë„ì–´ì“°ê¸°/ì˜™Â·ì˜ˆí”„)ë¥¼ ìƒì„±."""
    name_ko = (name_ko or "").strip()
    out = set()
    if not name_ko:
        return []
    out.add(name_ko)
    # "ì„±, ì´ë¦„" â†’ "ì´ë¦„ ì„±"
    if "," in name_ko:
        parts = [p.strip() for p in name_ko.split(",")]
        if len(parts) == 2 and parts[0] and parts[1]:
            out.add(f"{parts[1]} {parts[0]}")
    # 'ì˜™'â†”'ì˜ˆ' / 'ì˜ˆí”„'â†”'ì˜™' ë³€ì´
    seeds = list(out)
    for s in seeds:
        out.add(s.replace("ì˜™", "ì˜ˆ"))
        out.add(s.replace("ì˜ˆí”„", "ì˜™"))
    # ê³µë°± ì œê±°/ì¶”ê°€ ë³€ì´
    seeds = list(out)
    for s in seeds:
        out.add(s.replace(" ", ""))
    # ë„ˆë¬´ ë§ì•„ì§€ì§€ ì•Šê²Œ ìƒìœ„ ëª‡ ê°œë§Œ
    return list(out)[:8]

def resolve_original_name_prefer_lod(name_ko: str):
    """
    Aladinì—ì„œ ë°›ì€ í•œêµ­ì–´ ì €ìëª… ê·¸ëŒ€ë¡œë§Œ ì‚¬ìš©.
      1) NLK LOD â†’ ì„±ê³µ ì‹œ ì±„íƒ (route=LOD)
      2) ê¸°ì¡´ Wikidata í•¨ìˆ˜ â†’ ì„±ê³µ ì‹œ ì±„íƒ (route=Wikidata, note=legacy)
      3) Wikidata REST â†’ ìµœì¢… í´ë°± (route=Wikidata(REST))
    """
    key = (name_ko or "").strip()
    # 1) LOD
    try:
        val, prov = get_original_name_via_lod(key)
    except Exception as e:
        val, prov = (None, {"route":"LOD", "source":"NLK LOD", "error":repr(e), "key": key})
    if val:
        return val, {"route":"LOD", "key": key, **(prov or {})}
    # 2) legacy Wikidata (ìˆìœ¼ë©´)
    try:
        alt = get_original_name_via_wikidata(key)
    except Exception:
        alt = None
    if alt:
        return alt, {"route":"Wikidata", "note":"legacy", "key": key}
    # 3) REST fallback
    rest_val, rest_prov = get_original_name_via_wikidata_rest(key)
    return rest_val, {"route":"Wikidata(REST)", "key": key, **(rest_prov or {})}
def resolve_original_name_prefer_lod(name_ko: str):
    """
    Aladinì—ì„œ ë°›ì€ í•œêµ­ì–´ ì €ìëª… ê·¸ëŒ€ë¡œë§Œ ì‚¬ìš©.
      1) NLK LOD â†’ ì„±ê³µ ì‹œ ì±„íƒ (route=LOD)
      2) ê¸°ì¡´ Wikidata í•¨ìˆ˜ â†’ ì„±ê³µ ì‹œ ì±„íƒ (route=Wikidata, note=legacy)
      3) Wikidata REST â†’ ìµœì¢… í´ë°± (route=Wikidata(REST))
    """
    key = (name_ko or "").strip()
    # 1) LOD
    try:
        val, prov = get_original_name_via_lod(key)
    except Exception as e:
        val, prov = (None, {"route":"LOD", "source":"NLK LOD", "error":repr(e), "key": key})
    if val:
        return val, {"route":"LOD", "key": key, **(prov or {})}
    # 2) legacy Wikidata (ìˆìœ¼ë©´)
    try:
        alt = get_original_name_via_wikidata(key)
    except Exception:
        alt = None
    if alt:
        return alt, {"route":"Wikidata", "note":"legacy", "key": key}
    # 3) REST fallback
    rest_val, rest_prov = get_original_name_via_wikidata_rest(key)
    return rest_val, {"route":"Wikidata(REST)", "key": key, **(rest_prov or {})}

def build_90010_prefer_lod_then_wikidata_with_meta(people: dict, include_translator: bool = True):
    """
    1) NLK LOD â†’ 2) Wikidata â†’ 3) ko-wiki í´ë°±ìœ¼ë¡œ ì›ì–´ëª… ìƒì„±
    - í•œêµ­ êµ­ì (P27: Q884/Q423/Q180)ì€ 900 ì œì™¸
    - QID ì—†ìœ¼ë©´ í•œê¸€ 2â€“4ì íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í•œêµ­ì¸ ì¶”ì • ì‹œ ì œì™¸
    - ì¶œë ¥ í¬ë§· ê³ ì •: =900  10$a<ì›ì–´ëª…>  ( $9 ì œê±° )
    - LAST_PROV_90010ì— provenance trace ì €ì¥
    """
    global LAST_PROV_90010
    LAST_PROV_90010 = []

    if not people:
        return [], []

    names_author = list(people.get("author") or [])
    names_trans  = list(people.get("translator") or []) if include_translator else []
    names_all = names_author + names_trans

    out, seen, trace = [], set(), []

    for nm in names_all:
        val, prov = resolve_original_name_prefer_lod(nm)
        role = "author" if nm in names_author else "translator"

        if not val:
            trace.append({"who": nm, "resolved": None, "role": role, "provenance": prov})
            continue

        # êµ­ì  ê¸°ë°˜ í•„í„°ë§ (í•œêµ­ì¸ 900 ì œì™¸)
        qid = None
        if isinstance(prov, dict):
            qid = prov.get("qid") or (prov.get("provenance") or {}).get("qid")
        if qid and _wd_is_korean_national(qid):
            trace.append({"who": nm, "resolved": val, "role": role,
                          "provenance": {**(prov or {}), "filtered": "korean_p27"}})
            continue
        # QID ì—†ê³  ìˆœìˆ˜ í•œê¸€ 2-4ìë©´ í•œêµ­ì¸ ì¶”ì • â†’ ì œì™¸
        if (not qid) and looks_korean_person_name(nm):
            trace.append({"who": nm, "resolved": val, "role": role,
                          "provenance": {**(prov or {}), "filtered": "korean_heuristic"}})
            continue

        key = (val, role)
        if key in seen:
            continue
        seen.add(key)

        out.append(f"=900  10$a{val}")
        trace.append({"who": nm, "resolved": val, "role": role, "provenance": prov})

    LAST_PROV_90010 = trace[:]
    return out, trace

    
def build_90010_from_wikidata(people: dict, include_translator: bool = True) -> list[str]:
    lines, _prov = build_90010_prefer_lod_then_wikidata_with_meta(people, include_translator=include_translator)
    return lines



def get_candidate_names_for_isbn(isbn: str) -> list[str]:
    """NLK/ì•Œë¼ë”˜ì—ì„œ ê° 1ì°¨ ì €ìëª…(í•œê¸€)ì„ ë½‘ì•„ í›„ë³´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜."""
    author_raw, _ = fetch_nlk_author_only(isbn)
    item = fetch_aladin_item(isbn)

    # NLK ì²« ì €ì
    nlk_first = ""
    try:
        authors, _trs = split_authors_translators(author_raw or "")
        nlk_first = (authors[0] if authors else "").strip()
    except Exception:
        pass

    # ì•Œë¼ë”˜ ì²« ì €ì
    aladin_first = extract_primary_author_ko_from_aladin(item)

    out = []
    for v in [nlk_first, aladin_first]:
        if v and v not in out:
            out.append(v)
    return out

def looks_korean_person_name(name: str) -> bool:
    """í•œê¸€ë¡œë§Œ êµ¬ì„±ëœ í•œêµ­ì¸ í‘œê¸°ì²˜ëŸ¼ ë³´ì´ë©´ True"""
    s = (name or "").strip()
    if not s:
        return False
    # ë¼í‹´/í‚¤ë¦´/ê°€ë‚˜/í•œì ì—†ëŠ” ìˆœìˆ˜ í•œê¸€Â·ì¤‘ì  ì¡°í•©ì´ë©´ í•œêµ­ì¸ì¼ í™•ë¥ â†‘
    return bool(_KOREAN_ONLY_RX.fullmatch(s))


def prewarm_wikidata_cache(all_isbns: list[str]):
    """ì—¬ëŸ¬ ISBNì˜ í›„ë³´ ì €ìëª…ì„ ëª¨ì•„ ì¼ê´„ë¡œ Wikidata ìºì‹œë¥¼ ì±„ì›€."""
    all_names = []
    for isbn in all_isbns:
        all_names.extend(get_candidate_names_for_isbn(isbn))
    # ì¤‘ë³µ ì œê±°
    seen, uniq = set(), []
    for n in all_names:
        if n and n not in seen:
            seen.add(n); uniq.append(n)

    # âœ… í•œë²ˆì— ë°°ì¹˜ ì¡°íšŒ â†’ SQLite ìºì‹œì— ì €ì¥ë¨
    _ = fetch_wikidata_names_batch(uniq)







WIKIDATA_TIMEOUT = (3, 6)  # (connect, read) for requests

# ë””ìŠ¤í¬ ìºì‹œ (SQLite) â€” ê°™ì€ ì´ë¦„ì€ ì¬í˜¸ì¶œ ê¸ˆì§€
_cache_lock = threading.Lock()
_conn = sqlite3.connect("author_cache.sqlite3", check_same_thread=False)
_conn.execute("""CREATE TABLE IF NOT EXISTS name_cache(
  key TEXT PRIMARY KEY,
  value TEXT
)""")
_conn.commit()  # <- í•œë²ˆ ì»¤ë°‹

def cache_get(key: str):
    with _cache_lock:
        cur = _conn.execute("SELECT value FROM name_cache WHERE key=?", (key,))
        row = cur.fetchone()
    if not row:
        return None
    try:
        return json.loads(row[0])
    except Exception:
        return row[0]  # í˜¹ì‹œ JSONì´ ì•„ë‹ˆë©´ ì›ë¬¸ ë°˜í™˜

def cache_set(key: str, value: dict):
    with _cache_lock:
        _conn.execute(
            "INSERT OR REPLACE INTO name_cache(key,value) VALUES(?,?)",
            (key, json.dumps(_jsonify(value), ensure_ascii=False)),
        )
        _conn.commit()

def cache_get_sets(key: str):
    raw = cache_get(key)
    return _ensure_name_bundle(raw) if raw is not None else None
# ì„¸íŠ¸ ì§ë ¬í™” í—¬í¼ëŠ” ê¸°ì¡´(_jsonify) ê·¸ëŒ€ë¡œ ì‚¬ìš©

def cache_set_many(items: list[tuple[str, dict]]):
    """[(key, dict), ...]ë¥¼ í•œ ë²ˆì— ì €ì¥ í›„ commit"""
    if not items:
        return
    with _cache_lock:
        _conn.executemany(
            "INSERT OR REPLACE INTO name_cache(key,value) VALUES(?,?)",
            [(k, json.dumps(_jsonify(v), ensure_ascii=False)) for k, v in items]
        )
        _conn.commit()



def _http_json(url, params=None, headers=None, timeout=(3,6)):
    try:
        r = SESSION.get(url, params=params or {}, headers=headers or {}, timeout=timeout)
        r.raise_for_status()
        return r.json()
    except Exception:
        return None

Q_JAPAN = "Q17"
Q_KOREA = "Q884"
Q_CHINA = "Q148"
Q_RUSSIA = "Q159"

def _run_sparql(q: str):
    url = "https://query.wikidata.org/sparql"
    headers = {"Accept":"application/sparql-results+json","User-Agent":"isbn2marc/1.0 (contact: local)"}
    return _http_json(url, params={"query": q, "format":"json"}, headers=headers, timeout=WIKIDATA_TIMEOUT) or {"results":{"bindings":[]}}

def fetch_wikidata_author_names_by_name(name: str) -> dict:
    """
    ê²°ê³¼: {"native": set[str], "roman": set[str], "countries": set[str]}
    """
    import re
    name = (name or "").strip()
    if not name:
        return {"native": set(), "roman": set(), "countries": set()}

    PREFIXES = """\
PREFIX wd:  <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>
PREFIX skos:<http://www.w3.org/2004/02/skos/core#>
"""

    query_eq = PREFIXES + """
SELECT DISTINCT ?author ?jaLabel ?zhLabel ?koLabel ?ruLabel ?enLabel ?nativeName ?country WHERE {
  ?author wdt:P31 wd:Q5 .
  ?author (rdfs:label|skos:altLabel) ?lab .
  FILTER(lang(?lab) IN ("ko","en"))
  OPTIONAL { ?author rdfs:label ?jaLabel FILTER (lang(?jaLabel) = "ja") }
  OPTIONAL { ?author rdfs:label ?zhLabel FILTER (lang(?zhLabel) = "zh") }
  OPTIONAL { ?author rdfs:label ?koLabel FILTER (lang(?koLabel) = "ko") }
  OPTIONAL { ?author rdfs:label ?ruLabel FILTER (lang(?ruLabel) = "ru") }
  OPTIONAL { ?author rdfs:label ?enLabel FILTER (lang(?enLabel) = "en") }
  OPTIONAL { ?author wdt:P1559 ?nativeName }
  OPTIONAL { ?author wdt:P27 ?country }
  FILTER(?lab = "__NAME__"@ko)
}
LIMIT 30
""".replace("__NAME__", name)

    needle = name.lower()
    query_like = PREFIXES + """
SELECT DISTINCT ?author ?jaLabel ?zhLabel ?koLabel ?ruLabel ?enLabel ?nativeName ?country WHERE {
  ?author wdt:P31 wd:Q5 .
  ?author (rdfs:label|skos:altLabel) ?lab .
  FILTER(lang(?lab) IN ("ko","en"))
  OPTIONAL { ?author rdfs:label ?jaLabel FILTER (lang(?jaLabel) = "ja") }
  OPTIONAL { ?author rdfs:label ?zhLabel FILTER (lang(?zhLabel) = "zh") }
  OPTIONAL { ?author rdfs:label ?koLabel FILTER (lang(?koLabel) = "ko") }
  OPTIONAL { ?author rdfs:label ?ruLabel FILTER (lang(?ruLabel) = "ru") }
  OPTIONAL { ?author rdfs:label ?enLabel FILTER (lang(?enLabel) = "en") }
  OPTIONAL { ?author wdt:P1559 ?nativeName }
  OPTIONAL { ?author wdt:P27 ?country }
  FILTER(CONTAINS(LCASE(?lab), "__NEEDLE__"))
}
LIMIT 30
""".replace("__NEEDLE__", needle)

    data = _run_sparql(query_eq)
    if not data.get("results", {}).get("bindings"):
        data = _run_sparql(query_like)

    native, roman, countries = set(), set(), set()
    has_cjk = lambda s: bool(re.search(r"[\u4E00-\u9FFF\u3040-\u30FF\uAC00-\uD7A3]", s))
    has_cyr = lambda s: bool(re.search(r"[\u0400-\u04FF]", s))
    has_lat = lambda s: bool(re.search(r"[A-Za-z]", s))

    for b in data.get("results", {}).get("bindings", []):
        c = b.get("country", {}).get("value", "")
        if c.startswith("http://www.wikidata.org/entity/"):
            countries.add(c.rsplit("/",1)[-1])

        ja = b.get("jaLabel", {}).get("value", "").strip()
        zh = b.get("zhLabel", {}).get("value", "").strip()
        ko = b.get("koLabel", {}).get("value", "").strip()
        ru = b.get("ruLabel", {}).get("value", "").strip()
        en = b.get("enLabel", {}).get("value", "").strip()
        nn = b.get("nativeName", {}).get("value", "").strip()

        if "Q884" in countries:   # í•œêµ­ â†’ ì •ì±…ìƒ 90010 ìƒëµ
            continue
        elif "Q17" in countries:  # ì¼ë³¸
            if ja: native.add(ja)
            if nn and has_cjk(nn): native.add(nn)
            if en: roman.add(en)
        elif "Q148" in countries: # ì¤‘êµ­
            if zh: native.add(zh)
            if nn and has_cjk(nn): native.add(nn)
            if en: roman.add(en)
        elif "Q159" in countries: # ëŸ¬ì‹œì•„
            if ru: native.add(ru)
            if nn and has_cyr(nn): native.add(nn)
            if en: roman.add(en)
        else:
            if nn:
                if has_cjk(nn): native.add(nn)
                elif has_cyr(nn): native.add(nn)
                elif has_lat(nn): roman.add(nn)
            if en: roman.add(en)

        if not (native or roman) and en:
            roman.add(en)

    return {"native": native, "roman": roman, "countries": countries}

def _ensure_name_bundle(d):
    if d is None: return {"native": set(), "roman": set(), "countries": set()}
    return {"native": set(d.get("native", [])),
            "roman": set(d.get("roman", [])),
            "countries": set(d.get("countries", []))}


def fetch_wikidata_names_batch(names: list[str]) -> dict:
    """
    ì—¬ëŸ¬ ì €ìëª…ì„ batchë¡œ Wikidata ì¡°íšŒ (ko ë¼ë²¨ ê¸°ì¤€).
    ê²°ê³¼: {name: {"native": set, "roman": set, "countries": set}}
    """
    import re
    if not names:
        return {}

    # ìºì‹œ í™•ì¸
    out, to_query = {}, []
    for n in names:
        cached = cache_get(f"wikidata|{n}")
        if cached:
            out[n] = _ensure_name_bundle(cached)
        else:
            to_query.append(n)

    if not to_query:
        return out

    # VALUES ë¸”ë¡ êµ¬ì„±
    vals = " ".join(f'"{n}"@ko' for n in to_query)

    q = f"""
PREFIX wd:  <http://www.wikidata.org/entity/>
PREFIX wdt: <http://www.wikidata.org/prop/direct/>
PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>
PREFIX skos:<http://www.w3.org/2004/02/skos/core#>
SELECT DISTINCT ?name ?jaLabel ?zhLabel ?koLabel ?ruLabel ?enLabel ?nativeName ?country WHERE {{
  VALUES ?name {{ {vals} }}
  ?author wdt:P31 wd:Q5 .
  ?author (rdfs:label|skos:altLabel) ?lab .
  FILTER(?lab = ?name && lang(?lab)="ko")

  OPTIONAL {{ ?author rdfs:label ?jaLabel FILTER (lang(?jaLabel) = "ja") }}
  OPTIONAL {{ ?author rdfs:label ?zhLabel FILTER (lang(?zhLabel) = "zh") }}
  OPTIONAL {{ ?author rdfs:label ?koLabel FILTER (lang(?koLabel) = "ko") }}
  OPTIONAL {{ ?author rdfs:label ?ruLabel FILTER (lang(?ruLabel) = "ru") }}
  OPTIONAL {{ ?author rdfs:label ?enLabel FILTER (lang(?enLabel) = "en") }}
  OPTIONAL {{ ?author wdt:P1559 ?nativeName }}
  OPTIONAL {{ ?author wdt:P27 ?country }}
}} LIMIT 1000
"""

    data = _run_sparql(q)

    # grouped dict ì´ˆê¸°í™”
    grouped = {n: {"native": set(), "roman": set(), "countries": set()} for n in to_query}

    for b in data.get("results", {}).get("bindings", []):
        key = b.get("name", {}).get("value", "")
        if not key:
            continue

        ja = b.get("jaLabel", {}).get("value", "").strip()
        zh = b.get("zhLabel", {}).get("value", "").strip()
        ko = b.get("koLabel", {}).get("value", "").strip()
        ru = b.get("ruLabel", {}).get("value", "").strip()
        en = b.get("enLabel", {}).get("value", "").strip()
        nn = b.get("nativeName", {}).get("value", "").strip()
        c  = b.get("country", {}).get("value", "")

        if c.startswith("http://www.wikidata.org/entity/"):
            grouped[key]["countries"].add(c.rsplit("/", 1)[-1])

        if ja: grouped[key]["native"].add(ja)
        if zh: grouped[key]["native"].add(zh)
        if ko: grouped[key]["native"].add(ko)
        if ru: grouped[key]["native"].add(ru)

        if nn:
            if re.search(r"[\u4E00-\u9FFF\u3040-\u30FF\uAC00-\uD7A3]", nn): grouped[key]["native"].add(nn)
            elif re.search(r"[\u0400-\u04FF]", nn): grouped[key]["native"].add(nn)
            elif re.search(r"[A-Za-z]", nn): grouped[key]["roman"].add(nn)

        if en:
            grouped[key]["roman"].add(en)

    # âœ… ì—¬ê¸° ì €ì¥ íŒŒíŠ¸ êµì²´
    items = [(f"wikidata|{n}", grouped[n]) for n in to_query]
    cache_set_many(items)

    # out ë³‘í•©
    for n in to_query:
        out[n] = _ensure_name_bundle(cache_get(f"wikidata|{n}"))

    return out

_CJK_RX = re.compile(r"[\u4E00-\u9FFF\u3040-\u30FF\uAC00-\uD7A3]")  # í•œì/ê°€ë‚˜/í•œê¸€
_CYR_RX = re.compile(r"[\u0400-\u04FF]")  # í‚¤ë¦´

def reorder_western_like_name(name: str) -> str:
    """
    'ì´ë¦„ ì„±' â†’ 'ì„±, ì´ë¦„' ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê°„ë‹¨ í•¨ìˆ˜.
    - ë¼í‹´/í‚¤ë¦´ ë¬¸ìì—ë§Œ ì ìš©
    - í•œê¸€ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
    """
    if not name:
        return ""
    s = name.strip()
    # CJKëŠ” ê·¸ëŒ€ë¡œ
    if _CJK_RX.search(s):
        return s
    parts = s.split()
    if len(parts) >= 2:
        family = parts[-1]
        given = " ".join(parts[:-1])
        return f"{family}, {given}"
    return s


# 90010 ìƒì„±ê¸° (í‚¤ë¦´+ë¡œë§ˆì ë‘˜ ë‹¤)

# === [REPLACE] build_90010_from_wikidata (VIAF ì œê±°) ===

def build_90010_from_lod(people: dict, include_translator: bool = True) -> list[str]:
    """
    author(ï¼‹ì„ íƒì ìœ¼ë¡œ translator) ê°ê°ì— ëŒ€í•´
    êµ­ì¤‘ LODì—ì„œ 'í•œê¸€ì´ ì•„ë‹Œ ì´ë¦„' í•˜ë‚˜ë¥¼ ì°¾ì•„ 90010ì— ì‹£ëŠ”ë‹¤.
    í¬ë§· ì˜ˆ: =90010  \\$aĞ¤Ñ‘Ğ´Ğ¾Ñ€ Ğ”Ğ¾ÑÑ‚Ğ¾ĞµĞ²ÑĞºĞ¸Ğ¹$9author
    """
    if not (people and INCLUDE_ORIGINAL_NAME_IN_90010 and USE_NLK_LOD_AUTH):
        return []

    # ëŒ€ìƒ ì´ë¦„ ëª©ë¡
    names_author = list(people.get("author", []))
    names_trans  = list(people.get("translator", [])) if include_translator else []
    names_all = names_author + names_trans

    out, seen = [], set()
    for nm in names_all:
        orig = get_original_name_via_lod(nm)
        if not orig:
            continue
        role = "author" if nm in names_author else "translator"
        key = (orig, role)
        if key in seen:
            continue
        seen.add(key)
        out.append(f"=90010  \\\\$a{orig}$9{role}")
    return out





# =========================
# ğŸ§¹ ë¬¸ìì—´/245 ìœ í‹¸
# =========================
DELIMS = [": ", " : ", ":", " - ", " â€” ", "â€“", "â€”", "-", " Â· ", "Â·", "; ", ";", " | ", "|", "/"]

def _compat_normalize(s: str) -> str:
    if not s:
        return ""
    s = s.replace("ï¼š", ":").replace("ï¼", "-").replace("â€§", "Â·").replace("ï¼", "/")
    s = re.sub(r"[\u2000-\u200f\u202a-\u202e]", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

_TRAIL_PAREN_PAT = re.compile(
    r"""\s*(?:[\(\[](
        ê°œì •|ì¦ë³´|ê°œì—­|ì „ì •|í•©ë³¸|ì „ë©´ê°œì •|ê°œì •íŒ|ì¦ë³´íŒ|ì‹ íŒ|ë³´ê¸‰íŒ|
        ìµœì‹ ê°œì •íŒ|ê°œì •ì¦ë³´íŒ|êµ­ì—­|ë²ˆì—­|ì˜ë¬¸íŒ|ì´ˆíŒ|ì œ?\d+\s*íŒ|
        [^()\[\]]*ì´ì„œ[^()\[\]]*|[^()\[\]]*ì‹œë¦¬ì¦ˆ[^()\[\]]*
    )[\)\]])\s*$""", re.IGNORECASE | re.VERBOSE
)

def _strip_trailing_paren_notes(s: str) -> str:
    return _TRAIL_PAREN_PAT.sub("", s).strip(" .,/;:-â€”Â·|")

def _clean_piece(s: str) -> str:
    if not s:
        return ""
    s = _compat_normalize(s)
    s = _strip_trailing_paren_notes(s)
    s = s.strip(" .,/;:-â€”Â·|")
    return s

def _find_top_level_split(text: str, delims=DELIMS):
    pairs = {"(": ")", "[": "]", "{": "}", "ã€ˆ": "ã€‰", "ã€Š": "ã€‹", "ã€Œ": "ã€", "ã€": "ã€", "â€œ": "â€", "â€˜": "â€™", "Â«": "Â»"}
    opens, closes = set(pairs), {v: k for k, v in pairs.items()}
    stack, i, L = [], 0, len(text)
    while i < L:
        ch = text[i]
        if ch in opens:
            stack.append(ch); i += 1; continue
        if ch in closes:
            if stack and pairs.get(stack[-1]) == ch: stack.pop()
            i += 1; continue
        if not stack:
            for d in delims:
                if text.startswith(d, i):
                    return i, d
        i += 1
    return None

def split_title_only_for_245(title: str):
    if not title:
        return "", None
    t = _compat_normalize(title)
    hit = _find_top_level_split(t, DELIMS)
    if not hit:
        return _clean_piece(t), None
    idx, delim = hit
    left, right = t[:idx], t[idx + len(delim):]
    return _clean_piece(left), (_clean_piece(right) or None)

def extract_245_from_aladin_item(item: dict, collapse_a_spaces: bool = True):
    raw_title = (item.get("title") or "")
    raw_sub   = (item.get("subInfo", {}) or {}).get("subTitle") or ""

    # 1) ì•Œë¼ë”˜ì˜ title/subTitleì—ì„œ ê¸°ë³¸ $a/$b ì¶”ì¶œ (ë„¤ ê¸°ì¡´ ë¡œì§)
    t = _compat_normalize(raw_title)
    s = _clean_piece(raw_sub)
    if s:
        tail = [f" : {s}", f": {s}", f":{s}", f" - {s}", f"- {s}", f"-{s}"]
        t_removed = t
        for pat in tail:
            if t_removed.endswith(pat):
                t_removed = t_removed[: -len(pat)]
                break
        a0, b = _clean_piece(t_removed) or _clean_piece(t), s
    else:
        a0, b = split_title_only_for_245(t)

    # 2) $a ëì˜ ê¶Œì°¨ í›„ë³´ë¥¼ $nìœ¼ë¡œ ë–¼ê¸°
    a_base, n = _split_part_suffix_for_245(a0, item)

    # 3) $a ê³µë°± ìœ ì§€/ì œê±° ì˜µì…˜
    a_out = a_base.replace(" ", "") if collapse_a_spaces else a_base

    # 4) MRK ì¡°ë¦½ ($nì€ $a ë‹¤ìŒ, $bë³´ë‹¤ ë¨¼ì €)
    line = f"=245  00$a{a_out}"
    if n:
        line += (" " if a_out.endswith(".") else " .")  # a_outì´ ì´ë¯¸ '.'ë¡œ ëë‚˜ë©´ ê³µë°±ë§Œ, ì•„ë‹ˆë©´ ' .' ì¶”ê°€
        line += f"$n{n}"
    if b:
        line += f" :$b{b}"

    return {"ind1":"0","ind2":"0","a":a_out,"b":b,"n":n,"mrk":line}


# ê¶Œì°¨ í›„ë³´ íŒë‹¨ì— ì“°ëŠ” í‚¤ì›Œë“œ/íŒ¨í„´
_PART_LABEL_RX = re.compile(
    r"(?:ì œ?\s*\d+\s*(?:ê¶Œ|ë¶€|í¸|ì±…)|"     # ì œ1ê¶Œ/1ê¶Œ/1ë¶€/1í¸/1ì±…
    r"[IVXLCDM]+|"                         # ë¡œë§ˆ ìˆ«ì I, II, III ...
    r"[ìƒì¤‘í•˜]|[ì „í›„])$",                  # ìƒ/ì¤‘/í•˜, ì „/í›„
    re.IGNORECASE
)

def _has_series_evidence(item: dict) -> bool:
    """ì‹œë¦¬ì¦ˆ/ì›ì œ ë“± ê¶Œì°¨ ê°€ëŠ¥ì„± ë³´ê°• ì‹ í˜¸"""
    series = item.get("seriesInfo") or {}
    sub    = item.get("subInfo") or {}
    # seriesName/IDê°€ ìˆìœ¼ë©´ ì‹œë¦¬ì¦ˆ ê°€ëŠ¥ì„±â†‘
    if series.get("seriesName") or series.get("seriesId"):
        return True
    # ì›ì œê°€ ìˆê³ , ì›ì œëŠ” ìˆ«ìë¡œ ëë‚˜ì§€ ì•ŠëŠ”ë° í•œê¸€ì œëª©ë§Œ ìˆ«ìë¡œ ëë‚˜ë©´ ê¶Œì°¨ ê°€ëŠ¥ì„±â†‘
    orig = (sub.get("originalTitle") or "").strip()
    if orig and not re.search(r"\d\s*$", orig):
        return True
    return False

def _split_part_suffix_for_245(a_raw: str, item: dict) -> tuple[str, str|None]:
    """
    ì œëª© $a í›„ë³´ ë¬¸ìì—´ì—ì„œ ëì˜ ê¶Œì°¨/ë¶€/í¸/ìˆ«ì/ë¡œë§ˆìˆ«ì/ìƒì¤‘í•˜/ì „í›„ ë“±ì„ ë–¼ì–´ $nìœ¼ë¡œ.
    ë°˜í™˜: (a_base, n_or_None)
    """
    if not a_raw:
        return "", None

    a = _clean_piece(a_raw)  # ë„¤ê°€ ì´ë¯¸ ì“°ê³  ìˆëŠ” ì •ë¦¬ í•¨ìˆ˜
    # (1) ì „ë¶€ ìˆ«ì/ë¡œë§ˆìˆ«ìì¸ ì œëª©ì€ 'ìˆ«ì ì œëª©'ìœ¼ë¡œ ë³´ê³  ë¶„ë¦¬í•˜ì§€ ì•ŠìŒ (ì˜ˆ: '1984')
    if re.fullmatch(r"\d+|[IVXLCDM]+", a, re.IGNORECASE):
        return a, None

    # (2) '... (ì œ1ê¶Œ)' ê°™ì€ ê´„í˜¸í˜• ê¶Œì°¨ â†’ ìš°ì„  ì²˜ë¦¬
    m_paren = re.search(r"\s*[\(\[]\s*([^()\[\]]+)\s*[\)\]]\s*$", a)
    if m_paren and _PART_LABEL_RX.search(m_paren.group(1).strip()):
        n_token = m_paren.group(1).strip()
        a_base  = a[: m_paren.start()].rstrip(" .,/;:-â€”Â·|")
        # 'ì œ1ê¶Œ'ì€ ìˆ«ìë§Œ ë‚¨ê²¨ ì£¼ëŠ” ê²Œ ê¹”ë”
        m_num = re.search(r"\d+", n_token)
        return a_base, (m_num.group(0) if m_num else n_token)

    # (3) ë¼ë²¨í˜• ê¶Œì°¨(ë¶™ì€ í˜•íƒœ í¬í•¨): '... ì œ1ê¶Œ' / '... 1ê¶Œ' / '... 1ë¶€' / '...1í¸'
    m_label = re.search(r"\s*(ì œ?\s*\d+\s*(?:ê¶Œ|ë¶€|í¸|ì±…))\s*$", a, re.IGNORECASE)
    if m_label:
        a_base = a[: m_label.start()].rstrip(" .,/;:-â€”Â·|")
        num    = re.search(r"\d+", m_label.group(1))
        return a_base, (num.group(0) if num else m_label.group(1).strip())

    # (4) ìƒ/ì¤‘/í•˜, ì „/í›„
    m_kor = re.search(r"\s*([ìƒì¤‘í•˜]|[ì „í›„])\s*$", a)
    if m_kor:
        a_base = a[: m_kor.start()].rstrip(" .,/;:-â€”Â·|")
        return a_base, m_kor.group(1)

    # (5) ë¡œë§ˆìˆ«ì (I, II, III, â€¦)
    m_roman = re.search(r"\s*([IVXLCDM]+)\s*$", a, re.IGNORECASE)
    if m_roman:
        a_base = a[: m_roman.start()].rstrip(" .,/;:-â€”Â·|")
        token  = m_roman.group(1)
        # a ì „ì²´ê°€ ë¡œë§ˆìˆ«ìë§Œì€ ì•„ë‹Œì§€ ìœ„ì—ì„œ í•œ ë²ˆ ë” ì²´í¬í–ˆìœ¼ë‹ˆ OK
        return a_base, token

    # (6) ë§¨ ë 'ë§¨ë°”ë¡œ ìˆ«ì' â€” ê³¼ëŒ€ ë¶„ë¦¬ ë°©ì§€ ìœ„í•´ 'ì‹œë¦¬ì¦ˆ/ì›ì œ' ê°™ì€ ë³´ê°• ì‹ í˜¸ê°€ ìˆì„ ë•Œë§Œ
    m_tailnum = re.search(r"\s*(\d{1,3})\s*$", a)
    if m_tailnum and _has_series_evidence(item):
        a_base = a[: m_tailnum.start()].rstrip(" .,/;:-â€”Â·|")
        # 'íŒŒì´ì¬ 3' ê°™ì€ 'íŒ/ê°œì •'ì€ ë’¤ì— 'íŒ/ì‡„/ed'ê°€ ë¶™ëŠ” ê²½ìš°ê°€ ë§ì•„ ì—¬ê¸°ì—” ì•ˆ ê±¸ë¦¼
        if a_base:  # ë² ì´ìŠ¤ê°€ ë¹„ì§€ ì•Šì„ ë•Œë§Œ (ì „ë¶€ ìˆ«ìì¸ ì œëª© ë°©ì§€)
            return a_base, m_tailnum.group(1)

    # (7) ë¶„ë¦¬ ëª» í•˜ë©´ ê·¸ëŒ€ë¡œ
    return a, None

def get_title_a_from_aladin(item: dict) -> str:
    # 245 $aë¡œ ì“°ëŠ” ë³¸í‘œì œë§Œ (ë¶€ì œ ì œì™¸) â€” 245 ë¹Œë”ì™€ ë™ì¼ ì •ë¦¬ ê·œì¹™
    import re
    t = ((item or {}).get("title") or "").strip()
    t = re.sub(r"\s+([:;,./])", r"\1", t).strip()
    t = re.sub(r"[.:;,/]\s*$", "", t).strip()
    return t

def parse_245_a_n(marc245_line: str) -> tuple[str, str | None]:
    """
    '=245  00$a...$n...$b...' í•œ ì¤„ì—ì„œ
    - $a(ë³¸í‘œì œ)ë§Œ
    - $n(ê¶Œì°¨í‘œì‹œ) ìœ ë¬´/ê°’
    ì„ ë½‘ì•„ì¤€ë‹¤.
    """
    if not marc245_line:
        return "", None

    # $a ì¶”ì¶œ
    m_a = re.search(r"=245\s+\d{2}\$a(.*?)(?=\$[a-z]|$)", marc245_line)
    a_out = (m_a.group(1).strip() if m_a else "").strip()

    # $a ëì˜ ë¶ˆí•„ìš”í•œ êµ¬ë‘ì  ì •ë¦¬ (.,:;/ ê³µë°±)
    a_out = re.sub(r"\s+([:;,./])", r"\1", a_out)
    a_out = re.sub(r"[.:;,/]\s*$", "", a_out).strip()

    # $n ì¶”ì¶œ (ìˆìœ¼ë©´ ìˆ«ì ì½ê¸° ê¸ˆì§€ì— ì“°ì„)
    m_n = re.search(r"\$n(.*?)(?=\$[a-z]|$)", marc245_line)
    n_val = m_n.group(1).strip() if m_n else None

    return a_out, n_val if n_val else None

# """ì•Œë¼ë”˜ originalTitleì´ ìˆìœ¼ë©´ 246 19 $a ë¡œ ë°˜í™˜"""

# ì›ì œ ëì˜ (YYYY/YYYë…„), (rev. ed.), (2nd ed.), (ì œ2íŒ) ë“± ì œê±°
_YEAR_OR_EDITION_PAREN_PAT = re.compile(
    r"""
    \s*
    \(
      \s*
      (?:                                # ì•„ë˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë§ìœ¼ë©´ ì‚­ì œ
         \d{3,4}\s*ë…„?                   # 1866, 1866ë…„, 1942 ë“±
        |rev(?:ised)?\.?\s*ed\.?         # rev. ed., revised ed.
        |(?:\d+(?:st|nd|rd|th)\s*ed\.?)  # 2nd ed., 3rd ed.
        |edition                         # edition
        |ed\.?                           # ed.
        |ì œ?\s*\d+\s*íŒ                   # ì œ2íŒ, 2íŒ
        |ê°œì •(?:ì¦ë³´)?íŒ?                 # ê°œì •íŒ, ê°œì •ì¦ë³´íŒ
        |ì¦ë³´íŒ|ì´ˆíŒ|ì‹ íŒ|ë³´ê¸‰íŒ
      )
      [^()\[\]]*
    \)
    \s*$
    """,
    re.IGNORECASE | re.VERBOSE
)


def build_246_from_aladin_item(item: dict) -> str | None:
    if not item:
        return None
    orig = ((item.get("subInfo") or {}).get("originalTitle") or "").strip()
    # 1) ìš°ë¦¬ ê³µí†µ í´ë¦°ì—…: ì•ë’¤ ê³µë°±/ê¸°í˜¸, ê´„í˜¸í˜• íŒÂ·ì‹œë¦¬ì¦ˆ ê¼¬ë¦¬ ì œê±°
    orig = _clean_piece(orig)  # _strip_trailing_paren_notes í¬í•¨:contentReference[oaicite:2]{index=2}

    # 2) ëì˜ (YYYY/ë…„)Â·ì˜ë¬¸íŒ í‘œê¸° ë“± ì¶”ê°€ ì œê±°
    orig = _YEAR_OR_EDITION_PAREN_PAT.sub("", orig).strip()

    if orig:
        return f"=246  19$a{orig}"
    return None



# =========================
# ğŸ” ì™¸ë¶€ API (NLK / ì•Œë¼ë”˜)
# =========================
from urllib.parse import urlencode

def build_nlk_url_json(isbn: str, page_no: int = 1, page_size: int = 1) -> str:
    base = "https://seoji.nl.go.kr/landingPage/SearchApi.do"
    qs = urlencode({
        "cert_key": NLK_CERT_KEY,
        "result_style": "json",
        "page_no": page_no,
        "page_size": page_size,
        "isbn": isbn
    })
    return f"{base}?{qs}"

def fetch_nlk_seoji_json(isbn: str):
    """ë‹¤ì¤‘ ì—”ë“œí¬ì¸íŠ¸ ìˆœì°¨ ì‹œë„ â†’ (ì²« ì„±ê³µ) (ë ˆì½”ë“œ, ì‹¤ì œ URL) ë°˜í™˜"""
    if not NLK_CERT_KEY:
        raise RuntimeError("NLK_CERT_KEY ë¯¸ì„¤ì •")

    attempts = [
        "https://seoji.nl.go.kr/landingPage/SearchApi.do",
        "https://www.nl.go.kr/seoji/SearchApi.do",
        "http://seoji.nl.go.kr/landingPage/SearchApi.do",
        "http://www.nl.go.kr/seoji/SearchApi.do",
    ]
    params = {
        "cert_key": NLK_CERT_KEY, "result_style": "json",
        "page_no": 1, "page_size": 1, "isbn": isbn
    }
    last_err = None
    for base in attempts:
        try:
            r = SESSION.get(base, params=params, timeout=(10, 30))
            r.raise_for_status()
            data = r.json()
            docs = data.get("docs") or data.get("DOCS") or []
            if docs:
                return docs[0], r.url
        except Exception as e:
            last_err = e
            continue
    raise RuntimeError(f"NLK JSON ì‹¤íŒ¨: {last_err}")

def fetch_nlk_author_only(isbn: str):
    """(AUTHOR ì›ë¬¸, ì‹¤ì œ ì‚¬ìš© URL)"""
    try:
        rec, used_url = fetch_nlk_seoji_json(isbn)
        author = get_anycase(rec, "AUTHOR") or ""
        return author, used_url
    except Exception:
        return "", build_nlk_url_json(isbn)

def fetch_aladin_item(isbn13: str) -> dict:
    if not ALADIN_TTB_KEY:
        raise RuntimeError("ALADIN_TTB_KEY ë¯¸ì„¤ì •")
    url = "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
    params = {
        "ttbkey": ALADIN_TTB_KEY, "itemIdType": "ISBN13",
        "ItemId": isbn13, "output": "js", "Version": "20131101",
    }
    r = SESSION.get(url, params=params, timeout=(5, 20))
    r.raise_for_status()
    data = r.json()
    return (data.get("item") or [{}])[0]


# === 940: AI ë³´ê°• ===


_ai940_lock = threading.Lock()
_ai940_conn = sqlite3.connect("author_cache.sqlite3", check_same_thread=False)
_ai940_conn.execute("""CREATE TABLE IF NOT EXISTS name_cache(
  key TEXT PRIMARY KEY,
  value TEXT
)""")

def _ai940_get(key: str):
    with _ai940_lock:
        cur = _ai940_conn.execute("SELECT value FROM name_cache WHERE key=?", (key,))
        row = cur.fetchone()
    return json.loads(row[0]) if row else None

def _ai940_set(key: str, value: list[str]):
    with _ai940_lock:
        _ai940_conn.execute("INSERT OR REPLACE INTO name_cache(key,value) VALUES(?,?)",
                            (key, json.dumps(value, ensure_ascii=False)))
        _ai940_conn.commit()

def ai_korean_readings(title: str, n: int = 4) -> List[str]:
    title = (title or "").strip()
    if not title or _client is None:
        return []

    key = f"ai940|{title}"
    cached = _ai940_get(key)
    if cached is not None:
        return cached[:n]

    try:
        sys = (
            "ì—­í• : í•œêµ­ì–´ ë„ì„œ ì„œëª… 'ë°œìŒ í‘œê¸° ìƒì„±ê¸°'. "
            "ì…ë ¥ ì„œëª…ì˜ ì˜ì–´/ìˆ«ìë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë°œìŒìœ¼ë¡œ ë°”ê¾¸ì–´ë¼. "
            "ê° ì¤„ì— í•˜ë‚˜ì˜ ë³€í˜•ë§Œ ì¶œë ¥. ì„¤ëª…/ë²ˆí˜¸/ê¸°í˜¸ ê¸ˆì§€. ìµœëŒ€ 6ì¤„."
        )
        prompt = (
            f"ì„œëª…: {title}\n"
            "ì§€ì¹¨: í‘œê¸°ëŠ” í•œêµ­ì–´ë¡œë§Œ, ë§ì¶¤ë²• ì¤€ìˆ˜. "
            "ì˜ˆ: 2025â†’ì´ì²œì´ì‹­ì˜¤, 2.0â†’ì´ì ì˜, ChatGPTâ†’ì±—ì§€í”¼í‹°"
        )
        resp = _client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"system","content":sys},
                      {"role":"user","content":prompt}],
            temperature=0.3,
        )
        text = (resp.choices[0].message.content or "").strip()
        lines = [re.sub(r"^\d+[\).\s-]*", "", x).strip() for x in text.splitlines() if x.strip()]
        lines = [l for l in lines if l and l != title and re.search(r"[ê°€-í£]", l)]
        _ai940_set(key, lines)
        return lines[:n]
    except Exception:
        return []



EN_KO_MAP = {
    "chatgpt": "ì±—ì§€í”¼í‹°",
    "gpt": "ì§€í”¼í‹°",
    "ai": "ì—ì´ì•„ì´",
    "api": "ì—ì´í”¼ì•„ì´",
    "ml": "ì— ì—˜",
    "nlp": "ì—”ì—˜í”¼",
    "llm": "ì—˜ì—˜ì— ",
    "excel": "ì—‘ì…€",
    "youtube": "ìœ íŠœë¸Œ",
}

# ì†Œìˆ˜ ë“± íŠ¹ì • íŒ¨í„´ ê³ ì • ì½ê¸°
DECIMAL_MAP = {
    "2.0": "ì´ì ì˜",   # â† ìš”ì²­ ë°˜ì˜!
    "3.0": "ì‚¼ì ì˜",
    "4.0": "ì‚¬ì ì˜",
}

SINO = {"0":"ì˜","1":"ì¼","2":"ì´","3":"ì‚¼","4":"ì‚¬","5":"ì˜¤","6":"ìœ¡","7":"ì¹ ","8":"íŒ”","9":"êµ¬"}
ZERO_ALT = ["ì˜", "ê³µ"]  # ìë¦¿ìˆ˜ ì½ê¸° ëŒ€ì•ˆ

def replace_decimals(text: str) -> str:
    for k, v in DECIMAL_MAP.items():
        text = text.replace(k, v)
    return text

def replace_english_simple(text: str) -> str:
    if not EN_KO_MAP: 
        return text
    def _sub(m):
        return EN_KO_MAP.get(m.group(0).lower(), m.group(0))
    pattern = r"\b(" + "|".join(map(re.escape, EN_KO_MAP.keys())) + r")\b"
    return re.sub(pattern, _sub, text, flags=re.IGNORECASE)

def _read_year_yyyy(num: str) -> str:
    n = int(num)
    th = n // 1000; hu = (n // 100) % 10; te = (n // 10) % 10; on = n % 10
    out = []
    if th: out.append(SINO[str(th)] + "ì²œ")
    if hu: out.append(SINO[str(hu)] + "ë°±")
    if te: out.append("ì‹­" if te==1 else SINO[str(te)] + "ì‹­")
    if on: out.append(SINO[str(on)])
    return "".join(out) if out else "ì˜"

def _read_cardinal(num: str) -> str:
    return _read_year_yyyy(num)

def _read_digits(num: str, zero="ì˜") -> str:
    return "".join(SINO[ch] if ch in SINO and ch != "0" else (zero if ch=="0" else ch) for ch in num)

def generate_korean_title_variants(title: str, max_variants: int = 5) -> List[str]:
    """
    ê·œì¹™ ê¸°ë°˜ ë³€í˜• ìƒì„±:
      - ì˜ë¬¸ ê°„ì´ ì¹˜í™˜
      - ì†Œìˆ˜ ê³ ì • ì¹˜í™˜ (ì˜ˆ: 2.0â†’ì´ì ì˜)
      - ìˆ«ì: ì—°ë„ì‹/ìë¦¿ìˆ˜(ì˜Â·ê³µ) ì½ê¸°
    """
    base0 = (title or "").strip()
    base = replace_decimals(base0)
    base = replace_english_simple(base)

    variants = {base0, base}

    nums = re.findall(r"\d{2,}", base0)
    if nums:
        # ê° ìˆ«ìì— ëŒ€í•´ ëŒ€í‘œ ì½ê¸° í›„ë³´ ìƒì„±
        per_num_choices = []
        for n in nums:
            local = {_read_cardinal(n)}
            if len(n) == 4 and 1000 <= int(n) <= 2999:
                local.add(_read_year_yyyy(n))
            for z in ZERO_ALT:
                local.add(_read_digits(n, zero=z))
            per_num_choices.append(sorted(local, key=len))

        # ìˆœì°¨ ì ìš©ìœ¼ë¡œ ì¡°í•© í­ë°œ ë°©ì§€
        work = {base}
        for i, choices in enumerate(per_num_choices):
            new_work = set()
            for w in work:
                # í•´ë‹¹ ì°¨ë¡€ì˜ ìˆ«ìë§Œ 1íšŒ ì¹˜í™˜
                cnt = 0
                for c in choices:
                    def _repl(m, idx=i, repl=c):
                        nonlocal cnt
                        if cnt==0 and m.group(0)==nums[idx]:
                            cnt = 1
                            return repl
                        return m.group(0)
                    new_work.add(re.sub(r"\d{2,}", _repl, w))
            work = new_work
        variants |= work

    # í›„ì²˜ë¦¬
    outs = []
    for v in variants:
        if not v: continue
        v = re.sub(r"\s+([:;,./])", r"\1", v).strip()
        outs.append(v)
    outs = sorted(set(outs), key=lambda s: (len(s), s))
    return outs[:max_variants]

def build_940_from_title_a(title_a: str, use_ai: bool = True, *, disable_number_reading: bool = False) -> list[str]:
    import re
    base = (title_a or "").strip()
    if not base:
        return []

    # ìˆ«ì/ì˜ë¬¸ ì—†ìœ¼ë©´ ìƒì„± ìƒëµ
    if not re.search(r"[0-9A-Za-z]", base):
        return []

    # ê·œì¹™ ê¸°ë°˜
    if disable_number_reading:
        # ìˆ«ì ì½ê¸°ë¥¼ ë§‰ê³ , ì˜ì–´ ì¹˜í™˜/ì†Œìˆ˜ ê³ ì •ë§Œ ì ìš©
        v0 = replace_english_simple(base) if 'replace_english_simple' in globals() else base
        variants = sorted({v0})
    else:
        variants = generate_korean_title_variants(base, max_variants=5)

    # AI ë³´ê°•(ì—„ê²© ëª¨ë“œ)
    if 'ai_korean_readings_strict' in globals():
        variants += ai_korean_readings_strict(base, n=4)
    else:
        variants += ai_korean_readings(base, n=4)

    def _illegal_punct(v: str) -> bool:
        new_colon = (":" in v) and (":" not in base)
        new_dash  = (" - " in v) and (" - " not in base) and ("-" not in base)
        return new_colon or new_dash

    out, seen = [], set()
    for v in variants:
        v = (v or "").strip()
        if not v or v == base: 
            continue
        if _illegal_punct(v):
            continue
        if v not in seen:
            seen.add(v)
            out.append(f"=940  \\\\$a{v}")
    return out[:6]

def ai_korean_readings_strict(title_a: str, n: int = 4) -> list[str]:
    """
    OpenAIë¡œ ìˆ«ì/ì˜ë¬¸ì„ í•œêµ­ì–´ ë°œìŒìœ¼ë¡œ ë³€í™˜ (ì…ë ¥ $aë§Œ ì‚¬ìš©)
    - ë¶€ì œ/ì¶”ê°€ ë‹¨ì–´/ì½œë¡ /ëŒ€ì‹œ ì¶”ê°€ ê¸ˆì§€
    """
    import re
    if not title_a or _client is None:
        return []

    key = f"ai940|strict|{title_a}"
    cached = _ai940_get(key)
    if cached is not None:
        return cached[:n]

    try:
        sys = (
            "ì—­í• : í•œêµ­ì–´ ë„ì„œ ì„œëª… 'ë°œìŒ í‘œê¸° ìƒì„±ê¸°'. "
            "ì£¼ì–´ì§„ ë³¸í‘œì œ(245 $a)ì—ì„œ ìˆ«ì/ì˜ë¬¸ë§Œ í•œêµ­ì–´ ë°œìŒìœ¼ë¡œ ì¹˜í™˜í•˜ë¼. "
            "ì…ë ¥ì— ì—†ëŠ” ë‹¨ì–´/ë¶€ì œ($b) ì¶”ê°€ ê¸ˆì§€. ì½œë¡ (:), ëŒ€ì‹œ(-) ë“± ìƒˆ êµ¬ë‘ì  ì¶”ê°€ ê¸ˆì§€. "
            "ê° ì¤„ì— 1ê°œ ë³€í˜•ë§Œ, ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥."
        )
        prompt = (
            f"ë³¸í‘œì œ(245 $a): {title_a}\n"
            "ì˜ˆ: 2025â†’ì´ì²œì´ì‹­ì˜¤, 2.0â†’ì´ì ì˜, ChatGPTâ†’ì±—ì§€í”¼í‹°"
        )
        resp = _client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"system","content":sys},
                      {"role":"user","content":prompt}],
            temperature=0.2,
        )
        text = (resp.choices[0].message.content or "").strip()
        lines = [re.sub(r"^\d+[\).\s-]*", "", x).strip() for x in text.splitlines() if x.strip()]
        # í•œê¸€ í¬í•¨ + base ë¯¸í¬í•¨ êµ¬ë‘ì  ê¸ˆì§€
        safe = []
        for l in lines:
            if not re.search(r"[ê°€-í£]", l):
                continue
            # ì…ë ¥ì— ì—†ë˜ ì½œë¡ /ëŒ€ì‹œ ì¶”ê°€ ê¸ˆì§€
            if (":" in l and ":" not in title_a) or (" - " in l and " - " not in title_a and "-" not in title_a):
                continue
            safe.append(l)
        _ai940_set(key, safe)
        return safe[:n]
    except Exception:
        return []




# =========================
# ğŸ‘¤ NLK AUTHOR â†’ ì €ì/ì—­ì ë¶„ë¦¬ & 700
# =========================
# ì‚¬ëŒ ë‹¨ìœ„ ë¶„í• (ì„¸ë¯¸ì½œë¡ ì€ ê·¸ë£¹ ë¶„ë¦¬ë¡œ ë‹¤ë£¸)
SEP_PATTERN = re.compile(r"\s*[,/&Â·]\s*|\s+and\s+|\s+with\s+|\s*\|\s*", re.IGNORECASE)

# ì €ì ë¼ë²¨(â€˜ê·¸ë¦¼/ì‚½í™”/ì¼ëŸ¬ìŠ¤íŠ¸/ê·¸ë¦°â€™ í¬í•¨, â€˜ê¸€Â·ê·¸ë¦¼â€™ë„ ì €ì)
ROLE_AUTHOR_LABELS = (
    r"(?:ì§€ì€ì´|ì €ì|ì €|ì €ìˆ |ì§‘í•„|ì›ì‘|ì›ì €|"
    r"ê¸€|ê¸€ì“´ì´|ê¸€ì‘ê°€|ìŠ¤í† ë¦¬|ê°ìƒ‰|ë§Œí™”|"
    r"ê·¸ë¦¼|ê·¸ë¦¼ì‘ê°€|ì‚½í™”|ì¼ëŸ¬ìŠ¤íŠ¸(?:ë ˆì´í„°)?|ê·¸ë¦°|"
    r"ê¸€\s*[\u00B7Â·/,\+]\s*ê·¸ë¦¼|ê·¸ë¦¼\s*[\u00B7Â·/,\+]\s*ê¸€|ê¸€\s*ê·¸ë¦¼|ê¸€ê·¸ë¦¼)"
)
# ì—­ì ë¼ë²¨(ì¶•ì•½ â€˜ì—­â€™ í¬í•¨)
ROLE_TRANS_LABELS = r"(?:ì˜®ê¸´ì´|ì˜®ê¹€|ì—­ì|ì—­|ë²ˆì—­ì?|ì—­í•´|ì—­ì£¼|ê³µì—­)"

# ë§ë¯¸ ì—­í• (â€˜ì´ë¦„ ì—­í• â€™)
ROLE_AUTHOR_TRAIL = (
    r"(?:ê¸€|ì§€ìŒ|ì§€ì€ì´|ì €ì|ì €|ì €ìˆ |ì§‘í•„|ì›ì‘|ì›ì €|"
    r"ê·¸ë¦¼|ê·¸ë¦¼ì‘ê°€|ì‚½í™”|ì¼ëŸ¬ìŠ¤íŠ¸(?:ë ˆì´í„°)?|ê·¸ë¦°|ìŠ¤í† ë¦¬|ê°ìƒ‰|ë§Œí™”)"
)
ROLE_TRANS_TRAIL = r"(?:ì˜®ê¹€|ë²ˆì—­|ë²ˆì—­ì|ì—­ì|ì—­|ì—­í•´|ì—­ì£¼|ê³µì—­)"

def _strip_trailing_role(piece: str) -> str:
    return re.sub(
        rf"\s+(?:{ROLE_AUTHOR_TRAIL}|{ROLE_TRANS_TRAIL})\s*[\)\].,;:]*$",
        "", piece, flags=re.IGNORECASE
    ).strip()

def split_authors_translators(nlk_author_raw: str):
    """AUTHOR ë¬¸ìì—´ì„ ì €ì/ì—­ì ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬"""
    if not nlk_author_raw:
        return [], []
    s = re.sub(r"\s+", " ", nlk_author_raw.strip())
    # ê´„í˜¸í˜• ì—­í•  â†’ ë§ë¯¸ ë…¸ì¶œ
    s = re.sub(
        rf"\(\s*({ROLE_AUTHOR_LABELS}|{ROLE_TRANS_LABELS})\s*\)",
        lambda m: " " + m.group(1), s, flags=re.IGNORECASE
    )
    authors, translators = [], []
    groups = [g.strip() for g in re.split(r"\s*;\s*", s) if g.strip()]
    for g in groups:
        # ë ˆì´ë¸”í˜•
        m_lab = re.match(
            rf"(?P<label>{ROLE_AUTHOR_LABELS}|{ROLE_TRANS_LABELS})\s*:\s*(?P<names>.+)$",
            g, flags=re.IGNORECASE
        )
        if m_lab:
            label = m_lab.group("label"); names_part = m_lab.group("names")
            parts = [p.strip() for p in SEP_PATTERN.split(names_part) if p.strip()]
            (authors if re.match(ROLE_AUTHOR_LABELS, label, re.IGNORECASE) else translators).extend(parts)
            continue
        # ë§ë¯¸í˜•/ë¬´í‘œì‹œ
        chunks = [p.strip() for p in SEP_PATTERN.split(g) if p.strip()]
        for ch in chunks:
            is_author = bool(re.search(rf"\s+{ROLE_AUTHOR_TRAIL}$", ch, re.IGNORECASE) or
                             re.search(ROLE_AUTHOR_LABELS, ch, re.IGNORECASE))
            is_trans  = bool(re.search(rf"\s+{ROLE_TRANS_TRAIL}$", ch, re.IGNORECASE) or
                             re.search(ROLE_TRANS_LABELS, ch, re.IGNORECASE))
            base = _strip_trailing_role(ch)
            if is_author and not is_trans:
                authors.append(base)
            elif is_trans and not is_author:
                translators.append(base)
            else:
                authors.append(base)  # ë¬´í‘œì‹œëŠ” ê¸°ë³¸ ì €ì
    # ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°
    seen = set(); authors = [x for x in authors if not (x in seen or seen.add(x))]
    seen = set(); translators = [x for x in translators if not (x in seen or seen.add(x))]
    return authors, translators

def parse_nlk_authors(nlk_author_raw: str):
    """ì—­í• ì–´ ì œê±° í›„, ì‚¬ëŒ ì´ë¦„ë§Œ(ì €ì/ì—­ì í•©ì³ì„œ) ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ â†’ 700 ìƒì„±ìš©"""
    if not nlk_author_raw:
        return []
    s = nlk_author_raw
    ROLE_ANY_LABELS = rf"(?:{ROLE_AUTHOR_LABELS}|{ROLE_TRANS_LABELS})"
    ROLE_ANY_TRAIL  = rf"(?:{ROLE_AUTHOR_TRAIL}|{ROLE_TRANS_TRAIL})"
    # ë ˆì´ë¸”í˜•/ê´„í˜¸í˜•/ë§ë¯¸í˜• ì—­í• ì–´ ì œê±°
    s = re.sub(rf"{ROLE_ANY_LABELS}\s*:\s*", "", s, flags=re.IGNORECASE)
    s = re.sub(rf"\(\s*{ROLE_ANY_LABELS}\s*\)", "", s, flags=re.IGNORECASE)
    s = re.sub(rf"\s+{ROLE_ANY_TRAIL}(?=$|[\s),.;/|])", "", s, flags=re.IGNORECASE)
    # ì‚¬ëŒ ë‹¨ìœ„ ë¶„ë¦¬
    chunks = [c for c in SEP_PATTERN.split(s) if c and c.strip()]
    return [re.sub(r"\s+", " ", c).strip() for c in chunks]

def build_700_from_nlk_author(nlk_author_raw: str, *, aladin_item: dict | None = None):
    authors, translators = split_authors_translators(nlk_author_raw)
    names = authors + translators  # ì €ìë“¤ â†’ ì—­ìë“¤ ìˆœì„œ
    lines = []
    for nm in names:
        if not nm.strip():
            continue
        fixed = reorder_hangul_name_for_700(nm, aladin_item=aladin_item)
        lines.append(f"=700  1\\$a{fixed}")
    return lines

# =========================
# ğŸ§± 245 (ì•Œë¼ë”˜ $a/$b) + ì±…ì„í‘œì‹œ(ì €ì/ì—­ì ê·œì¹™) + 700 â†’ MRK
# =========================
def build_245_with_people_from_sources(aladin_item: dict, nlk_author_raw: str, prefer="aladin") -> str:
    tb = extract_245_from_aladin_item(aladin_item, collapse_a_spaces=False)  # ê¸°ì¡´ ì œëª©/ë¶€ì œ êµ¬ì„± í•¨ìˆ˜ ì‚¬ìš©
    a_out, b, n = tb["a"], tb.get("b"), tb.get("n")

    line = f"=245  00$a{a_out}"
    if n: line += (" " if a_out.endswith(".") else " .") + f"$n{n}"
    if b: line += f" :$b{b}"

    people = extract_people_from_aladin(aladin_item) if (prefer=="aladin" and aladin_item) else None
    authors = (people or {}).get("author", [])
    trans   = (people or {}).get("translator", [])

    if not (authors or trans):
        parsed = parse_people_flexible(nlk_author_raw or "")
        authors, trans = parsed.get("author", []), parsed.get("translator", [])

    if authors:
        head, tail = authors[0], authors[1:]
        line += f" /$d{head}"
        for t in tail: line += f", $e{t}"
        line += " ì§€ìŒ"

    if trans:
        line += f" ;$e{trans[0]}"
        for t in trans[1:]: line += f", $e{t}"
        line += " ì˜®ê¹€"

    return line


def build_700_people_pref_aladin(author_raw: str, aladin_item: dict):
    people = extract_people_from_aladin(aladin_item) if aladin_item else {}
    if people.get("author") or people.get("translator"):
        return build_700_from_people(people, reorder_fn=reorder_hangul_name_for_700, aladin_item=aladin_item)
    if author_raw:
        parsed = parse_people_flexible(author_raw)
        return build_700_from_people(parsed, reorder_fn=reorder_hangul_name_for_700, aladin_item=aladin_item)
    return []

# ì´ë¦„ ë’¤ì— ì—­í•  ê¼¬ë¦¬í‘œ ì œê±°ìš©
_ROLE_SUFFIX_RX = re.compile(r"\s*(ì§€ìŒ|ì§€ì€ì´|ì—®ìŒ|ì˜®ê¹€|ì—­|í¸|ê¸€|ê·¸ë¦¼)\s*$")

def _strip_role_suffix(s: str) -> str:
    return _ROLE_SUFFIX_RX.sub("", (s or "").strip())

def extract_primary_author_ko_from_aladin(item: dict) -> str:
    """
    ì•Œë¼ë”˜ itemì—ì„œ 'ì²« ì €ì(ì§€ì€ì´)' í•œê¸€ í‘œê¸°ë¥¼ ì¶”ì¶œí•œë‹¤.
    ì˜ˆ) "ë„ìŠ¤í† ì˜™ìŠ¤í‚¤ (ì§€ì€ì´), ì´ì •ì‹ (ì˜®ê¸´ì´)" â†’ "ë„ìŠ¤í† ì˜™ìŠ¤í‚¤"
    ìš°ì„ ìˆœìœ„: subInfo.authors ë°°ì—´(ì§€ì€ì´/ì €ì) â†’ ì „ì²´ author ë¬¸ìì—´ íŒŒì‹±
    """
    if not item:
        return ""

    sub = (item.get("subInfo") or {})

    # 1) êµ¬ì¡°í™”ëœ authors ë°°ì—´ ìš°ì„ 
    authors_list = sub.get("authors")
    if isinstance(authors_list, list) and authors_list:
        # (1) authorTypeNameì— 'ì§€ì€ì´' ë˜ëŠ” 'ì €ì' í¬í•¨ ì°¾ê¸°
        for a in authors_list:
            atype = (a.get("authorTypeName") or a.get("authorType") or "").strip()
            nm = (a.get("authorName") or a.get("name") or "").strip()
            if not nm:
                continue
            if ("ì§€ì€ì´" in atype) or ("ì €ì" in atype):
                return _strip_role_suffix(nm)
        # (2) ëª» ì°¾ìœ¼ë©´ ì²« í•­ëª©ì˜ ì´ë¦„
        first = (authors_list[0].get("authorName") or authors_list[0].get("name") or "").strip()
        return _strip_role_suffix(first)

    # 2) ë¬¸ìì—´ í•„ë“œ íŒŒì‹± (ì˜ˆ: "ë„ìŠ¤í† ì˜™ìŠ¤í‚¤ (ì§€ì€ì´), ì´ì •ì‹ (ì˜®ê¸´ì´)")
    author_str = (item.get("author") or "").strip()
    if author_str:
        first_seg = author_str.split(",")[0]
        # ëì˜ "(ì—­ì)" "(ì§€ì€ì´)" ë“± ê´„í˜¸ ì—­í•  ì œê±°
        first = re.sub(r"\s*\(.*?\)\s*$", "", first_seg).strip()
        # ì—­í•  ê¼¬ë¦¬í‘œ(ì§€ìŒ/ì˜®ê¹€ ë“±) ì œê±°
        first = _strip_role_suffix(first)
        return first

    return ""

def build_049(reg_mark: str, reg_no: str, copy_symbol: str) -> str:
    """
    049 ì†Œì¥ì‚¬í•­ í•„ë“œ ìƒì„±
    - $I ë“±ë¡ê¸°í˜¸+ë“±ë¡ë²ˆí˜¸
    - $f ë³„ì¹˜ê¸°í˜¸ (ìˆì„ ë•Œë§Œ)
    """
    reg_mark = (reg_mark or "").strip()
    reg_no = (reg_no or "").strip()
    copy_symbol = (copy_symbol or "").strip()

    if not (reg_mark or reg_no):
        return ""  # ë“±ë¡ê¸°í˜¸+ë“±ë¡ë²ˆí˜¸ ì—†ìœ¼ë©´ ìƒì„± ì•ˆ í•¨

    field = f"=049  \\\\$I{reg_mark}{reg_no}"
    if copy_symbol:
        field += f"$f{copy_symbol}"
    return field

# --- 700 ë™ì•„ì‹œì•„ ë³´ì •ì— í•„ìš”í•œ ì „ì—­/í—¬í¼  ---
# 900 ìƒì„± ë•Œ ìŒ“ëŠ” provenanceê°€ ë¹„ì–´ ìˆì–´ë„ ì•ˆì „í•˜ê²Œ ê¸°ë³¸ê°’ ë³´ì¥
try:
    LAST_PROV_90010
except NameError:
    LAST_PROV_90010 = []

# ë™ì•„ì‹œì•„ êµ­ê°€(QID) ì„¸íŠ¸
_EAST_ASIAN_P27 = {"Q17","Q148","Q884","Q423","Q865","Q864","Q14773"}

def _east_asian_konames_from_prov(prov900: list[dict]) -> set[str]:
    """
    900 provenanceì—ì„œ ë™ì•„ì‹œì•„ êµ­ì (P27)ì´ í™•ì¸ëœ ì¸ë¬¼ì˜ í•œê¸€í‘œê¸°('who') ì§‘í•©ì„ ë§Œë“ ë‹¤.
    P27 ì¡°íšŒ í•¨ìˆ˜(_wd_get_p27_list)ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•˜ë©´ ì¡°ìš©íˆ ê±´ë„ˆëœ€(ì•ˆì „).
    """
    out = set()
    if not prov900:
        return out
    for t in prov900:
        try:
            prov = t.get("provenance") if isinstance(t, dict) else None
            qid = None
            if isinstance(prov, dict):
                qid = prov.get("qid") or (prov.get("provenance") or {}).get("qid")
            who = (t.get("who") or "").strip()
            if not (qid and who):
                continue
            # êµ­ì (P27) ì²´í¬ (í•¨ìˆ˜ ì¡´ì¬ ì‹œ)
            is_east = False
            try:
                p27s = _wd_get_p27_list(qid)  # ì—†ìœ¼ë©´ exceptë¡œ ë„˜ì–´ê°
                if p27s and any(c in _EAST_ASIAN_P27 for c in p27s):
                    is_east = True
            except Exception:
                # P27 ì¡°íšŒ ë¶ˆê°€ ì‹œì—” ë³´ìˆ˜ì ìœ¼ë¡œ 'ë™ì•„ì‹œì•„ ì•„ë‹˜' ì²˜ë¦¬
                is_east = False
            if is_east:
                out.add(who)
        except Exception:
            # provenance í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¬ë¼ë„ ì „ì²´ ì‹¤íŒ¨í•˜ì§€ ì•Šë„ë¡ ë¬´ì‹œ
            pass
    return out

def _fix_700_order_with_nationality(lines: list[str], east_konames: set[str]) -> list[str]:
    """
    700 ë¼ì¸ì—ì„œ 'ì´ë¦„, ì„±' í˜•íƒœê°€ ìˆì„ ë•Œ,
    (who ê¸°ì¤€) ë™ì•„ì‹œì•„ ì¸ë¬¼ì€ 'ì„± ì´ë¦„'(ì‰¼í‘œ ì—†ìŒ)ìœ¼ë¡œ ë³´ì •í•œë‹¤.
    """
    if not lines or not east_konames:
        return lines or []

    import re
    patt = re.compile(r"^(=700\s\s1\\?\$a)([^,]+),\s*([^$\r\n]+)(.*)$")
    out = []
    for ln in lines:
        m = patt.match(ln)
        if not m:
            out.append(ln)
            continue
        prefix, left, right, suffix = m.groups()  # left=ì´ë¦„, right=ì„± (í•œê¸€)
        candidate = f"{right.strip()} {left.strip()}"  # 'ì„± ì´ë¦„'
        if candidate in east_konames:
            out.append(f"{prefix}{candidate}{suffix}")
        else:
            out.append(ln)
    return out


# ===== í™˜ê²½ë³€ìˆ˜ ë¡œë“œ =====
load_dotenv()
ALADIN_KEY = os.getenv("ALADIN_TTB_KEY")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_KEY)

# ===== ISDS ì–¸ì–´ì½”ë“œ ë§¤í•‘ =====
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´',
    'rus': 'ëŸ¬ì‹œì•„ì–´', 'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´',
    'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´', 'por': 'í¬ë¥´íˆ¬ê°ˆì–´', 'tur': 'í„°í‚¤ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}
ALLOWED_CODES = set(ISDS_LANGUAGE_CODES.keys()) - {"und"}

# ===== ê³µí†µ ìœ í‹¸: GPT ì‘ë‹µ íŒŒì‹±(ì½”ë“œ + ì´ìœ ) =====
def _extract_code_and_reason(content, code_key="$h"):
    code, reason, signals = "und", "", ""
    lines = [l.strip() for l in (content or "").splitlines() if l.strip()]
    for ln in lines:
        if ln.startswith(f"{code_key}="):
            code = ln.split("=", 1)[1].strip()
        elif ln.lower().startswith("#reason="):
            reason = ln.split("=", 1)[1].strip()
        elif ln.lower().startswith("#signals="):
            signals = ln.split("=", 1)[1].strip()
    return code, reason, signals

# ===== GPT íŒë‹¨ í•¨ìˆ˜ (ì›ì„œ; ì¼ë°˜) =====
def gpt_guess_original_lang(title, category, publisher, author="", original_title=""):
    prompt = f"""
    ì•„ë˜ ë„ì„œì˜ ì›ì„œ ì–¸ì–´(041 $h)ë¥¼ ISDS ì½”ë“œë¡œ ì¶”ì •í•´ì¤˜.
    ê°€ëŠ¥í•œ ì½”ë“œ: kor, eng, jpn, chi, rus, fre, ger, ita, spa, por, tur

    ë„ì„œì •ë³´:
    - ì œëª©: {title}
    - ì›ì œ: {original_title or "(ì—†ìŒ)"}
    - ë¶„ë¥˜: {category}
    - ì¶œíŒì‚¬: {publisher}
    - ì €ì: {author}

    ì§€ì¹¨:
    - êµ­ê°€/ì§€ì—­ì„ ì–¸ì–´ë¡œ ê³§ë°”ë¡œ ì¹˜í™˜í•˜ì§€ ë§ ê²ƒ.
    - ì €ì êµ­ì Â·ì£¼ ì§‘í•„ ì–¸ì–´Â·ìµœì´ˆ ì¶œê°„ ì–¸ì–´ë¥¼ ìš°ì„  ê³ ë ¤.
    - ë¶ˆí™•ì‹¤í•˜ë©´ ì„ì˜ ì¶”ì • ëŒ€ì‹  'und' ì‚¬ìš©.

    ì¶œë ¥í˜•ì‹(ì •í™•íˆ ì´ 2~3ì¤„):
    $h=[ISDS ì½”ë“œ]
    #reason=[ì§§ê²Œ ê·¼ê±° ìš”ì•½]
    #signals=[ì¡ì€ ë‹¨ì„œë“¤, ì½¤ë§ˆë¡œ](ì„ íƒ)
    """.strip()
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system","content":"ì‚¬ì„œìš© ì–¸ì–´ ì¶”ì •ê¸°"},
                      {"role":"user","content":prompt}],
            temperature=0
        )
        content = (resp.choices[0].message.content or "").strip()
        code, reason, signals = _extract_code_and_reason(content, "$h")
        if code not in ALLOWED_CODES:
            code = "und"
        dbg(f"ğŸ§­ [GPT ê·¼ê±°] $h={code}")
        if reason: dbg(f"ğŸ§­ [ì´ìœ ] {reason}")
        if signals: dbg(f"ğŸ§­ [ë‹¨ì„œ] {signals}")
        return code
    except Exception as e:
        dbg_error(f"GPT ì˜¤ë¥˜: {e}")
        return "und"

# ===== GPT íŒë‹¨ í•¨ìˆ˜ (ë³¸ë¬¸) =====
def gpt_guess_main_lang(title, category, publisher):
    prompt = f"""
    ì•„ë˜ ë„ì„œì˜ ë³¸ë¬¸ ì–¸ì–´(041 $a)ë¥¼ ISDS ì½”ë“œë¡œ ì¶”ì •.
    ê°€ëŠ¥í•œ ì½”ë“œ: kor, eng, jpn, chi, rus, fre, ger, ita, spa, por, tur

    ì…ë ¥:
    - ì œëª©: {title}
    - ë¶„ë¥˜: {category}
    - ì¶œíŒì‚¬: {publisher}

    ì§€ì¹¨:
    - 'ë³¸ë¬¸ ì–¸ì–´'ëŠ” ì´ ìë£Œì˜ **í˜„ì‹œë³¸(Manifestation)** ì–¸ì–´ë‹¤.
    - ì €ì êµ­ì , ì›ì‘ ì–¸ì–´, ì‹œë¦¬ì¦ˆ ì›ì‚°ì§€ ë“± **ì›ì‘ ê´€ë ¨ ë‹¨ì„œ ì‚¬ìš© ê¸ˆì§€**.
    - ì¹´í…Œê³ ë¦¬ì— 'êµ­ë‚´ë„ì„œ'ê°€ ìˆê±°ë‚˜, ì œëª©ì— **í•œê¸€ì´ 1ìë¼ë„** í¬í•¨ë˜ë©´ ë°˜ë“œì‹œ kor.
    - í—ˆìš© ì½”ë“œ ë°–ì´ê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ë©´ 'und'.

    ì¶œë ¥í˜•ì‹:
    $a=[ISDS ì½”ë“œ]
    #reason=[ì§§ê²Œ ê·¼ê±° ìš”ì•½]
    #signals=[ì¡ì€ ë‹¨ì„œë“¤, ì½¤ë§ˆë¡œ](ì„ íƒ)
    """.strip()
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system","content":"ì‚¬ì„œìš© ë³¸ë¬¸ ì–¸ì–´ ì¶”ì •ê¸°"},
                      {"role":"user","content":prompt}],
            temperature=0
        )
        content = (resp.choices[0].message.content or "").strip()
        code, reason, signals = _extract_code_and_reason(content, "$a")
        if code not in ALLOWED_CODES:
            code = "und"
        st.write(f"ğŸ§­ [GPT ê·¼ê±°] $a={code}")
        if reason: st.write(f"ğŸ§­ [ì´ìœ ] {reason}")
        if signals: st.write(f"ğŸ§­ [ë‹¨ì„œ] {signals}")
        return code
    except Exception as e:
        st.error(f"GPT ì˜¤ë¥˜: {e}")
        return "und"

# ===== GPT íŒë‹¨ í•¨ìˆ˜ (ì‹ ê·œ) â€” ì €ì ê¸°ë°˜ ì›ì„œ ì–¸ì–´ ì¶”ì • =====
def gpt_guess_original_lang_by_author(author, title="", category="", publisher=""):
    prompt = f"""
    ì €ì ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì›ì„œ ì–¸ì–´(041 $h)ë¥¼ ISDS ì½”ë“œë¡œ ì¶”ì •.
    ê°€ëŠ¥í•œ ì½”ë“œ: kor, eng, jpn, chi, rus, fre, ger, ita, spa, por, tur

    ì…ë ¥:
    - ì €ì: {author}
    - (ì°¸ê³ ) ì œëª©: {title}
    - (ì°¸ê³ ) ë¶„ë¥˜: {category}
    - (ì°¸ê³ ) ì¶œíŒì‚¬: {publisher}

    ì§€ì¹¨:
    - ì €ì êµ­ì Â·ì£¼ ì§‘í•„ ì–¸ì–´Â·ëŒ€í‘œ ì‘í’ˆ ì›ì–´ë¥¼ ìš°ì„ .
    - êµ­ê°€=ì–¸ì–´ ë‹¨ìˆœ ì¹˜í™˜ ê¸ˆì§€.
    - ë¶ˆí™•ì‹¤í•˜ë©´ 'und'.

    ì¶œë ¥í˜•ì‹:
    $h=[ISDS ì½”ë“œ]
    #reason=[ì§§ê²Œ ê·¼ê±° ìš”ì•½]
    #signals=[ì¡ì€ ë‹¨ì„œë“¤, ì½¤ë§ˆë¡œ](ì„ íƒ)
    """.strip()
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role":"system","content":"ì €ì ê¸°ë°˜ ì›ì„œ ì–¸ì–´ ì¶”ì •ê¸°"},
                      {"role":"user","content":prompt}],
            temperature=0
        )
        content = (resp.choices[0].message.content or "").strip()
        code, reason, signals = _extract_code_and_reason(content, "$h")
        if code not in ALLOWED_CODES:
            code = "und"
        st.write(f"ğŸ§­ [ì €ìê¸°ë°˜ ê·¼ê±°] $h={code}")
        if reason: st.write(f"ğŸ§­ [ì´ìœ ] {reason}")
        if signals: st.write(f"ğŸ§­ [ë‹¨ì„œ] {signals}")
        return code
    except Exception as e:
        st.error(f"GPT(ì €ìê¸°ë°˜) ì˜¤ë¥˜: {e}")
        return "und"

# ===== ì–¸ì–´ ê°ì§€ í•¨ìˆ˜ë“¤ =====
def detect_language_by_unicode(text):
    text = re.sub(r'[\s\W_]+', '', text or "")
    if not text:
        return 'und'
    c = text[0]
    if '\uac00' <= c <= '\ud7a3': return 'kor'
    if '\u3040' <= c <= '\u30ff': return 'jpn'
    if '\u4e00' <= c <= '\u9fff': return 'chi'
    if '\u0600' <= c <= '\u06FF': return 'ara'
    if '\u0e00' <= c <= '\u0e7f': return 'tha'
    return 'und'

def override_language_by_keywords(text, initial_lang):
    text = (text or "").lower()
    if initial_lang == 'chi' and re.search(r'[\u3040-\u30ff]', text): return 'jpn'
    if initial_lang in ['und', 'eng']:
        if "spanish" in text or "espaÃ±ol" in text: return "spa"
        if "italian" in text or "italiano" in text: return "ita"
        if "french" in text or "franÃ§ais" in text: return "fre"
        if "portuguese" in text or "portuguÃªs" in text: return "por"
        if "german" in text or "deutsch" in text: return "ger"
        if any(ch in text for ch in ['Ã©','Ã¨','Ãª','Ã ','Ã§','Ã¹','Ã´','Ã¢','Ã®','Ã»']): return "fre"
        if any(ch in text for ch in ['Ã±','Ã¡','Ã­','Ã³','Ãº']): return "spa"
        if any(ch in text for ch in ['Ã£','Ãµ']): return "por"
    return initial_lang

def detect_language(text):
    lang = detect_language_by_unicode(text)
    return override_language_by_keywords(text, lang)

def detect_language_from_category(text):
    words = re.split(r'[>/\s]+', text or "")
    for w in words:
        if "ì¼ë³¸" in w: return "jpn"
        if "ì¤‘êµ­" in w: return "chi"
        if "ì˜ë¯¸" in w or "ì˜ì–´" in w or "ì•„ì¼ëœë“œ" in w: return "eng"
        if "í”„ë‘ìŠ¤" in w: return "fre"
        if "ë…ì¼" in w or "ì˜¤ìŠ¤íŠ¸ë¦¬ì•„" in w: return "ger"
        if "ëŸ¬ì‹œì•„" in w: return "rus"
        if "ì´íƒˆë¦¬ì•„" in w: return "ita"
        if "ìŠ¤í˜ì¸" in w: return "spa"
        if "í¬ë¥´íˆ¬ê°ˆ" in w: return "por"
        if "íŠ€ë¥´í‚¤ì˜ˆ" in w or "í„°í‚¤" in w: return "tur"
    return None

# ===== ì¹´í…Œê³ ë¦¬ í† í¬ë‚˜ì´ì¦ˆ & íŒì • ìœ í‹¸ =====
def tokenize_category(text: str):
    if not text:
        return []
    t = re.sub(r'[()]+', ' ', text)
    raw = re.split(r'[>/\s]+', t)
    tokens = []
    for w in raw:
        w = w.strip()
        if not w:
            continue
        if '/' in w and w.count('/') <= 3 and len(w) <= 20:
            tokens.extend([p for p in w.split('/') if p])
        else:
            tokens.append(w)
    lower_tokens = tokens + [w.lower() for w in tokens if any('A'<=ch<='Z' or 'a'<=ch<='z' for ch in w)]
    return lower_tokens

def has_kw_token(tokens, kws):
    s = set(tokens)
    return any(k in s for k in kws)

def trigger_kw_token(tokens, kws):
    s = set(tokens)
    for k in kws:
        if k in s:
            return k
    return None

def is_literature_top(category_text: str) -> bool:
    return "ì†Œì„¤/ì‹œ/í¬ê³¡" in (category_text or "")

def is_literature_category(category_text: str) -> bool:
    tokens = tokenize_category(category_text or "")
    ko_hits = ["ë¬¸í•™", "ì†Œì„¤", "ì‹œ", "í¬ê³¡"]
    en_hits = ["literature", "fiction", "novel", "poetry", "poem", "drama", "play"]
    return has_kw_token(tokens, ko_hits) or has_kw_token(tokens, en_hits)

def is_nonfiction_override(category_text: str) -> bool:
    """
    ë¬¸í•™ì²˜ëŸ¼ ë³´ì—¬ë„ 'ì—­ì‚¬/ì§€ì—­/ì „ê¸°/ì‚¬íšŒê³¼í•™/ì—ì„¸ì´' ë“± ë¹„ë¬¸í•™ ì§€í‘œê°€ ìˆìœ¼ë©´ ë¹„ë¬¸í•™ìœ¼ë¡œ ê°•ì œ.
    ë‹¨, ë¬¸í•™ ìµœìƒìœ„(ì†Œì„¤/ì‹œ/í¬ê³¡)ë©´ 'ê³¼í•™/ê¸°ìˆ 'ì€ ì œì™¸(SF ë³´í˜¸).
    """
    tokens = tokenize_category(category_text or "")
    lit_top = is_literature_top(category_text or "")

    ko_nf_strict = ["ì—­ì‚¬","ê·¼í˜„ëŒ€ì‚¬","ì„œì–‘ì‚¬","ìœ ëŸ½ì‚¬","ì „ê¸°","í‰ì „",
                    "ì‚¬íšŒ","ì •ì¹˜","ì² í•™","ê²½ì œ","ê²½ì˜","ì¸ë¬¸","ì—ì„¸ì´","ìˆ˜í•„"]
    en_nf_strict = ["history","biography","memoir","politics","philosophy",
                    "economics","science","technology","nonfiction","essay","essays"]

    sci_keys = ["ê³¼í•™","ê¸°ìˆ "]; sci_keys_en = ["science","technology"]

    k = trigger_kw_token(tokens, ko_nf_strict) or trigger_kw_token(tokens, en_nf_strict)
    if k:
        dbg(f"ğŸ” [íŒì •ê·¼ê±°] ë¹„ë¬¸í•™ í‚¤ì›Œë“œ ë°œê²¬: '{k}'")
        return True

    if not lit_top:
        k2 = trigger_kw_token(tokens, sci_keys) or trigger_kw_token(tokens, sci_keys_en)
        if k2:
            dbg(f"ğŸ” [íŒì •ê·¼ê±°] ë¹„ë¬¸í•™ ìµœìƒìœ„ ì¶”ì • & '{k2}' ë°œê²¬ â†’ ë¹„ë¬¸í•™ ì˜¤ë²„ë¼ì´ë“œ")
            return True

    if lit_top:
        dbg("ğŸ” [íŒì •ê·¼ê±°] ë¬¸í•™ ìµœìƒìœ„ ê°ì§€: 'ê³¼í•™/ê¸°ìˆ 'ì€ ì˜¤ë²„ë¼ì´ë“œì—ì„œ ì œì™¸(SF ë³´í˜¸).")
    return False

# ===== ê¸°íƒ€ ìœ í‹¸ =====
def strip_ns(tag): return tag.split('}')[-1] if '}' in tag else tag

def generate_546_from_041_kormarc(marc_041):
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"): a_codes.append(part[2:])
        elif part.startswith("$h"): h_code = part[2:]
    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{h_lang}ì›ì‘ì„ {a_lang}ë¡œ ë²ˆì—­"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"
    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"

# ===== ì›¹ í¬ë¡¤ë§ =====
def crawl_aladin_fallback(isbn13):
    url = f"https://www.aladin.co.kr/shop/wproduct.aspx?ISBN={isbn13}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        original = soup.select_one("div.info_original")
        lang_info = soup.select_one("div.conts_info_list1")
        category_text = ""
        categories = soup.select("div.conts_info_list2 li")
        for cat in categories:
            category_text += cat.get_text(separator=" ", strip=True) + " "
        detected_lang = ""
        if lang_info and "ì–¸ì–´" in lang_info.text:
            if "Japanese" in lang_info.text: detected_lang = "jpn"
            elif "Chinese" in lang_info.text: detected_lang = "chi"
            elif "English" in lang_info.text: detected_lang = "eng"
        return {
            "original_title": original.text.strip() if original else "",
            "subject_lang": detect_language_from_category(category_text) or detected_lang,
            "category_text": category_text
        }
    except Exception as e:
        dbg_error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}

# ===== ê²°ê³¼ ì¡°ì •(ì¶©ëŒ í•´ì†Œ) =====
def reconcile_language(candidate, fallback_hint=None, author_hint=None):
    """
    candidate: 1ì°¨ GPT ê²°ê³¼
    fallback_hint: ì¹´í…Œê³ ë¦¬/ì›ì œ ê·œì¹™ì—ì„œ ì–»ì€ íŒíŠ¸(ì˜ˆ: 'ger')
    author_hint: ì €ì ê¸°ë°˜ GPT ê²°ê³¼
    """
    if author_hint and author_hint != "und" and author_hint != candidate:
        st.write(f"ğŸ” [ì¡°ì •] ì €ìê¸°ë°˜({author_hint}) â‰  1ì°¨({candidate}) â†’ ì €ìê¸°ë°˜ ìš°ì„ ")
        return author_hint
    if fallback_hint and fallback_hint != "und" and fallback_hint != candidate:
        if candidate in {"ita","fre","spa","por"}:
            st.write(f"ğŸ” [ì¡°ì •] ê·œì¹™íŒíŠ¸({fallback_hint}) vs 1ì°¨({candidate}) â†’ ê·œì¹™íŒíŠ¸ ìš°ì„ ")
            return fallback_hint
    return candidate

# ===== $h ìš°ì„ ìˆœìœ„ ê²°ì • (ì €ì ê¸°ë°˜ ë³´ì • + ê·¼ê±° ë¡œê¹… í¬í•¨) =====
def determine_h_language(
    title: str,
    original_title: str,
    category_text: str,
    publisher: str,
    author: str,
    subject_lang: str
) -> str:
    """
    ë¬¸í•™: ì¹´í…Œê³ ë¦¬/ì›¹ â†’ (ë¶€ì¡±ì‹œ) GPT â†’ (ì—¬ì „íˆ ë¶ˆí™•ì‹¤) ì €ì ê¸°ë°˜ ë³´ì •
    ë¹„ë¬¸í•™: GPT â†’ (ë¶€ì¡±ì‹œ) ì¹´í…Œê³ ë¦¬/ì›¹ â†’ (ì—¬ì „íˆ ë¶ˆí™•ì‹¤) ì €ì ê¸°ë°˜ ë³´ì •
    """
    lit_raw = is_literature_category(category_text)
    nf_override = is_nonfiction_override(category_text)
    is_lit_final = lit_raw and not nf_override

    # ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ì„¤ëª…
    if lit_raw and not nf_override:
        dbg("ğŸ“˜ [íŒì •] ì´ ìë£ŒëŠ” ë¬¸í•™(ì†Œì„¤/ì‹œ/í¬ê³¡ ë“±) ì„±ê²©ì´ ëšœë ·í•©ë‹ˆë‹¤.")
    elif lit_raw and nf_override:
        dbg("ğŸ“˜ [íŒì •] ê²‰ë³´ê¸°ì—ëŠ” ë¬¸í•™ì´ì§€ë§Œ, 'ì—­ì‚¬Â·ì—ì„¸ì´Â·ì‚¬íšŒê³¼í•™' ë“± ë¹„ë¬¸í•™ ìš”ì†Œê°€ í•¨ê»˜ ë³´ì—¬ ìµœì¢…ì ìœ¼ë¡œëŠ” ë¹„ë¬¸í•™ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif not lit_raw and nf_override:
        dbg("ğŸ“˜ [íŒì •] ë¬¸í•™ì  ë‹¨ì„œëŠ” ì—†ê³ , ë¹„ë¬¸í•™(ì—­ì‚¬Â·ì‚¬íšŒÂ·ì² í•™ ë“±) ì„±ê²©ì´ ê°•í•©ë‹ˆë‹¤.")
    else:
        dbg("ğŸ“˜ [íŒì •] ë¬¸í•™/ë¹„ë¬¸í•™ íŒë‹¨ ë‹¨ì„œê°€ ì•½í•´ ì¶”ê°€ íŒë‹¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    rule_from_original = detect_language(original_title) if original_title else "und"
    lang_h = None
    author_hint = None

    if is_lit_final:
        # ë¬¸í•™: 1) ì¹´í…Œê³ ë¦¬/ì›¹ â†’ 2) ì›ì œ ìœ ë‹ˆì½”ë“œ â†’ 3) GPT â†’ 4) ì €ì ê¸°ë°˜
        lang_h = subject_lang or rule_from_original
        dbg(f"ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) 1ì°¨ í›„ë³´: {lang_h or 'und'}")
        if not lang_h or lang_h == "und":
            dbg("ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) GPT ë³´ì™„ ì‹œë„â€¦")
            lang_h = gpt_guess_original_lang(title, category_text, publisher, author, original_title)
            dbg(f"ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) GPT ê²°ê³¼: {lang_h}")
        if (not lang_h or lang_h == "und") and author:
            dbg("ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) ì›ì œ ì—†ìŒ/ì• ë§¤ â†’ ì €ì ê¸°ë°˜ ë³´ì • ì‹œë„â€¦")
            author_hint = gpt_guess_original_lang_by_author(author, title, category_text, publisher)
            dbg(f"ğŸ“˜ [ì„¤ëª…] (ë¬¸í•™ íë¦„) ì €ì ê¸°ë°˜ ê²°ê³¼: {author_hint}")
    else:
        # ë¹„ë¬¸í•™: 1) GPT â†’ 2) ì¹´í…Œê³ ë¦¬/ì›¹ â†’ 3) ì›ì œ ìœ ë‹ˆì½”ë“œ â†’ 4) ì €ì ê¸°ë°˜
        dbg("ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) GPT ì„ í–‰ íŒë‹¨â€¦")
        lang_h = gpt_guess_original_lang(title, category_text, publisher, author, original_title)
        dbg(f"ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) GPT ê²°ê³¼: {lang_h or 'und'}")
        if not lang_h or lang_h == "und":
            lang_h = subject_lang or rule_from_original
            dbg(f"ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) ë³´ì¡° ê·œì¹™ ì ìš© â†’ í›„ë³´: {lang_h or 'und'}")
        if author and (not lang_h or lang_h == "und"):
            dbg("ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) ì›ì œ ì—†ìŒ/ì• ë§¤ â†’ ì €ì ê¸°ë°˜ ë³´ì • ì‹œë„â€¦")
            author_hint = gpt_guess_original_lang_by_author(author, title, category_text, publisher)
            dbg(f"ğŸ“˜ [ì„¤ëª…] (ë¹„ë¬¸í•™ íë¦„) ì €ì ê¸°ë°˜ ê²°ê³¼: {author_hint}")

    # ì¶©ëŒ ì¡°ì •
    fallback_hint = subject_lang or rule_from_original
    lang_h = reconcile_language(candidate=lang_h, fallback_hint=fallback_hint, author_hint=author_hint)
    dbg("ğŸ“˜ [ê²°ê³¼] ì¡°ì • í›„ ì›ì„œ ì–¸ì–´(h) =", lang_h)

    return (lang_h if lang_h in ALLOWED_CODES else "und") or "und"

# ===== êµ­ë‚´ë„ì„œ ì—¬ë¶€ ê°€ë“œ =====
def is_domestic_category(category_text: str) -> bool:
    return "êµ­ë‚´ë„ì„œ" in (category_text or "")

# ===== KORMARC íƒœê·¸ ìƒì„±ê¸° =====
def get_kormarc_tags(isbn):
    isbn = isbn.strip().replace("-", "")
    url = "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
    params = {
        "ttbkey": ALADIN_KEY,
        "itemIdType": "ISBN13",
        "ItemId": isbn,
        "output": "xml",
        "Version": "20131101"
    }
    try:
        response = requests.get(url, params=params)
        if response.status_code != 200:
            raise ValueError("API í˜¸ì¶œ ì‹¤íŒ¨")
        root = ET.fromstring(response.content)
        for elem in root.iter():
            elem.tag = strip_ns(elem.tag)
        item = root.find("item")
        if item is None:
            raise ValueError("<item> íƒœê·¸ ì—†ìŒ")

        title = item.findtext("title", default="")
        publisher = item.findtext("publisher", default="")
        author = item.findtext("author", default="")
        subinfo = item.find("subInfo")
        original_title = subinfo.findtext("originalTitle") if subinfo is not None else ""

        crawl = crawl_aladin_fallback(isbn)
        if not original_title:
            original_title = crawl.get("original_title", "")
        subject_lang = crawl.get("subject_lang")
        category_text = crawl.get("category_text", "")

        # ---- $a: ë³¸ë¬¸ ì–¸ì–´ ----
        
        # 1) ê·œì¹™ ê¸°ë°˜ 1ì°¨ ê°ì§€
        lang_a = detect_language(title)
        dbg("ğŸ“˜ [DEBUG] ê·œì¹™ ê¸°ë°˜ 1ì°¨ lang_a =", lang_a)
        # 2) ê°•í•œ ê°€ë“œ: 'êµ­ë‚´ë„ì„œ'ë©´ korë¡œ ê³ ì •
        if is_domestic_category(category_text):
            dbg("ğŸ“˜ [íŒì •] ì¹´í…Œê³ ë¦¬ì— 'êµ­ë‚´ë„ì„œ' ê°ì§€ â†’ $a=kor(ê°•í•œ ê°€ë“œ)")
            lang_a = "kor"
        # 3) GPT ë³´ì¡°: und/engì¼ ë•Œë§Œ í˜¸ì¶œ
        if lang_a in ('und', 'eng'):
            dbg("ğŸ“˜ [ì„¤ëª…] und/eng â†’ GPT ë³´ì¡°ë¡œ ë³¸ë¬¸ ì–¸ì–´ ì¬íŒì •â€¦")
            gpt_a = gpt_guess_main_lang(title, category_text, publisher)
            dbg(f"ğŸ“˜ [ì„¤ëª…] GPT íŒë‹¨ lang_a = {gpt_a}")
            if gpt_a in ALLOWED_CODES:
                lang_a = gpt_a
            else:
                lang_a = "und"

        # ---- $h: ì›ì € ì–¸ì–´ (ì €ì ê¸°ë°˜ ë³´ì • & ê·¼ê±° ë¡œê¹… í¬í•¨) ----
        dbg("ğŸ“˜ [DEBUG] ì›ì œ ê°ì§€ë¨:", bool(original_title), "| ì›ì œ:", original_title or "(ì—†ìŒ)")
        dbg("ğŸ“˜ [DEBUG] ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ lang_h í›„ë³´ =", subject_lang or "(ì—†ìŒ)")
        lang_h = determine_h_language(
            title=title,
            original_title=original_title,
            category_text=category_text,
            publisher=publisher,
            author=author,
            subject_lang=subject_lang
        )
        dbg("ğŸ“˜ [ê²°ê³¼] ìµœì¢… ì›ì„œ ì–¸ì–´(h) =", lang_h)

        # ---- íƒœê·¸ ì¡°í•© ----
        if lang_h and lang_h != lang_a and lang_h != "und":
            tag_041 = f"041 $a{lang_a} $h{lang_h}"
        else:
            tag_041 = f"041 $a{lang_a}"
        tag_546 = generate_546_from_041_kormarc(tag_041)

        return tag_041, tag_546, original_title
    except Exception as e:
        return f"ğŸ“• ì˜ˆì™¸ ë°œìƒ: {e}", "", ""

def _as_mrk_041(tag_041: str | None) -> str | None:
    """
    '041 $akor$hrus' â†’ '=041  0\\$akor$hrus'
    (=041 / 041 ì ‘ë‘ì™€ ì¤‘ê°„ ê³µë°±ì´ ë“¤ì–´ì™€ë„ ì •ê·œí™”)
    """
    if not tag_041:
        return None
    s = tag_041.strip()
    # ì•ì˜ '041' / '=041' ì œê±°
    s = re.sub(r"^=?\s*041\s*", "", s)
    # ì„œë¸Œí•„ë“œ ì‚¬ì´ ê³µë°± ì œê±°
    s = re.sub(r"\s+", "", s)
    if not s.startswith("$a"):
        return None
    return f"=041  0\\{s}"

def _as_mrk_546(tag_546_text: str | None) -> str | None:
    """
    'ëŸ¬ì‹œì•„ì–´ì›ì‘ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­' â†’ '=546  \\\\$aëŸ¬ì‹œì•„ì–´ì›ì‘ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­'
    (ì´ë¯¸ '=546'ë¡œ ì‹œì‘í•˜ë©´ ê·¸ëŒ€ë¡œ)
    """
    if not tag_546_text:
        return None
    t = tag_546_text.strip()
    if not t:
        return None
    if t.startswith("=546"):
        return t
    if t.startswith("$a"):
        return f"=546  \\\\{t}"
    return f"=546  \\\\$a{t}"

# ============================= í•œêµ­ ë°œí–‰ì§€ ë¬¸ìì—´ â†’ KORMARC 3ìë¦¬ ì½”ë“œ (í•„ìš” ì‹œ í™•ì¥)
KR_REGION_TO_CODE = {
    "ì„œìš¸": "ulk", "ì„œìš¸íŠ¹ë³„ì‹œ": "ulk",
    "ê²½ê¸°": "ggk", "ê²½ê¸°ë„": "ggk",
    "ë¶€ì‚°": "bnk", "ë¶€ì‚°ê´‘ì—­ì‹œ": "bnk",
    "ëŒ€êµ¬": "tgk", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "tgk",
    "ì¸ì²œ": "ick", "ì¸ì²œê´‘ì—­ì‹œ": "ick",
    "ê´‘ì£¼": "kjk", "ê´‘ì£¼ê´‘ì—­ì‹œ": "kjk",
    "ëŒ€ì „": "tjk", "ëŒ€ì „ê´‘ì—­ì‹œ": "tjk",
    "ìš¸ì‚°": "usk", "ìš¸ì‚°ê´‘ì—­ì‹œ": "usk",
    "ì„¸ì¢…": "sjk", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "sjk",
    "ê°•ì›": "gak", "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "gak",
    "ì¶©ë¶": "hbk", "ì¶©ì²­ë¶ë„": "hbk",
    "ì¶©ë‚¨": "hck", "ì¶©ì²­ë‚¨ë„": "hck",
    "ì „ë¶": "jbk", "ì „ë¼ë¶ë„": "jbk",
    "ì „ë‚¨": "jnk", "ì „ë¼ë‚¨ë„": "jnk",
    "ê²½ë¶": "gbk", "ê²½ìƒë¶ë„": "gbk",
    "ê²½ë‚¨": "gnk", "ê²½ìƒë‚¨ë„": "gnk",
    "ì œì£¼": "jjk", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "jjk",
}

# ê¸°ë³¸ê°’: ë°œí–‰êµ­/ì–¸ì–´/ëª©ë¡ì „ê±°
COUNTRY_FIXED = "ulk"   # ë°œí–‰êµ­ ê¸°ë³¸ê°’
LANG_FIXED    = "kor"   # ì–¸ì–´ ê¸°ë³¸ê°’

# 008 ë³¸ë¬¸(40ì) ì¡°ë¦½ê¸° â€” ë‹¨í–‰ë³¸ ê¸°ì¤€(type_of_date ê¸°ë³¸ 's')
def build_008_kormarc_bk(
    date_entered,          # 00-05 YYMMDD
    date1,                 # 07-10 4ìë¦¬(ì˜ˆ: '2025' / '19uu')
    country3,              # 15-17 3ìë¦¬
    lang3,                 # 35-37 3ìë¦¬
    date2="",              # 11-14
    illus4="",             # 18-21 ìµœëŒ€ 4ì(ì˜ˆ: 'a','ad','ado'â€¦)
    has_index="0",         # 31 '0' ì—†ìŒ / '1' ìˆìŒ
    lit_form=" ",          # 33 (pì‹œ/fì†Œì„¤/eìˆ˜í•„/iì„œê°„ë¬¸í•™/mê¸°í–‰Â·ì¼ê¸°Â·ìˆ˜ê¸°)
    bio=" ",               # 34 (a ìì„œì „ / b ì „ê¸°Â·í‰ì „ / d ë¶€ë¶„ì  ì „ê¸°)
    type_of_date="s",      # 06
    modified_record=" ",   # 28
    cataloging_src="a",    # 32  â† ê¸°ë³¸ê°’ 'a'
):
    def pad(s, n, fill=" "):
        s = "" if s is None else str(s)
        return (s[:n] + fill * n)[:n]

    if len(date_entered) != 6 or not date_entered.isdigit():
        raise ValueError("date_enteredëŠ” YYMMDD 6ìë¦¬ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
    if len(date1) != 4:
        raise ValueError("date1ì€ 4ìë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: '2025', '19uu'")

    body = "".join([
        date_entered,               # 00-05
        pad(type_of_date,1),        # 06
        date1,                      # 07-10
        pad(date2,4),               # 11-14
        pad(country3,3),            # 15-17
        pad(illus4,4),              # 18-21
        " " * 4,                    # 22-25 (ì´ìš©ëŒ€ìƒ/ìë£Œí˜•íƒœ/ë‚´ìš©í˜•ì‹) ê³µë°±
        " " * 2,                    # 26-27 ê³µë°±
        pad(modified_record,1),     # 28
        "0",                        # 29 íšŒì˜ê°„í–‰ë¬¼
        "0",                        # 30 ê¸°ë…ë…¼ë¬¸ì§‘
        has_index if has_index in ("0","1") else "0",  # 31 ìƒ‰ì¸
        pad(cataloging_src,1),      # 32 ëª©ë¡ ì „ê±°
        pad(lit_form,1),            # 33 ë¬¸í•™í˜•ì‹
        pad(bio,1),                 # 34 ì „ê¸°
        pad(lang3,3),               # 35-37 ì–¸ì–´
        " " * 2                     # 38-39 (ì •ë¶€ê¸°ê´€ë¶€í˜¸ ë“±) ê³µë°±
    ])
    if len(body) != 40:
        raise AssertionError(f"008 length != 40: {len(body)}")
    return body

# ë°œí–‰ì—°ë„ ì¶”ì¶œ(ì•Œë¼ë”˜ pubDate ìš°ì„ )
def extract_year_from_aladin_pubdate(pubdate_str: str) -> str:
    m = re.search(r"(19|20)\d{2}", pubdate_str or "")
    return m.group(0) if m else "19uu"

# 300 ë°œí–‰ì§€ ë¬¸ìì—´ â†’ country3 ì¶”ë¡ 
def guess_country3_from_place(place_str: str) -> str:
    if not place_str:
        return COUNTRY_FIXED
    for key, code in KR_REGION_TO_CODE.items():
        if key in place_str:
            return code
    # í•œêµ­ ì¼ë°˜ì½”ë“œ("ko ")ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ê¸°ë³¸ê°’ìœ¼ë¡œ í†µì¼
    return COUNTRY_FIXED


# ====== ë‹¨ì–´ ê°ì§€ ======
def detect_illus4(text: str) -> str:
    # a: ì‚½í™”/ì¼ëŸ¬ìŠ¤íŠ¸/ê·¸ë¦¼, d: ë„í‘œ/ê·¸ë˜í”„/ì°¨íŠ¸, o: ì‚¬ì§„/í™”ë³´
    keys = []
    if re.search(r"ì‚½í™”|ì‚½ë„|ë„í•´|ì¼ëŸ¬ìŠ¤íŠ¸|ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜|ê·¸ë¦¼|illustration", text, re.I): keys.append("a")
    if re.search(r"ë„í‘œ|í‘œ|ì°¨íŠ¸|ê·¸ë˜í”„|chart|graph", text, re.I):                          keys.append("d")
    if re.search(r"ì‚¬ì§„|í¬í† |í™”ë³´|photo|photograph|ì»¬ëŸ¬ì‚¬ì§„|ì¹¼ë¼ì‚¬ì§„", text, re.I):          keys.append("o")
    out = []
    for k in keys:
        if k not in out:
            out.append(k)
    return "".join(out)[:4]

def detect_index(text: str) -> str:
    return "1" if re.search(r"ìƒ‰ì¸|ì°¾ì•„ë³´ê¸°|ì¸ëª…ìƒ‰ì¸|ì‚¬í•­ìƒ‰ì¸|index", text, re.I) else "0"

def detect_lit_form(title: str, category: str, extra_text: str = "") -> str:
    blob = f"{title} {category} {extra_text}"
    if re.search(r"ì„œê°„ì§‘|í¸ì§€|ì„œê°„ë¬¸|letters?", blob, re.I): return "i"    # ì„œê°„ë¬¸í•™
    if re.search(r"ê¸°í–‰|ì—¬í–‰ê¸°|ì—¬í–‰ ì—ì„¸ì´|ì¼ê¸°|ìˆ˜ê¸°|diary|travel", blob, re.I): return "m"  # ê¸°í–‰/ì¼ê¸°/ìˆ˜ê¸°
    if re.search(r"ì‹œì§‘|ì‚°ë¬¸ì‹œ|poem|poetry", blob, re.I): return "p"        # ì‹œ
    if re.search(r"ì†Œì„¤|ì¥í¸|ì¤‘ë‹¨í¸|novel|fiction", blob, re.I): return "f"  # ì†Œì„¤
    if re.search(r"ì—ì„¸ì´|ìˆ˜í•„|essay", blob, re.I): return "e"               # ìˆ˜í•„
    return " "

def detect_bio(text: str) -> str:
    if re.search(r"ìì„œì „|íšŒê³ ë¡|autobiograph", text, re.I): return "a"
    if re.search(r"ì „ê¸°|í‰ì „|ì¸ë¬¼ í‰ì „|biograph", text, re.I): return "b"
    if re.search(r"ì „ê¸°ì |ìì „ì |íšŒê³ |íšŒìƒ", text): return "d"
    return " "

# ë©”ì¸: ISBN í•˜ë‚˜ë¡œ 008 ìƒì„± (toc/300/041 ì—°ë™ ê°€ëŠ¥)
def build_008_from_isbn(
    isbn: str,
    *,
    aladin_pubdate: str = "",
    aladin_title: str = "",
    aladin_category: str = "",
    aladin_desc: str = "",
    aladin_toc: str = "",            # ëª©ì°¨ê°€ ìˆìœ¼ë©´ ê°ì§€ì— í™œìš©
    source_300_place: str = "",      # 300 ë°œí–‰ì§€ ë¬¸ìì—´(ìˆìœ¼ë©´ country3 ì¶”ì •)
    override_country3: str = None,   # ì™¸ë¶€ ëª¨ë“ˆì´ ì£¼ë©´ ìµœìš°ì„ 
    override_lang3: str = None,      # ì™¸ë¶€ ëª¨ë“ˆì´ ì£¼ë©´ ìµœìš°ì„ (041)
    cataloging_src: str = "a",       # 32 ëª©ë¡ ì „ê±°(ê¸°ë³¸ 'a')
):
    today  = datetime.datetime.now().strftime("%y%m%d")  # YYMMDD
    date1  = extract_year_from_aladin_pubdate(aladin_pubdate)

    # country ìš°ì„ ìˆœìœ„: override > 300ë°œí–‰ì§€ ë§¤í•‘ > ê¸°ë³¸ê°’
    if override_country3:
        country3 = override_country3
    elif source_300_place:
        country3 = guess_country3_from_place(source_300_place)
    else:
        country3 = COUNTRY_FIXED

    # lang ìš°ì„ ìˆœìœ„: override(041) > ê¸°ë³¸ê°’
    lang3 = override_lang3 or LANG_FIXED

    # ë‹¨ì–´ ê°ì§€ìš© í…ìŠ¤íŠ¸: ì œëª© + ì†Œê°œ + ëª©ì°¨
    bigtext = " ".join([aladin_title or "", aladin_desc or "", aladin_toc or ""])
    illus4    = detect_illus4(bigtext)
    has_index = detect_index(bigtext)
    lit_form  = detect_lit_form(aladin_title or "", aladin_category or "", bigtext)
    bio       = detect_bio(bigtext)

    return build_008_kormarc_bk(
        date_entered=today,
        date1=date1,
        country3=country3,
        lang3=lang3,
        illus4=illus4,
        has_index=has_index,
        lit_form=lit_form,
        bio=bio,
        cataloging_src=cataloging_src,
    )
# ========= 008 ìƒì„± ë¸”ë¡ v3 ë =========

# ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ (konlpy ì—†ì´)
def extract_keywords_from_text(text, top_n=7):
    words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
    filtered = [w for w in words if len(w) > 1]
    freq = Counter(filtered)
    return [kw for kw, _ in freq.most_common(top_n)]

def clean_keywords(words):
    stopwords = {"ì•„ì£¼", "ê°€ì§€", "í•„ìš”í•œ", "ë“±", "ìœ„í•´", "ê²ƒ", "ìˆ˜", "ë”", "ì´ëŸ°", "ìˆë‹¤", "ëœë‹¤", "í•œë‹¤"}
    return [w for w in words if w not in stopwords and len(w) > 1]



# ğŸ“¡ ë¶€ê°€ê¸°í˜¸ ì¶”ì¶œ (êµ­ë¦½ì¤‘ì•™ë„ì„œê´€)
@st.cache_data(ttl=24*3600)
def fetch_additional_code_from_nlk(isbn: str) -> str:
    """
    êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ì„œì§€API(ì„œì§€ì •ë³´)ì—ì„œ EA_ADD_CODE(ë¶€ê°€ê¸°í˜¸)ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì™€ ë°˜í™˜.
    - ì—¬ëŸ¬ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ìˆœí™˜ ì‹œë„
    - JSON/ XML ëª¨ë‘ ì§€ì›
    - ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    """
    attempts = [
        "https://seoji.nl.go.kr/landingPage/SearchApi.do",
        "https://www.nl.go.kr/seoji/SearchApi.do",
        "http://seoji.nl.go.kr/landingPage/SearchApi.do",
        "http://www.nl.go.kr/seoji/SearchApi.do",
    ]
    params = {
        "cert_key": NLK_CERT_KEY,
        "result_style": "json",   # json ìš°ì„ 
        "page_no": 1,
        "page_size": 1,
        "isbn": isbn.strip().replace("-", ""),
    }

    for base in attempts:
        try:
            r = SESSION.get(base, params=params, timeout=(5, 10))
            r.raise_for_status()

            # 1) JSON ìš°ì„  íŒŒì‹±
            try:
                j = r.json()
                doc = None
                # ì‘ë‹µ êµ¬ì¡°: { "docs": { "doc": [ {...} ] } } or { "docs": [ {...} ] } ë“± ë³€í˜• ëŒ€ì‘
                if isinstance(j, dict):
                    if "docs" in j and isinstance(j["docs"], dict):
                        arr = j["docs"].get("doc") or []
                        if isinstance(arr, list) and arr:
                            doc = arr[0]
                    elif "docs" in j and isinstance(j["docs"], list) and j["docs"]:
                        doc = j["docs"][0]
                    elif "doc" in j and isinstance(j["doc"], list) and j["doc"]:
                        doc = j["doc"][0]
                if doc:
                    val = (doc.get("EA_ADD_CODE") or "").strip()
                    if val:
                        return val
            except Exception:
                pass

            # 2) XML í´ë°± íŒŒì‹±
            try:
                root = ET.fromstring(r.text)
                # ë³´í†µ //docs/e/EA_ADD_CODE í˜•íƒœ
                node = root.find(".//docs")
                if node is None:
                    node = root
                # ê°€ì¥ ì²« ë ˆì½”ë“œ(e) íƒìƒ‰
                e = node.find(".//e") or node.find(".//item") or node
                if e is not None:
                    val = (e.findtext("EA_ADD_CODE") or "").strip()
                    if val:
                        return val
            except Exception:
                pass

        except Exception:
            # ë‹¤ìŒ ì—”ë“œí¬ì¸íŠ¸ë¡œ í´ë°±
            continue

    # ì „ë¶€ ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¬¸ìì—´
    return ""


# ğŸ”¤ ì–¸ì–´ ê°ì§€ ë° 041, 546 ìƒì„±
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}

def detect_language(text):
    text = re.sub(r'[\s\W_]+', '', text)
    if not text:
        return 'und'
    first_char = text[0]
    if '\uac00' <= first_char <= '\ud7a3':
        return 'kor'
    elif '\u3040' <= first_char <= '\u30ff':
        return 'jpn'
    elif '\u4e00' <= first_char <= '\u9fff':
        return 'chi'
    elif '\u0400' <= first_char <= '\u04FF':
        return 'rus'
    elif 'a' <= first_char.lower() <= 'z':
        return 'eng'
    else:
        return 'und'

def generate_546_from_041_kormarc(marc_041: str) -> str:
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"):
            a_codes.append(part[2:])
        elif part.startswith("$h"):
            h_code = part[2:]
    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{a_lang}ë¡œ ì”€, ì›ì €ëŠ” {h_lang}ì„"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"
    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"

def crawl_aladin_original_and_price(isbn13):
    url = f"https://www.aladin.co.kr/shop/wproduct.aspx?ISBN={isbn13}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        original = soup.select_one("div.info_original")
        price = soup.select_one("span.price2")
        return {
            "original_title": original.text.strip() if original else "",
            "price": price.text.strip().replace("ì •ê°€ : ", "").replace("ì›", "").replace(",", "").strip() if price else ""
        }
    except:
        return {}

# ---- 653 ì „ì²˜ë¦¬ ìœ í‹¸ ----
def _norm(text: str) -> str:
    import re, unicodedata
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text).lower()
    text = re.sub(r"[^\w\s\uac00-\ud7a3]", " ", text)  # í•œ/ì˜/ìˆ«ì/ê³µë°±ë§Œ
    return re.sub(r"\s+", " ", text).strip()

def _clean_author_str(s: str) -> str:
    import re
    if not s:
        return ""
    s = re.sub(r"\(.*?\)", " ", s)      # (ì§€ì€ì´), (ì˜®ê¸´ì´) ë“± ì œê±°
    s = re.sub(r"[/;Â·,]", " ", s)       # êµ¬ë¶„ì ê³µë°±í™”
    return re.sub(r"\s+", " ", s).strip()

def _build_forbidden_set(title: str, authors: str) -> set:
    t_norm = _norm(title)
    a_norm = _norm(authors)
    forb = set()
    if t_norm:
        forb.update(t_norm.split())
        forb.add(t_norm.replace(" ", ""))  # 'ì£½ìŒ íŠ¸ë¦´ë¡œì§€' â†’ 'ì£½ìŒíŠ¸ë¦´ë¡œì§€'
    if a_norm:
        forb.update(a_norm.split())
        forb.add(a_norm.replace(" ", ""))
    return {f for f in forb if f and len(f) >= 2}  # 1ê¸€ì ì œê±°

def _should_keep_keyword(kw: str, forbidden: set) -> bool:
    n = _norm(kw)
    if not n or len(n.replace(" ", "")) < 2:
        return False
    for tok in forbidden:
        if tok in n or n in tok:
            return False
    return True
# -------------------------

# ğŸ“„ 653 í•„ë“œ í‚¤ì›Œë“œ ìƒì„±
# â‘¡ ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„° í˜¸ì¶œ í•¨ìˆ˜
def fetch_aladin_metadata(isbn):
    url = (
        "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        f"?ttbkey={aladin_key}"
        "&ItemIdType=ISBN"
        f"&ItemId={isbn}"
        "&output=js"
        "&Version=20131101"
        "&OptResult=Toc"
    )
    data = requests.get(url).json()
    item = (data.get("item") or [{}])[0]

    # ì €ì í•„ë“œ ë‹¤ì–‘í•œ í‚¤ ëŒ€ì‘
    raw_author = item.get("author") or item.get("authors") or item.get("author_t") or ""
    authors = _clean_author_str(raw_author)

    return {
        "category": item.get("categoryName", "") or "",
        "title": item.get("title", "") or "",
        "authors": authors,                           # â¬…ï¸ ì¶”ê°€ë¨
        "description": item.get("description", "") or "",
        "toc": item.get("toc", "") or "",
    }



# â‘¢ GPT-4 ê¸°ë°˜ 653 ìƒì„± í•¨ìˆ˜
def generate_653_with_gpt(category, title, authors, description, toc, max_keywords=7):
    parts = [p.strip() for p in (category or "").split(">") if p.strip()]
    cat_kw = parts[-1] if parts else ""

    forbidden = _build_forbidden_set(title, authors)

    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì±…ì˜ ë¶„ë¥˜, ì„¤ëª…, ëª©ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ MARC 653 ì£¼ì œì–´ë¥¼ ë„ì¶œí•˜ì„¸ìš”. "
            "ì„œëª…(245)Â·ì €ì(100/700)ì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ëŠ” ì œì™¸í•©ë‹ˆë‹¤."
        )
    }
    user_msg = {
        "role": "user",
        "content": (
            f"ì…ë ¥ ì •ë³´ë¡œë¶€í„° ìµœëŒ€ {max_keywords}ê°œì˜ MARC 653 ì£¼ì œì–´ë¥¼ í•œ ì¤„ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n\n"
            f"- ë¶„ë¥˜: \"{cat_kw}\"\n"
            f"- ì œëª©(245): \"{title}\"\n"
            f"- ì €ì(100/700): \"{authors}\"\n"
            f"- ì„¤ëª…: \"{description}\"\n"
            f"- ëª©ì°¨: \"{toc}\"\n\n"
            "ì œì™¸ì–´ ëª©ë¡(ì„œëª…/ì €ìì—ì„œ ìœ ë˜): "
            f"{', '.join(sorted(forbidden)) or '(ì—†ìŒ)'}\n\n"
            "ê·œì¹™:\n"
            "1) 'ì œëª©'ê³¼ 'ì €ì'ì— ì“°ì¸ ë‹¨ì–´Â·í‘œí˜„ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
            "2) ë¶„ë¥˜/ì„¤ëª…/ëª©ì°¨ì—ì„œ í•µì‹¬ ê°œë…ì„ ëª…ì‚¬ ì¤‘ì‹¬ìœ¼ë¡œ ë½‘ìœ¼ì„¸ìš”.\n"
            "3) ì¶œë ¥ í˜•ì‹: $aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 â€¦ (í•œ ì¤„)\n"
        )
    }
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=180,
        )
        raw = (resp.choices[0].message.content or "").strip()

        # $a ë‹¨ìœ„ íŒŒì‹±
        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]
        if not kws:
            # ë°±ì—… íŒŒì‹±
            tmp = re.split(r"[,\n]", raw)
            kws = [t.strip().lstrip("$a") for t in tmp if t.strip()]

        # ê³µë°± ì‚­ì œ(ì›í•˜ë©´ ìœ ì§€ ê°€ëŠ¥)
        kws = [kw.replace(" ", "") for kw in kws]

        # 1ì°¨: ê¸ˆì¹™ì–´(ì„œëª…/ì €ì) í•„í„°
        kws = [kw for kw in kws if _should_keep_keyword(kw, forbidden)]

        # 2ì°¨: ì •ê·œí™” ì¤‘ë³µ ì œê±°
        seen = set()
        uniq = []
        for kw in kws:
            n = _norm(kw)
            if n not in seen:
                seen.add(n)
                uniq.append(kw)

        # 3ì°¨: ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        uniq = uniq[:max_keywords]

        return "".join(f"$a{kw}" for kw in uniq)

    except Exception as e:
        st.warning(f"âš ï¸ 653 ì£¼ì œì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
   


# ğŸ“š MARC ìƒì„±
@st.cache_data(show_spinner=False)
def fetch_book_data_from_aladin(isbn, reg_mark="", reg_no="", copy_symbol=""):
    import re
    from concurrent.futures import ThreadPoolExecutor

    # 1) ì•Œë¼ë”˜ + (ì˜µì…˜) êµ­ì¤‘ ë¶€ê°€ê¸°í˜¸ ë™ì‹œ ìš”ì²­
    url = (
        f"https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?"
        f"ttbkey={aladin_key}&itemIdType=ISBN&ItemId={isbn}"
        f"&output=js&Version=20131101"
    )
    with ThreadPoolExecutor(max_workers=2) as ex:
        future_aladin = ex.submit(lambda: requests.get(url, verify=False, timeout=5))
        future_nlk    = ex.submit(fetch_additional_code_from_nlk, isbn)

        try:
            resp = future_aladin.result()
            resp.raise_for_status()
            data = resp.json().get("item", [{}])[0]
        except Exception as e:
            st.error(f"ğŸš¨ ì•Œë¼ë”˜API ì˜¤ë¥˜: {e}")
            return ""

        add_code = future_nlk.result()  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´

    # 2) ë©”íƒ€ë°ì´í„° (ì•Œë¼ë”˜)
    title       = data.get("title",       "ì œëª©ì—†ìŒ")
    author      = data.get("author",      "ì €ìë¯¸ìƒ")
    publisher   = data.get("publisher",   "ì¶œíŒì‚¬ë¯¸ìƒ")
    pubdate     = data.get("pubDate",     "2025")  # 'YYYY' ë˜ëŠ” 'YYYY-MM-DD'
    category    = data.get("categoryName", "")
    description = data.get("description", "")
    toc         = data.get("subInfo", {}).get("toc", "")
    price       = str(data.get("priceStandard", ""))  # 020/950 ìš©

    # 3) =008 ìƒì„± (ISBNë§Œìœ¼ë¡œ ìë™, country/langì€ ì„ì‹œ ê³ ì •ê°’ â†’ ì¶”í›„ override)
    tag_008 = "=008  " + build_008_from_isbn(
        isbn,
        aladin_pubdate=pubdate,
        aladin_title=title,
        aladin_category=category,
        aladin_desc=description,
        # override_country3="ulk",  # 300 ëª¨ë“ˆ ì™„ì„± ì‹œ ì‚¬ìš©
        # override_lang3="kor",     # 041 ëª¨ë“ˆ ì™„ì„± ì‹œ ì‚¬ìš©
    )

    # 4) 041/546 (ê°„ì´ ê°ì§€: ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    lang_a  = detect_language(title)
    lang_h  = detect_language(data.get("title", ""))
    tag_041 = f"=041  \\$a{lang_a}" + (f"$h{lang_h}" if lang_h != "und" else "")
    tag_546 = f"=546  \\$a{generate_546_from_041_kormarc(tag_041)}"

    # 5) 020 (ë¶€ê°€ê¸°í˜¸ ìˆìœ¼ë©´ $g ì¶”ê°€)
    tag_020 = f"=020  \\$a{isbn}"
    if price:
        tag_020 += f":$c{price}"
    if add_code:
        tag_020 += f"$g{add_code}"


    # 6) 653/KDC â€” âœ… ì—¬ê¸°ì„œë§Œ ìƒì„± (GPTAPI ìµœì‹  í•¨ìˆ˜ë¡œ í†µì¼)
    kdc     = recommend_kdc(title, author, api_key=openai_key)

    # â¬‡ï¸ authors ì¸ì ì¶”ê°€(ì €ì ë¬¸ìì—´ì„ ì „ì²˜ë¦¬í•´ì„œ ë„˜ê¹€)
    gpt_653 = generate_653_with_gpt(
    category,
    title,
    _clean_author_str(author),   # â† ì¶”ê°€ëœ ë¶€ë¶„
    description,
    toc,
    max_keywords=7
    )

    tag_653 = f"=653  \\{gpt_653.replace(' ', '')}" if gpt_653 else ""


    # 7) ê¸°ë³¸ MARC ë¼ì¸
    marc_lines = [
        tag_008,
        "=007  ta",
        f"=245  00$a{title} /$c{author}",
        f"=260  \\$aì„œìš¸ :$b{publisher},$c{pubdate[:4]}.",
    ]

    # 8) 490Â·830 (ì´ì„œ)
    series = data.get("seriesInfo", {})
    name = (series.get("seriesName") or "").strip()
    vol  = (series.get("volume")    or "").strip()
    if name:
        marc_lines.append(f"=490  \\$a{name};$v{vol}")
        marc_lines.append(f"=830  \\$a{name};$v{vol}")

    # 9) ê¸°íƒ€ í•„ë“œ
    marc_lines.append(tag_020)
    marc_lines.append(tag_041)
    marc_lines.append(tag_546)
    if kdc and kdc != "000":
        marc_lines.append(f"=056  \\$a{kdc}$26")
    if tag_653:
        marc_lines.append(tag_653)
    marc_lines.append(f"=950  0\\$b{price}")

    # 10) 049: ì†Œì¥ê¸°í˜¸(ì…ë ¥ëœ ê²½ìš°ë§Œ)
    if reg_mark or reg_no or copy_symbol:
        line = f"=049  0\\$I{reg_mark}{reg_no}"
        if copy_symbol:
            line += f"$f{copy_symbol}"
        marc_lines.append(line)

    # 11) ë²ˆí˜¸ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ í›„ ì¶œë ¥
    marc_lines.sort(key=lambda L: int(re.match(r"=(\d+)", L).group(1)))
    return "\n".join(marc_lines)

def _lang3_from_tag041(tag_041: str | None) -> str | None:
    """'041 $akor$hrus'ì—ì„œ ì²« $aë§Œ ë½‘ì•„ 008 lang3 overrideì— ì‚¬ìš©."""
    if not tag_041: return None
    m = re.search(r"\$a([a-z]{3})", tag_041, flags=re.I)
    return m.group(1).lower() if m else None

def _build_020_from_item_and_nlk(isbn: str, item: dict) -> str:
    """020 $a$g(:$c) â€” NLK ë¶€ê°€ê¸°í˜¸ë¥¼ $c(ê°€ê²©)ë³´ë‹¤ ì•ì— ë°°ì¹˜"""
    # 1) ì •ê°€
    price = str((item or {}).get("priceStandard", "") or "").strip()

    # 2) ë¶€ê°€ê¸°í˜¸(NLK)
    try:
        add_code = fetch_additional_code_from_nlk(isbn) or ""
    except Exception:
        add_code = ""

    # 3) ì¡°í•©: =020  \ $a{isbn}$g{EA_ADD_CODE}:$c{price}
    parts = [f"=020  \\\\$a{isbn}"]
    if add_code:
        parts.append(f"$g{add_code}")
    if price:
        parts.append(f":$c{price}")

    return "".join(parts)


def _build_653_via_gpt(item: dict) -> str | None:
    """ë„¤ê°€ ì˜¬ë¦° generate_653_with_gpt() ê·¸ëŒ€ë¡œ í™œìš©í•´ì„œ 653 í•œ ì¤„ ë°˜í™˜."""
    title = (item or {}).get("title","") or ""
    category = (item or {}).get("categoryName","") or ""
    raw_author = (item or {}).get("author","") or ""
    desc = (item or {}).get("description","") or ""
    toc  = ((item or {}).get("subInfo",{}) or {}).get("toc","") or ""
    kwline = generate_653_with_gpt(
        category=category,
        title=title,
        authors=_clean_author_str(raw_author),
        description=desc,
        toc=toc,
        max_keywords=7
    )
    return f"=653  \\\\{kwline.replace(' ', '')}" if kwline else None

# --- ê°€ê²© ì¶”ì¶œ í—¬í¼: ì•Œë¼ë”˜ priceStandard ìš°ì„ , ì—†ìœ¼ë©´ í¬ë¡¤ë§ ë°±ì—… ---
def _extract_price_kr(item: dict, isbn: str) -> str:
    # 1) ì•Œë¼ë”˜ í‘œì¤€ê°€ ìš°ì„ 
    raw = str((item or {}).get("priceStandard", "") or "").strip()
    # 2) ë¹„ì–´ ìˆìœ¼ë©´ í¬ë¡¤ë§ ë°±ì—… ì‹œë„
    if not raw:
        try:
            crawl = crawl_aladin_original_and_price(isbn) or {}
            raw = crawl.get("price", "").strip()
        except Exception:
            raw = ""
    # 3) ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
    import re
    digits = re.sub(r"[^\d]", "", raw)
    return digits  # "15000" ê°™ì€ í˜•íƒœ

# --- 950 ë¹Œë” ---
def build_950_from_item_and_price(item: dict, isbn: str) -> str:
    price = _extract_price_kr(item, isbn)
    if not price:
        return ""  # ê°€ê²© ì—†ìœ¼ë©´ 950 ìƒëµ
    return f"=950  0\\$b{price}"

# =========================
# --- êµ¬ê¸€ì‹œíŠ¸ ë¡œë“œ & ìºì‹œ ê´€ë¦¬ ---
# =========================
@st.cache_data(ttl=3600)
def load_publisher_db():
    creds = ServiceAccountCredentials.from_json_keyfile_dict(st.secrets["gspread"], 
                                                            ["https://spreadsheets.google.com/feeds",
                                                             "https://www.googleapis.com/auth/drive"])
    client = gspread.authorize(creds)
    sh = client.open("ì¶œíŒì‚¬ DB")
    
    # KPIPA_PUB_REG: ë²ˆí˜¸, ì¶œíŒì‚¬ëª…, ì£¼ì†Œ, ì „í™”ë²ˆí˜¸ â†’ ì¶œíŒì‚¬ëª…, ì£¼ì†Œë§Œ ì‚¬ìš©
    pub_rows = sh.worksheet("KPIPA_PUB_REG").get_all_values()[1:]
    pub_rows_filtered = [row[1:3] for row in pub_rows]  # ì¶œíŒì‚¬ëª…, ì£¼ì†Œ
    publisher_data = pd.DataFrame(pub_rows_filtered, columns=["ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ"])
    
    # 008: ë°œí–‰êµ­ ë°œí–‰êµ­ ë¶€í˜¸ â†’ ì²« 2ì—´ë§Œ
    region_rows = sh.worksheet("008").get_all_values()[1:]
    region_rows_filtered = [row[:2] for row in region_rows]
    region_data = pd.DataFrame(region_rows_filtered, columns=["ë°œí–‰êµ­", "ë°œí–‰êµ­ ë¶€í˜¸"])
    
    # IM_* ì‹œíŠ¸: ì¶œíŒì‚¬/ì„í”„ë¦°íŠ¸ í•˜ë‚˜ì˜ ì¹¼ëŸ¼
    imprint_frames = []
    for ws in sh.worksheets():
        if ws.title.startswith("IM_"):
            data = ws.get_all_values()[1:]
            imprint_frames.extend([row[0] for row in data if row])
    imprint_data = pd.DataFrame(imprint_frames, columns=["ì„í”„ë¦°íŠ¸"])
    
    return publisher_data, region_data, imprint_data

# =========================
# --- ì•Œë¼ë”˜ API ---
# =========================
def search_aladin_by_isbn(isbn):
    try:
        ttbkey = st.secrets["aladin"]["ttbkey"]
        url = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        params = {"ttbkey": ttbkey, "itemIdType": "ISBN", "ItemId": isbn, 
                  "output": "js", "Version": "20131101"}
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        data = res.json()
        if "item" not in data or not data["item"]:
            return None, f"ë„ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. [ì‘ë‹µ: {data}]"
        book = data["item"][0]
        title = book.get("title", "ì œëª© ì—†ìŒ")
        author = book.get("author", "")
        publisher = book.get("publisher", "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ")
        pubdate = book.get("pubDate", "")
        pubyear = pubdate[:4] if len(pubdate) >= 4 else "ë°œí–‰ë…„ë„ ì—†ìŒ"
        authors = [a.strip() for a in author.split(",")] if author else []
        creator_str = " ; ".join(authors) if authors else "ì €ì ì •ë³´ ì—†ìŒ"
        field_245 = f"=245  10$a{title} /$c{creator_str}"
        return {"title": title, "creator": creator_str, "publisher": publisher, "pubyear": pubyear, "245": field_245}, None
    except Exception as e:
        return None, f"Aladin API ì˜ˆì™¸: {e}"

# =========================
# --- ì •ê·œí™” í•¨ìˆ˜ ---
# =========================
def normalize_publisher_name(name):
    return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬", "", name).lower()

def normalize_stage2(name):
    name = re.sub(r"(ì£¼ë‹ˆì–´|JUNIOR|ì–´ë¦°ì´|í‚¤ì¦ˆ|ë¶ìŠ¤|ì•„ì´ì„¸ì›€|í”„ë ˆìŠ¤)", "", name, flags=re.IGNORECASE)
    eng_to_kor = {"springer": "ìŠ¤í”„ë§ê±°", "cambridge": "ì¼€ì„ë¸Œë¦¬ì§€", "oxford": "ì˜¥ìŠ¤í¬ë“œ"}
    for eng, kor in eng_to_kor.items():
        name = re.sub(eng, kor, name, flags=re.IGNORECASE)
    return name.strip().lower()

def split_publisher_aliases(name):
    aliases = []
    bracket_contents = re.findall(r"\((.*?)\)", name)
    for content in bracket_contents:
        parts = re.split(r"[,/]", content)
        parts = [p.strip() for p in parts if p.strip()]
        aliases.extend(parts)
    name_no_brackets = re.sub(r"\(.*?\)", "", name).strip()
    if "/" in name_no_brackets:
        parts = [p.strip() for p in name_no_brackets.split("/") if p.strip()]
        rep_name = parts[0]
        aliases.extend(parts[1:])
    else:
        rep_name = name_no_brackets
    return rep_name, aliases

def normalize_publisher_location_for_display(location_name):
    if not location_name or location_name in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
        return location_name
    location_name = location_name.strip()
    major_cities = ["ì„œìš¸", "ì¸ì²œ", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ëŒ€êµ¬", "ë¶€ì‚°", "ì„¸ì¢…"]
    for city in major_cities:
        if city in location_name:
            return location_name[:2]
    parts = location_name.split()
    loc = parts[1] if len(parts) > 1 else parts[0]
    if loc.endswith("ì‹œ"):
        loc = loc[:-1]
    return loc

# =========================
# --- KPIPA DB ê²€ìƒ‰ ë³´ì¡° í•¨ìˆ˜ ---
# =========================
def search_publisher_location_with_alias(name, publisher_data):
    debug_msgs = []
    if not name:
        return "ì¶œíŒì§€ ë¯¸ìƒ", ["âŒ ê²€ìƒ‰ ì‹¤íŒ¨: ì…ë ¥ëœ ì¶œíŒì‚¬ëª…ì´ ì—†ìŒ"]
    norm_name = normalize_publisher_name(name)
    candidates = publisher_data[publisher_data["ì¶œíŒì‚¬ëª…"].apply(lambda x: normalize_publisher_name(x)) == norm_name]
    if not candidates.empty:
        address = candidates.iloc[0]["ì£¼ì†Œ"]
        debug_msgs.append(f"âœ… KPIPA DB ë§¤ì¹­ ì„±ê³µ: {name} â†’ {address}")
        return address, debug_msgs
    else:
        debug_msgs.append(f"âŒ KPIPA DB ë§¤ì¹­ ì‹¤íŒ¨: {name}")
        return "ì¶œíŒì§€ ë¯¸ìƒ", debug_msgs

# =========================
# --- IM ì„í”„ë¦°íŠ¸ ë³´ì¡° í•¨ìˆ˜ ---
# =========================
def find_main_publisher_from_imprints(rep_name, imprint_data, publisher_data):
    """
    IM_* ì‹œíŠ¸ì—ì„œ ì„í”„ë¦°íŠ¸ëª…ì„ ê²€ìƒ‰í•˜ê³ , KPIPA DBì—ì„œ í•´ë‹¹ ì¶œíŒì‚¬ëª…ìœ¼ë¡œ ì£¼ì†Œë¥¼ ë°˜í™˜
    """
    norm_rep = normalize_publisher_name(rep_name)
    for full_text in imprint_data["ì„í”„ë¦°íŠ¸"]:
        if "/" in full_text:
            pub_part, imprint_part = [p.strip() for p in full_text.split("/", 1)]
        else:
            pub_part, imprint_part = full_text.strip(), None

        if imprint_part:
            norm_imprint = normalize_publisher_name(imprint_part)
            if norm_imprint == norm_rep:
                # KPIPA DBì—ì„œ pub_partë¥¼ ê²€ìƒ‰
                location, debug_msgs = search_publisher_location_with_alias(pub_part, publisher_data)
                return location, debug_msgs
    return None, [f"âŒ IM DB ê²€ìƒ‰ ì‹¤íŒ¨: ë§¤ì¹­ë˜ëŠ” ì„í”„ë¦°íŠ¸ ì—†ìŒ ({rep_name})"]

    

# =========================
# --- KPIPA í˜ì´ì§€ ê²€ìƒ‰ ---
# =========================
def get_publisher_name_from_isbn_kpipa(isbn):
    search_url = "https://bnk.kpipa.or.kr/home/v3/addition/search"
    params = {"ST": isbn, "PG": 1, "PG2": 1, "DSF": "Y", "SO": "weight", "DT": "A"}
    headers = {"User-Agent": "Mozilla/5.0"}
    def normalize(name):
        return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬|í”„ë ˆìŠ¤", "", name).lower()
    try:
        res = requests.get(search_url, params=params, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        first_result_link = soup.select_one("a.book-grid-item")
        if not first_result_link:
            return None, None, "âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (KPIPA)"
        detail_href = first_result_link.get("href")
        detail_url = f"https://bnk.kpipa.or.kr{detail_href}"
        detail_res = requests.get(detail_url, headers=headers, timeout=15)
        detail_res.raise_for_status()
        detail_soup = BeautifulSoup(detail_res.text, "html.parser")
        pub_info_tag = detail_soup.find("dt", string="ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸")
        if not pub_info_tag:
            return None, None, "âŒ 'ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸' í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"
        dd_tag = pub_info_tag.find_next_sibling("dd")
        if dd_tag:
            full_text = dd_tag.get_text(strip=True)
            publisher_name_full = full_text
            publisher_name_part = publisher_name_full.split("/")[0].strip()
            publisher_name_norm = normalize(publisher_name_part)
            return publisher_name_full, publisher_name_norm, None
        return None, None, "âŒ 'dd' íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"
    except Exception as e:
        return None, None, f"KPIPA ì˜ˆì™¸: {e}"

# =========================
# ----ë°œí–‰êµ­ ë¶€í˜¸ ì°¾ê¸°-----
# =========================

def get_country_code_by_region(region_name, region_data):
    """
    ì§€ì—­ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ 008 ë°œí–‰êµ­ ë¶€í˜¸ë¥¼ ì°¾ìŒ.
    region_data: DataFrame, columns=["ë°œí–‰êµ­", "ë°œí–‰êµ­ ë¶€í˜¸"]
    """
    try:
        def normalize_region_for_code(region):
            region = (region or "").strip()
            if region.startswith(("ì „ë¼", "ì¶©ì²­", "ê²½ìƒ")):
                return region[0] + (region[2] if len(region) > 2 else "")
            return region[:2]
        normalized_input = normalize_region_for_code(region_name)
        for idx, row in region_data.iterrows():
            sheet_region, country_code = row["ë°œí–‰êµ­"], row["ë°œí–‰êµ­ ë¶€í˜¸"]
            if normalize_region_for_code(sheet_region) == normalized_input:
                return country_code.strip() or "xxu"

        return "xxu"
    except Exception as e:
        st.write(f"âš ï¸ get_country_code_by_region ì˜ˆì™¸: {e}")
        return "xxu"

# =========================
# --- ë¬¸ì²´ë¶€ ê²€ìƒ‰ ---
# =========================
def get_mcst_address(publisher_name):
    url = "https://book.mcst.go.kr/html/searchList.php"
    params = {"search_area": "ì „ì²´", "search_state": "1", "search_kind": "1", 
              "search_type": "1", "search_word": publisher_name}
    debug_msgs = []
    try:
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        results = []
        for row in soup.select("table.board tbody tr"):
            cols = row.find_all("td")
            if len(cols) >= 4:
                reg_type = cols[0].get_text(strip=True)
                name = cols[1].get_text(strip=True)
                address = cols[2].get_text(strip=True)
                status = cols[3].get_text(strip=True)
                if status == "ì˜ì—…":
                    results.append((reg_type, name, address, status))
        if results:
            debug_msgs.append(f"[ë¬¸ì²´ë¶€] ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê±´")
            return results[0][2], results, debug_msgs
        else:
            debug_msgs.append("[ë¬¸ì²´ë¶€] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return "ë¯¸í™•ì¸", [], debug_msgs
    except Exception as e:
        debug_msgs.append(f"[ë¬¸ì²´ë¶€] ì˜ˆì™¸ ë°œìƒ: {e}")
        return "ì˜¤ë¥˜ ë°œìƒ", [], debug_msgs

def build_pub_location_bundle(isbn, publisher_name_raw):
    debug = []
    try:
        publisher_data, region_data, imprint_data = load_publisher_db()
        debug.append("âœ“ êµ¬ê¸€ì‹œíŠ¸ DB ì ì¬ ì„±ê³µ")

        kpipa_full, kpipa_norm, err = get_publisher_name_from_isbn_kpipa(isbn)
        if err: debug.append(f"KPIPA ê²€ìƒ‰: {err}")

        rep_name, aliases = split_publisher_aliases(kpipa_full or publisher_name_raw or "")
        resolved_pub_for_search = rep_name or (publisher_name_raw or "").strip()
        debug.append(f"ëŒ€í‘œ ì¶œíŒì‚¬ëª… ì¶”ì •: {resolved_pub_for_search} | ALIAS: {aliases}")

        place_raw, msgs = search_publisher_location_with_alias(resolved_pub_for_search, publisher_data)
        debug += msgs
        source = "KPIPA_DB"

        if place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ", None):
            place_raw, msgs = find_main_publisher_from_imprints(resolved_pub_for_search, imprint_data, publisher_data)
            debug += msgs
            if place_raw: source = "IMPRINTâ†’KPIPA"

        if not place_raw or place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
            mcst_addr, mcst_rows, mcst_dbg = get_mcst_address(resolved_pub_for_search)
            debug += mcst_dbg
            if mcst_addr not in ("ë¯¸í™•ì¸", "ì˜¤ë¥˜ ë°œìƒ", None):
                place_raw, source = mcst_addr, "MCST"

        if not place_raw or place_raw in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ", "ë¯¸í™•ì¸", "ì˜¤ë¥˜ ë°œìƒ"):
            place_raw, source = "ì¶œíŒì§€ ë¯¸ìƒ", "FALLBACK"
            debug.append("âš ï¸ ëª¨ë“  ê²½ë¡œ ì‹¤íŒ¨ â†’ 'ì¶œíŒì§€ ë¯¸ìƒ'")

        place_display = normalize_publisher_location_for_display(place_raw)
        country_code = get_country_code_by_region(place_raw, region_data)

        return {
            "place_raw": place_raw,
            "place_display": place_display,
            "country_code": country_code,
            "resolved_publisher": resolved_pub_for_search,
            "source": source,
            "debug": debug,
        }
    except Exception as e:
        return {
            "place_raw": "ì¶œíŒì§€ ë¯¸ìƒ",
            "place_display": "ì¶œíŒì§€ ë¯¸ìƒ",
            "country_code": "xxu",
            "resolved_publisher": publisher_name_raw or "",
            "source": "ERROR",
            "debug": [f"ì˜ˆì™¸: {e}"],
        }

def build_260(place_display: str, publisher_name: str, pubyear: str):
    place = (place_display or "ë°œí–‰ì§€ ë¯¸ìƒ")
    pub = (publisher_name or "ë°œí–‰ì ë¯¸ìƒ")
    year = (pubyear or "ë°œí–‰ë…„ ë¯¸ìƒ")
    return f"=260  \\1$a{place} :$b{pub},$c{year}"

def _today_yymmdd():
    return datetime.now().strftime("%y%m%d")

def _derive_date1(pubyear: str) -> str:
    y = (pubyear or "").strip()
    return y[:4] if re.fullmatch(r"\d{4}", y) else "19uu"

def patch_008_country_code(mrk_008_line: str, country_code: str = "xxu") -> str:
    if not mrk_008_line or not mrk_008_line.startswith("=008"):
        return mrk_008_line
    cc = (country_code or "xxu")[:3].ljust(3)
    header = mrk_008_line[:6]            # '=008  '
    body   = mrk_008_line[6:] or ""
    if len(body) < 40:
        body = body.ljust(40)
    body_list = list(body)
    body_list[15:18] = list(cc)          # ë³¸ë¬¸ 15â€“17
    return header + "".join(body_list)


# ==========================================================================================
# 056 ë‹¨ë… ì½”ë“œ
# ==========================================================================================

@dataclass
class BookInfo:
    title: str = ""
    author: str = ""
    pub_date: str = ""
    publisher: str = ""
    isbn13: str = ""
    category: str = ""
    description: str = ""
    toc: str = ""
    extra: Optional[Dict[str, Any]] = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_text(s: Optional[str]) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def first_match_number(text: str) -> Optional[str]:
    """KDC ìˆ«ìë§Œ ì¶”ì¶œ: 0~999 ë˜ëŠ” ì†Œìˆ˜ì  í¬í•¨(ì˜ˆ: 813.7)"""
    if not text:
        return None
    m = re.search(r"\b([0-9]{1,3}(?:\.[0-9]+)?)\b", text)
    return m.group(1) if m else None

def first_or_empty(lst):
    return lst[0] if lst else ""

def strip_tags(html_text: str) -> str:
    return re.sub(r"<[^>]+>", " ", html_text)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) ì•Œë¼ë”˜ API ìš°ì„  â”€â”€â”€â”€â”€â”€â”€â”€â”€
def aladin_lookup_by_api(isbn13: str, ttbkey: str) -> Optional[BookInfo]:
    if not ttbkey:
        return None
    params = {
        "ttbkey": ttbkey,
        "itemIdType": "ISBN13",
        "ItemId": isbn13,
        "output": "js",
        "Version": "20131101",
        "OptResult": "authors,categoryName,fulldescription,toc,packaging,ratings"
    }
    try:
        r = requests.get("https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx", params=params, headers=HEADERS, timeout=15)
        r.raise_for_status()
        data = r.json()
        items = data.get("item", [])
        if not items:
            # ë””ë²„ê·¸: APIê°€ ë¹„ì–´ìˆìœ¼ë©´ ì´ìœ ë¥¼ í™”ë©´ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆê²Œ
            st.info("ì•Œë¼ë”˜ API(ItemLookUp)ì—ì„œ ê²°ê³¼ ì—†ìŒ â†’ ìŠ¤í¬ë ˆì´í•‘ ë°±ì—… ì‹œë„")
            return None
        it = items[0]
        return BookInfo(
            title=clean_text(it.get("title")),
            author=clean_text(it.get("author")),
            pub_date=clean_text(it.get("pubDate")),
            publisher=clean_text(it.get("publisher")),
            isbn13=clean_text(it.get("isbn13")) or isbn13,
            category=clean_text(it.get("categoryName")),
            description=clean_text(it.get("fulldescription")) or clean_text(it.get("description")),
            toc=clean_text(it.get("toc")),
            extra=it,
        )
    except Exception as e:
        st.info(f"ì•Œë¼ë”˜ API í˜¸ì¶œ ì˜ˆì™¸ â†’ {e} / ìŠ¤í¬ë ˆì´í•‘ ë°±ì—… ì‹œë„")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) ì•Œë¼ë”˜ ì›¹ ìŠ¤í¬ë ˆì´í•‘(ë°±ì—…) â”€â”€â”€â”€â”€â”€â”€â”€â”€


def aladin_lookup_by_web(isbn13: str) -> Optional[BookInfo]:
    try:
        # ê²€ìƒ‰ URL (Book íƒ€ê²Ÿ ìš°ì„ )
        params = {"SearchTarget": "Book", "SearchWord": f"isbn:{isbn13}"}
        sr = requests.get(ALADIN_SEARCH_URL, params=params, headers=HEADERS, timeout=15)
        sr.raise_for_status()

        soup = BeautifulSoup(sr.text, "html.parser")

        # 1) ê°€ì¥ ì•ˆì •ì ì¸ ì¹´ë“œ íƒ€ì´í‹€ ë§í¬ (a.bo3)
        link_tag = soup.select_one("a.bo3")
        item_url = None
        if link_tag and link_tag.get("href"):
            item_url = urllib.parse.urljoin("https://www.aladin.co.kr", link_tag["href"])

        # 2) ë°±ì—…: ì •ê·œì‹ìœ¼ë¡œ wproduct ë§í¬ ì¡ê¸°(ìŒ/í™‘ë”°ì˜´í‘œ ëª¨ë‘)
        if not item_url:
            m = re.search(r'href=[\'"](/shop/wproduct\.aspx\?ItemId=\d+[^\'"]*)[\'"]', sr.text, re.I)
            if m:
                item_url = urllib.parse.urljoin("https://www.aladin.co.kr", html.unescape(m.group(1)))

        # 3) ê·¸ë˜ë„ ì—†ìœ¼ë©´, ì²« ìƒí’ˆ ì¹´ë“œ ë‚´ ë‹¤ë¥¸ ë§í¬ ì‹œë„
        if not item_url:
            first_card = soup.select_one(".ss_book_box, .ss_book_list")
            if first_card:
                a = first_card.find("a", href=True)
                if a:
                    item_url = urllib.parse.urljoin("https://www.aladin.co.kr", a["href"])

        if not item_url:
            st.warning("ì•Œë¼ë”˜ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            with st.expander("ë””ë²„ê·¸: ê²€ìƒ‰ í˜ì´ì§€ HTML ì¼ë¶€"):
                st.code(sr.text[:2000])
            return None

        # ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ìš”ì²­
        pr = requests.get(item_url, headers=HEADERS, timeout=15)
        pr.raise_for_status()
        psoup = BeautifulSoup(pr.text, "html.parser")

        # ë©”íƒ€ íƒœê·¸ë¡œ ê¸°ë³¸ ì •ë³´ í™•ë³´
        og_title = psoup.select_one('meta[property="og:title"]')
        og_desc  = psoup.select_one('meta[property="og:description"]')
        title = clean_text(og_title["content"]) if og_title and og_title.has_attr("content") else ""
        desc  = clean_text(og_desc["content"]) if og_desc and og_desc.has_attr("content") else ""

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë°±ì—…(ê¸¸ì´ ì œí•œ)
        body_text = clean_text(psoup.get_text(" "))[:4000]
        description = desc or body_text

        # ì €ì/ì¶œíŒì‚¬/ì¶œê°„ì¼ ì¶”ì¶œ(ìˆìœ¼ë©´)
        author = ""
        publisher = ""
        pub_date = ""
        cat_text = ""

        # ìƒí’ˆ ì •ë³´ í‘œì—ì„œ í‚¤ì›Œë“œë¡œ ì¶”ì¶œ ì‹œë„
        info_box = psoup.select_one("#Ere_prod_allwrap, #Ere_prod_mconts_wrap, #Ere_prod_titlewrap")
        if info_box:
            text = clean_text(info_box.get_text(" "))
            # ì•„ì£¼ ëŠìŠ¨í•œ íŒ¨í„´(ìˆì„ ë•Œë§Œ ì¡í˜)
            m_author = re.search(r"(ì €ì|ì§€ì€ì´)\s*:\s*([^\|Â·/]+)", text)
            m_publisher = re.search(r"(ì¶œíŒì‚¬)\s*:\s*([^\|Â·/]+)", text)
            m_pubdate = re.search(r"(ì¶œê°„ì¼|ì¶œíŒì¼)\s*:\s*([0-9]{4}\.[0-9]{1,2}\.[0-9]{1,2})", text)
            if m_author:   author   = clean_text(m_author.group(2))
            if m_publisher: publisher = clean_text(m_publisher.group(2))
            if m_pubdate:  pub_date = clean_text(m_pubdate.group(2))

        # ì¹´í…Œê³ ë¦¬(ë¹µë¶€ìŠ¤ëŸ¬ê¸°) ì‹œë„
        crumbs = psoup.select(".location, .path, .breadcrumb")
        if crumbs:
            cat_text = clean_text(" > ".join(c.get_text(" ") for c in crumbs))

        # ë””ë²„ê·¸: ì–´ëŠ ë§í¬ë¡œ ë“¤ì–´ê°”ëŠ”ì§€/íƒ€ì´í‹€ í™•ì¸
        with st.expander("ë””ë²„ê·¸: ìŠ¤í¬ë ˆì´í•‘ ì§„ì… URL / íŒŒì‹± ê²°ê³¼"):
            st.write({"item_url": item_url, "title": title})
        
        return BookInfo(
            title=title,
            description=description,
            isbn13=isbn13,
            author=author,
            publisher=publisher,
            pub_date=pub_date,
            category=cat_text
        )
    except Exception as e:
        st.error(f"ì›¹ ìŠ¤í¬ë ˆì´í•‘ ì˜ˆì™¸: {e}")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) ì±—Gì—ê²Œ 'KDC ìˆ«ìë§Œ' ìš”ì²­ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask_llm_for_kdc(book: BookInfo, api_key: str, model: str = DEFAULT_MODEL) -> Optional[str]:

    if model is None:
        # secrets â†’ env â†’ í•˜ë“œì½”ë”© ìˆœìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì„ íƒ
        try:
            model = (st.secrets.get("openai", {}) or {}).get("model", "")
        except Exception:
            model = ""
        if not model:
            model = "gpt-4o-mini"

    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ ì‹­ì§„ë¶„ë¥˜(KDC) ì „ë¬¸ê°€ë‹¤. "
        "ì•„ë˜ ë„ì„œ ì •ë³´ë¥¼ ë³´ê³  KDC ë¶„ë¥˜ê¸°í˜¸ë¥¼ 'ìˆ«ìë§Œ' ì¶œë ¥í•´ë¼. "
        "í˜•ì‹ ì˜ˆì‹œ: 813.7 / 325.1 / 005 / 181 ë“±. "
        "ì„¤ëª…, ì ‘ë‘/ì ‘ë¯¸ í…ìŠ¤íŠ¸, ê¸°íƒ€ ë¬¸ìëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆë¼."
    )
    payload = {
        "title": book.title,
        "author": book.author,
        "publisher": book.publisher,
        "pub_date": book.pub_date,
        "isbn13": book.isbn13,
        "category": book.category,
        "description": book.description,
        "toc": book.toc,
    }
    user_prompt = (
        "ë„ì„œ ì •ë³´(JSON):\n"
        f"{json.dumps(payload, ensure_ascii=False, indent=2)}\n\n"
        "KDC ìˆ«ìë§Œ ì¶œë ¥:"
    )

    try:
        resp = requests.post(
            OPENAI_CHAT_COMPLETIONS,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": 0.0,
                "max_tokens": 8,
            },
            timeout=30,
        )
        resp.raise_for_status()
        data = resp.json()
        text = (data["choices"][0]["message"]["content"] or "").strip()
        return first_match_number(text)
    except Exception as e:
        st.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) íŒŒì´í”„ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_kdc_from_isbn(isbn13: str, ttbkey: Optional[str], openai_key: str, model: str) -> Optional[str]:
    info = aladin_lookup_by_api(isbn13, ttbkey) if ttbkey else None
    if not info:
        info = aladin_lookup_by_web(isbn13)
    if not info:
        st.warning("ì•Œë¼ë”˜ì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
    code = ask_llm_for_kdc(info, api_key=openai_key, model=model)
    # ë””ë²„ê·¸ìš©: ì–´ë–¤ ì •ë³´ë¥¼ ë„˜ê²¼ëŠ”ì§€ ë³´ì—¬ì£¼ê¸°(ê°œì¸ì •ë³´ ì—†ìŒ)
    with st.expander("LLM ì…ë ¥ ì •ë³´(í™•ì¸ìš©)"):
        st.json({
            "title": info.title,
            "author": info.author,
            "publisher": info.publisher,
            "pub_date": info.pub_date,
            "isbn13": info.isbn13,
            "category": info.category,
            "description": (info.description[:600] + "â€¦") if info.description and len(info.description) > 600 else info.description,
            "toc": info.toc,
        })
    return code

# (ê¹€: ì¶”ê°€) mrc íŒŒì¼ ìƒì„± (ê°ì²´ë³€í™˜)
def mrk_str_to_field(mrk_str):
    """MRK ë¬¸ìì—´ì„ Field ê°ì²´ë¡œ ë³€í™˜ (Subfield ê°ì²´ ì‚¬ìš©)"""
    if not mrk_str or not mrk_str.startswith('='):
        return None
    tag = mrk_str[1:4]
    # Control field(008, 001 ë“±) ì²´í¬
    if tag in ['008', '001', '005', '006']:
        # Control FieldëŠ” dataë§Œ ì‚¬ìš©, indicators/subfields ì—†ìŒ
        data = mrk_str[6:]  # '=008  20231009...' â†’ '20231009...'
        return Field(tag=tag, data=data)
    
    raw_ind = mrk_str[6:8]
    indicators = list(raw_ind) if raw_ind.strip() else [' ', ' ']
    subfields = []
    parts = mrk_str.split('$')[1:]
    for part in parts:
        if len(part) < 2:
            continue
        code = part[0]
        value = part[1:].strip()
        subfields.append(Subfield(code, value))
    return Field(tag=tag, indicators=indicators, subfields=subfields)

# =========================================================================================

def generate_all_oneclick(isbn: str, reg_mark: str = "", reg_no: str = "", copy_symbol: str = "", use_ai_940: bool = True):
    global CURRENT_DEBUG_LINES
    CURRENT_DEBUG_LINES = []
    pieces = []
    
    author_raw, _ = fetch_nlk_author_only(isbn)
    item = fetch_aladin_item(isbn)

    # 245 / 246 / 700
    marc245 = build_245_with_people_from_sources(item, author_raw, prefer="aladin")
    f_245 = mrk_str_to_field(marc245)
    marc246 = build_246_from_aladin_item(item)
    f_246 = mrk_str_to_field(marc246)
    mrk_700 = build_700_people_pref_aladin(author_raw, item) or []

    # 90010: LODì—ì„œ ì›ì–´ëª… ê°€ì ¸ì˜¤ê¸° (ì§€ì€ì´+ì˜®ê¸´ì´)
    people = extract_people_from_aladin(item) if item else {}
    mrk_90010 = build_90010_from_wikidata(people, include_translator=True)

    # 940: 245 $aë§Œìœ¼ë¡œ ìƒì„±, $n ìˆìœ¼ë©´ ìˆ«ì ì½ê¸° ê¸ˆì§€
    a_out, n = parse_245_a_n(marc245)
    mrk_940 = build_940_from_title_a(a_out, use_ai=use_ai_940, disable_number_reading=bool(n))

    # â‘  041/546 (ë„¤ ìµœì¢… get_kormarc_tags ì‚¬ìš©)
    tag_041_text = tag_546_text = _orig = None
    try:
        res = get_kormarc_tags(isbn)  # (tag_041:str, tag_546_text:str, original_title:str) ê¸°ëŒ€
        if isinstance(res, (list, tuple)) and len(res) == 3:
            tag_041_text, tag_546_text, _orig = res
        # ì•Œë¼ë”˜/í¬ë¡¤ë§ ì˜ˆì™¸ ì‹œ "ğŸ“• ì˜ˆì™¸ ë°œìƒ:" ê°™ì€ ë¬¸ìì—´ì´ ì˜¬ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ë°©ì–´
        if isinstance(tag_041_text, str) and tag_041_text.startswith("ğŸ“• ì˜ˆì™¸ ë°œìƒ"):
            tag_041_text = None
        if isinstance(tag_546_text, str) and tag_546_text.startswith("ğŸ“• ì˜ˆì™¸ ë°œìƒ"):
            tag_546_text = None
    except Exception:
        tag_041_text = None
        tag_546_text = None


    # 260 ë°œí–‰ì‚¬í•­
    publisher_raw = (item or {}).get("publisher", "")          
    pubdate       = (item or {}).get("pubDate", "") or ""      
    pubyear       = (pubdate[:4] if len(pubdate) >= 4 else "") 

    bundle = build_pub_location_bundle(isbn, publisher_raw)     
    dbg(
        "ğŸ“[BUNDLE]",
        f"source={bundle.get('source')}",
        f"place_raw={bundle.get('place_raw')}",
        f"place_display={bundle.get('place_display')}",
        f"country_code={bundle.get('country_code')}",
    )
    for m in (bundle.get("debug") or []):
        dbg("[BUNDLE]", m)

    tag_260 = build_260(                                      
        place_display=bundle["place_display"],
        publisher_name=bundle["resolved_publisher"] or publisher_raw,
        pubyear=pubyear,
    )
    f_260 = mrk_str_to_field(tag_260)

     # â‘¡ 008 (041ì˜ $aë¡œ lang3 override)
    title   = (item or {}).get("title","") or ""
    category= (item or {}).get("categoryName","") or ""
    desc    = (item or {}).get("description","") or ""
    toc     = ((item or {}).get("subInfo",{}) or {}).get("toc","") or ""
    lang3_override = _lang3_from_tag041(tag_041_text) if tag_041_text else None
    
    data_008 = build_008_from_isbn(
        isbn,
        aladin_pubdate=(item or {}).get("pubDate","") or "",
        aladin_title=(item or {}).get("title","") or "",
        aladin_category=(item or {}).get("categoryName","") or "",
        aladin_desc=(item or {}).get("description","") or "",
        aladin_toc=((item or {}).get("subInfo",{}) or {}).get("toc","") or "",
        override_lang3=lang3_override,
        cataloging_src="a",
    )
    tag_008 = patch_008_country_code(tag_008, bundle["country_code"]) # â˜… 008 ì•ˆì˜ 15â€“17(ë°œí–‰êµ­ì½”ë“œ)ë§Œ ì•ˆì „í•˜ê²Œ ë®ì–´ì“°ê¸°
    field_008 = Field(tag="008", data=tag_008.replace("=008  ", ""))

    # â‘¢ 020 (ê°€ê²© + NLK ë¶€ê°€ê¸°í˜¸)
    tag_020 = _build_020_from_item_and_nlk(isbn, item)
    f_020 = mrk_str_to_field(tag_020)

    # â˜… 056 (KDC) â€” ì•Œë¼ë”˜/ìŠ¤í¬ë ˆì´í•‘ + LLMë¡œ ìˆ«ìë§Œ ë°›ì•„ ìƒì„±
    kdc_code = None
    try:
        kdc_code = get_kdc_from_isbn(isbn, ttbkey=ALADIN_TTB_KEY, openai_key=openai_key, model=model)
    # ìˆ«ì í¬ë§· ê²€ì¦(ì•ˆì „)
        if kdc_code and not re.fullmatch(r"\d{1,3}(?:\.\d+)?", kdc_code):
            kdc_code = None
    except Exception as e:
        dbg_err(f"056 ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
    tag_056 = f"=056  \\\\$a{kdc_code}$2KDC10" if kdc_code else None  # $2ëŠ” ì‚¬ìš©í•˜ëŠ” íŒìœ¼ë¡œ(KDC10 ë“±)
    f_056 = mrk_str_to_field(tag_056)

    # â‘£ 653 (GPT)
    tag_653 = _build_653_via_gpt(item)
    f_653 = mrk_str_to_field(tag_653)

    # 950 (ê°€ê²©ë§Œ ë”°ë¡œ ìƒì„±)
    tag_950 = build_950_from_item_and_price(item, isbn)
    f_950 = mrk_str_to_field(tag_950)
    
    # 049
    field_049 = build_049(reg_mark, reg_no, copy_symbol)
    f_049 = mrk_str_to_field(field_049)    

    # 700 ì •ë ¬
    mrk_strings = [m for f, m in pieces]
    mrk_strings = _fix_700_order_with_nationality(
        mrk_strings,
        _east_asian_konames_from_prov(LAST_PROV_90010)
    )

    # Record ê°ì²´ ìƒì„±
    record = Record(force_utf8=True)
    for f, _ in pieces:
        record.add_field(f)
     # MRK ë¬¸ìì—´ ë³‘í•©
    combined = "\n".join(mrk_strings).strip()

    # =====================
    # ìˆœì„œëŒ€ë¡œ ì¡°ë¦½ (MRK ì¶œë ¥ ìˆœì„œ ìœ ì§€)
    # ====================
    pieces.append((field_008, tag_008))
    if f_020: pieces.append((f_020, tag_020))
    if tag_041_text:
        f_041 = mrk_str_to_field(_as_mrk_041(tag_041_text))
        if f_041: pieces.append((f_041, _as_mrk_041(tag_041_text)))
    if f_056: pieces.append((f_056, tag_056))
    if f_245: pieces.append((f_245, marc245))
    if f_246: pieces.append((f_246, marc246))
    if f_260: pieces.append((f_260, tag_260))
    if tag_546_text:
        f_546 = mrk_str_to_field(_as_mrk_546(tag_546_text))
        if f_546: pieces.append((f_546, _as_mrk_546(tag_546_text)))
    if f_653: pieces.append((f_653, tag_653))
    for m in mrk_700:
        f = mrk_str_to_field(m)
        if f: pieces.append((f, m))
    for m in mrk_90010:
        f = mrk_str_to_field(m)
        if f: pieces.append((f, m))
    for m in mrk_940:
        f = mrk_str_to_field(m)
        if f: pieces.append((f, m))
    if f_950: pieces.append((f_950, tag_950))
    if f_049: pieces.append((f_049, field_049))

    # ë©”íƒ€ì •ë³´
    meta = {
        "TitleA": a_out,
        "has_n": bool(n),
        "700_count": sum(1 for x in mrk_strings if x.startswith("=700")),
        "90010_count": sum(1 for x in mrk_strings if x.startswith("=90010")),
        "940_count": len(mrk_940),
        "Candidates": get_candidate_names_for_isbn(isbn),
        "041": tag_041_text,
        "546": tag_546_text,
        "008": tag_008,
        "020": tag_020,
        "056": tag_056,
        "653": tag_653,
        "kdc_code": kdc_code,
        "price_for_950": _extract_price_kr(item, isbn),
        "Publisher_raw": publisher_raw,
        "pubyear": pubyear,
        "Place_display": bundle.get("place_display"),
        "CountryCode_008": bundle.get("country_code"),
        "Publisher_resolved": bundle.get("resolved_publisher"),
        "Bundle_source": bundle.get("source"),
        "debug_lines": list(CURRENT_DEBUG_LINES),
        "Provenance": {"90010": LAST_PROV_90010},
    }

    return record, combined, meta

# =========================
# ğŸ›ï¸ Streamlit UI
# =========================

st.header("ğŸ“š ISBN â†’ MARC (ì¼ê´„ ì²˜ë¦¬ ì§€ì›)")
st.checkbox("ğŸ§  940 ìƒì„±ì— OpenAI í™œìš©", value=True, key="use_ai_940")

# ë‹¨ê±´ ì…ë ¥
single_isbn = st.text_input("ğŸ”¹ ë‹¨ì¼ ISBN", placeholder="ì˜ˆ: 9788937462849").strip()

# CSV ì—…ë¡œë” (ì—´: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)
uploaded = st.file_uploader("ğŸ“ CSV ì—…ë¡œë“œ (UTF-8, ì—´: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)", type=["csv"])

# ì…ë ¥ ìˆ˜ì§‘
jobs = []
if single_isbn:
    jobs.append([single_isbn, "", "", ""])

if uploaded is not None:
    try:
        df = load_uploaded_csv(uploaded)
    except Exception as e:
        st.error(f"âŒ CSV ì½ê¸° ì‹¤íŒ¨: {e}")
        st.stop()

    # í•„ìš”í•œ ì»¬ëŸ¼ ì²´í¬
    need_cols = {"ISBN", "ë“±ë¡ê¸°í˜¸", "ë“±ë¡ë²ˆí˜¸", "ë³„ì¹˜ê¸°í˜¸"}
    if not need_cols.issubset(df.columns):
        st.error("âŒ í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸")
        st.stop()

    # ISBN ìˆëŠ” í–‰ë§Œ, ë³„ì¹˜ê¸°í˜¸ NaN -> ""
    rows = df[["ISBN", "ë“±ë¡ê¸°í˜¸", "ë“±ë¡ë²ˆí˜¸", "ë³„ì¹˜ê¸°í˜¸"]].dropna(subset=["ISBN"]).copy()
    rows["ë³„ì¹˜ê¸°í˜¸"] = rows["ë³„ì¹˜ê¸°í˜¸"].fillna("")

    jobs.extend(rows.values.tolist())

if st.button("ğŸš€ ë³€í™˜ ì‹¤í–‰", disabled=not jobs):
    # ì§„í–‰ ì•ˆë‚´
    st.write(f"ì´ {len(jobs)}ê±´ ì²˜ë¦¬ ì¤‘â€¦")
    prog = st.progress(0)

    marc_all: list[str] = []
    st.session_state.meta_all = {}
    results: list[tuple[Record, str, str, dict]] = []  # (Record, isbn, combined, meta)

    for i, (isbn, reg_mark, reg_no, copy_symbol) in enumerate(jobs, start=1):
        # ì›í´ë¦­ ë³€í™˜ (ë‚´ë¶€ì—ì„œ 245/246/700/90010(LOD)/940ê¹Œì§€ ìƒì„±)
        record, combined, meta = generate_all_oneclick(
            isbn,
            reg_mark=reg_mark,
            reg_no=reg_no,
            copy_symbol=copy_symbol,
            use_ai_940=st.session_state.get("use_ai_940", True),
        )

        # í™”ë©´ ì¶œë ¥ (í›„ë³´ì €ì + ìƒì„± ì¹´ìš´íŠ¸ë“¤)
        cand = ", ".join(meta.get("Candidates", []))
        c700 = meta.get("700_count", None)
        c90010 = meta.get("90010_count", 0)
        c940 = meta.get("940_count", 0)
        st.caption(f"ISBN: {isbn}  |  í›„ë³´ì €ì: {cand}  | 700={c700 if c700 is not None else 'â€”'}  90010={c90010}  940={c940}")
        st.code(combined, language="text")
        with st.expander(f"ğŸ§­ ë©”íƒ€ ë³´ê¸° Â· {isbn}", expanded=True):
            if meta:
        # 1) ë©”íƒ€ ìš”ì•½(JSON) â€” debug_lines ì œì™¸
                safe_meta = {k: v for k, v in meta.items() if k != "debug_lines"}
                st.subheader("Meta (ìš”ì•½)")
                st.json(safe_meta)

        # 2) ë””ë²„ê·¸: í•­ìƒ í‘œì‹œ
                dbg_lines = meta.get("debug_lines") or []
                st.subheader("Debug Lines")
            if dbg_lines:
            # ê¸¸ë©´ ìë™ ìŠ¤í¬ë¡¤ ë˜ëŠ” ì˜ì—­ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ
                st.text("\n".join(str(x) for x in dbg_lines))
            # í•„ìš”í•˜ë©´ í…ìŠ¤íŠ¸ ì˜ì—­ ì‚¬ìš©:
            # st.text_area("Debug", value="\n".join(map(str, dbg_lines)), height=240)
            else:
                st.caption("í‘œì‹œí•  ë””ë²„ê·¸ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ëˆ„ì 
        marc_all.append(combined)
        st.session_state.meta_all[isbn] = meta
        results.append((isbn, combined, meta))
        prog.progress(i / len(jobs))

    # ì¼ê´„ ë‹¤ìš´ë¡œë“œ (UTF-8-SIG â†’ ì—‘ì…€/ë©”ëª¨ì¥ í˜¸í™˜)
    blob = ("\n\n".join(marc_all)).encode("utf-8-sig")
    st.download_button(
        "ğŸ“¦ ëª¨ë“  MARC ë‹¤ìš´ë¡œë“œ",
        data=blob,
        file_name="marc_output.txt",
        mime="text/plain",
        key="dl_all_marc",
    )
    # (ê¹€: ì¶”ê°€) ğŸ’¾ MRC ë‹¤ìš´ë¡œë“œ (TXT ë°”ë¡œ ì•„ë˜)
    buffer = io.BytesIO()
    writer = MARCWriter(buffer)
    for record_obj, isbn, _, _ in results:
        if not isinstance(record_obj, Record):
            st.warning(f"âš ï¸ MRC ë³€í™˜ ì‹¤íŒ¨: Record ê°ì²´ê°€ ì•„ë‹˜, {isbn}")
            continue
        writer.write(record_obj)
        
    buffer.seek(0)
    st.download_button(
        label="ğŸ“¥ MRC íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        data=buffer,
        file_name="marc_output.mrc",
        mime="application/octet-stream",
        key="dl_mrc",
    )
    st.session_state["last_results"] = results


with st.expander("âš™ï¸ ì‚¬ìš© íŒ"):
    st.markdown(
        """
- ì €ìëª…: **NLK SearchApi(JSON)** â†’ `AUTHOR` íŒŒì‹±  
  (ì—­í• ì–´: ê¸€Â·ê·¸ë¦¼/ì˜®ê¸´ì´/ì €ì/ì—­ ë“± ì œê±°Â·ë¶„ë¦¬)  
  â†’ ì•„ì‹œì•„ê¶Œì€ **ê·¸ëŒ€ë¡œ(KEEP)**, ê·¸ ì™¸ëŠ” **â€˜ì„±, ì´ë¦„â€™**ìœ¼ë¡œ ì •ë ¬í•´ `=700  1\\$aâ€¦`.

- ì„œëª…/ë¶€ì œ: **ì•Œë¼ë”˜ TTB** `title`/`subInfo.subTitle` â†’ `=245  00$aâ€¦ :$bâ€¦`  
  (ë¶€ì œ ì—†ìœ¼ë©´ íƒ€ì´í‹€ ë¶„í•´ ê·œì¹™ ì ìš©). `$a`ëŠ” ê³µë°± **ìœ ì§€**.
        """
    )

# í•´ì•¼í•  ê²ƒ
#1. ë°œí–‰êµ­ ë¶€í˜¸ ì˜¤ë¥˜ ìˆ˜ì •
#2. mrc ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€
#3. ê¸°íƒ€ ê²¹ì¹˜ëŠ” ë¶€ë¶„ ì‚­ì œ





