import streamlit as st
import requests
import re
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import copy

# --- êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° í•œë²ˆë§Œ ì½ê¸° ë° ìºì‹± ---
@st.cache_data(ttl=3600)
def load_publisher_db():
    json_key = dict(st.secrets["gspread"])
    json_key["private_key"] = json_key["private_key"].replace('\\n', '\n')

    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
    client = gspread.authorize(creds)
    publisher_sheet = client.open("ì¶œíŒì‚¬ DB").worksheet("ì‹œíŠ¸3")
    region_sheet = client.open("ì¶œíŒì‚¬ DB").worksheet("Sheet2")

    publisher_data = publisher_sheet.get_all_values()[1:]  # í—¤ë” ì œì™¸
    region_data = region_sheet.get_all_values()[1:]      # í—¤ë” ì œì™¸

    return publisher_data, region_data


# --- ì¶œíŒì‚¬ëª… ì •ê·œí™”(êµ¬ê¸€ì‹œíŠ¸ ëŒ€ì¡°ìš©) ---
def normalize_publisher_name(name):
    return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬|í”„ë ˆìŠ¤", "", name).lower()


# --- ì¶œíŒì‚¬ ì§€ì—­ëª… í‘œì‹œìš© ì •ê·œí™” (UI/260ì— ì“¸ ì´ë¦„) ---
def normalize_publisher_location_for_display(location_name):
    if not location_name or location_name in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
        return location_name
    location_name = location_name.strip()
    major_cities = ["ì„œìš¸", "ì¸ì²œ", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ëŒ€êµ¬", "ë¶€ì‚°", "ì„¸ì¢…"]
    for city in major_cities:
        if city in location_name:
            return location_name[:2]
    parts = location_name.split()
    loc = parts[1] if len(parts) > 1 else parts[0]
    if loc.endswith("ì‹œ") or loc.endswith("êµ°"):
        loc = loc[:-1]
    return loc


# --- êµ¬ê¸€ì‹œíŠ¸(publisher_data)ì—ì„œ ì¶œíŒì‚¬ â†’ ì§€ì—­ ì¡°íšŒ (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©) ---
def get_publisher_location(publisher_name, publisher_data):
    try:
        st.write(f"ğŸ“¥ ì¶œíŒì‚¬ ì§€ì—­ì„ êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤... `{publisher_name}`")
        target = normalize_publisher_name(publisher_name)
        st.write(f"ğŸ§ª ì •ê·œí™”ëœ ì…ë ¥ê°’: `{target}`")

        for row in publisher_data:
            if len(row) < 3:
                continue
            sheet_name, region = row[1], row[2]
            if normalize_publisher_name(sheet_name) == target:
                return region.strip() or "ì¶œíŒì§€ ë¯¸ìƒ"

        # fallback: ì›ë³¸ ë¬¸ìì—´ ì¼ì¹˜
        for row in publisher_data:
            if len(row) < 3:
                continue
            sheet_name, region = row[1], row[2]
            if sheet_name.strip() == publisher_name.strip():
                return region.strip() or "ì¶œíŒì§€ ë¯¸ìƒ"

        return "ì¶œíŒì§€ ë¯¸ìƒ"
    except Exception as e:
        st.write(f"âš ï¸ get_publisher_location ì˜ˆì™¸: {e}")
        return "ì˜ˆì™¸ ë°œìƒ"


# --- ì¶œíŒì‚¬ëª…ì—ì„œ ëŒ€í‘œëª…ê³¼ ë³„ì¹­(ê´„í˜¸/ìŠ¬ë˜ì‹œ ë¶„ë¦¬) ì¶”ì¶œ ---
def split_publisher_aliases(name):
    aliases = []

    # ê´„í˜¸ ì•ˆ ë‚´ìš© ì¶”ì¶œ, ì‰¼í‘œë‚˜ ìŠ¬ë˜ì‹œë¡œ ë‚˜ëˆ„ê¸°
    bracket_contents = re.findall(r"\((.*?)\)", name)
    for content in bracket_contents:
        parts = re.split(r"[,/]", content)
        parts = [p.strip() for p in parts if p.strip()]
        aliases.extend(parts)

    # ê´„í˜¸ ì œê±°
    name_no_brackets = re.sub(r"\(.*?\)", "", name).strip()

    # ìŠ¬ë˜ì‹œ ë¶„ë¦¬
    if "/" in name_no_brackets:
        parts = [p.strip() for p in name_no_brackets.split("/") if p.strip()]
        rep_name = parts[0]
        aliases.extend(parts[1:])
    else:
        rep_name = name_no_brackets

    return rep_name, aliases


# --- ê´„í˜¸/ë³„ì¹­ ë¶„ë¦¬ í›„ ë‘ë²ˆ ê²€ìƒ‰ ì ìš©í•œ ì¶œíŒì§€ ì¡°íšŒ ---
def search_publisher_location_with_alias(publisher_name, publisher_data):
    rep_name, aliases = split_publisher_aliases(publisher_name)

    st.write(f"ğŸ” ëŒ€í‘œëª…ìœ¼ë¡œ 1ì°¨ ê²€ìƒ‰: `{rep_name}`")
    location = get_publisher_location(rep_name, publisher_data)
    if location != "ì¶œíŒì§€ ë¯¸ìƒ":
        return location

    # 1ì°¨ì—ì„œ ë¯¸ìƒì¼ ê²½ìš° ë³„ì¹­ìœ¼ë¡œ 2ì°¨ ê²€ìƒ‰
    for alias in aliases:
        st.write(f"ğŸ” ë³„ì¹­ìœ¼ë¡œ 2ì°¨ ê²€ìƒ‰ ì‹œë„: `{alias}`")
        location = get_publisher_location(alias, publisher_data)
        if location != "ì¶œíŒì§€ ë¯¸ìƒ":
            return location

    return "ì¶œíŒì§€ ë¯¸ìƒ"


# --- êµ¬ê¸€ì‹œíŠ¸(region_data)ë¡œ ë°œí–‰êµ­ ë¶€í˜¸ ì¡°íšŒ (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©) ---
def get_country_code_by_region(region_name, region_data):
    try:
        st.write(f"ğŸŒ ë°œí–‰êµ­ ë¶€í˜¸ ì°¾ëŠ” ì¤‘... ì°¸ì¡° ì§€ì—­: `{region_name}`")

        def normalize_region_for_code(region):
            region = (region or "").strip()
            if region.startswith(("ì „ë¼", "ì¶©ì²­", "ê²½ìƒ")):
                if len(region) >= 3:
                    return region[0] + region[2]
                return region[:2]
            return region[:2]

        normalized_input = normalize_region_for_code(region_name)
        st.write(f"ğŸ§ª ì •ê·œí™”ëœ ì°¸ì¡°ì§€ì—­(ì½”ë“œëŒ€ì¡°ìš©): `{normalized_input}`")

        for row in region_data:
            if len(row) < 2:
                continue
            sheet_region, country_code = row[0], row[1]
            if normalize_region_for_code(sheet_region) == normalized_input:
                return country_code.strip() or "xxu"

        return "xxu"
    except Exception as e:
        st.write(f"âš ï¸ get_country_code_by_region ì˜ˆì™¸: {e}")
        return "xxu"


# --- Aladin API: ISBNìœ¼ë¡œ ë„ì„œ ì •ë³´ ì¡°íšŒ (title, author, publisher, pubyear, 245 í•„ë“œ) ---
def search_aladin_by_isbn(isbn):
    try:
        ttbkey = st.secrets["aladin"]["ttbkey"]
        url = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        params = {
            "ttbkey": ttbkey,
            "itemIdType": "ISBN",
            "ItemId": isbn,
            "output": "js",
            "Version": "20131101"
        }
        res = requests.get(url, params=params, timeout=15)
        if res.status_code != 200:
            return None, f"API ìš”ì²­ ì‹¤íŒ¨ (status: {res.status_code})"

        data = res.json()
        if "item" not in data or not data["item"]:
            return None, f"ë„ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. [ì‘ë‹µ: {data}]"

        book = data["item"][0]
        title = book.get("title", "ì œëª© ì—†ìŒ")
        author = book.get("author", "")
        publisher = book.get("publisher", "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ")
        pubdate = book.get("pubDate", "")
        pubyear = pubdate[:4] if len(pubdate) >= 4 else "ë°œí–‰ë…„ë„ ì—†ìŒ"

        authors = [a.strip() for a in author.split(",")] if author else []
        creator_str = " ; ".join(authors) if authors else "ì €ì ì •ë³´ ì—†ìŒ"
        field_245 = f"=245  10$a{title} /$c{creator_str}"

        return {
            "title": title,
            "creator": creator_str,
            "publisher": publisher,
            "pubyear": pubyear,
            "245": field_245
        }, None

    except Exception as e:
        return None, f"Aladin API ì˜ˆì™¸: {e}"


# --- Aladin í¬ë¡¤ë§: í˜•íƒœì‚¬í•­(ìª½ìˆ˜/í¬ê¸°) ì¶”ì¶œ (300 í•„ë“œ ìƒì„±) ---
def extract_physical_description_by_crawling(isbn):
    try:
        search_url = f"https://www.aladin.co.kr/search/wsearchresult.aspx?SearchWord={isbn}"
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(search_url, headers=headers, timeout=15)
        if res.status_code != 200:
            return "=300  \\$a1ì±….", f"ê²€ìƒ‰ ì‹¤íŒ¨ (status {res.status_code})"

        soup = BeautifulSoup(res.text, "html.parser")
        link_tag = soup.select_one("div.ss_book_box a.bo3")
        if not link_tag or not link_tag.get("href"):
            return "=300  \\$a1ì±….", "ë„ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        detail_url = link_tag["href"]
        detail_res = requests.get(detail_url, headers=headers, timeout=15)
        if detail_res.status_code != 200:
            return "=300  \\$a1ì±….", f"ìƒì„¸í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (status {detail_res.status_code})"

        detail_soup = BeautifulSoup(detail_res.text, "html.parser")
        form_wrap = detail_soup.select_one("div.conts_info_list1")
        a_part = ""
        c_part = ""

        if form_wrap:
            items = [s.strip() for s in form_wrap.stripped_strings]
            for item in items:
                # ìª½ìˆ˜ (~ìª½, ~p)
                if re.search(r"(ìª½|p)\s*$", item):
                    m = re.search(r"(\d+)\s*(ìª½|p)?$", item)
                    if m:
                        a_part = f"{m.group(1)} p."
                # í¬ê¸° (mm í¬í•¨, ex. 148*210mm)
                elif "mm" in item:
                    size_match = re.search(r"(\d+)\s*[\*xÃ—X]\s*(\d+)\s*mm", item)
                    if size_match:
                        width = int(size_match.group(1))
                        height = int(size_match.group(2))
                        w_cm = round(width / 10)
                        h_cm = round(height / 10)
                        c_part = f"{w_cm}x{h_cm} cm"

        if a_part or c_part:
            field_300 = "=300  \\\\$a"
            if a_part:
                field_300 += a_part
            if c_part:
                if a_part:
                    field_300 += f" ;$c{c_part}."
                else:
                    field_300 += f"$c{c_part}."
        else:
            field_300 = "=300  \\$a1ì±…."

        return field_300, None

    except Exception as e:
        return "=300  \\$a1ì±….", f"í¬ë¡¤ë§ ì˜ˆì™¸: {e}"


# --- KPIPAì—ì„œ ISBNìœ¼ë¡œ ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸ í¬ë¡¤ë§ (ì›ë¬¸ + ì •ê·œí™”) ---
def get_publisher_name_from_isbn_kpipa(isbn):
    search_url = "https://bnk.kpipa.or.kr/home/v3/addition/search"
    params = {"ST": isbn, "PG": 1, "PG2": 1, "DSF": "Y", "SO": "weight", "DT": "A"}
    headers = {"User-Agent": "Mozilla/5.0"}

    def normalize(name):
        return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬|í”„ë ˆìŠ¤", "", name).lower()

    try:
        res = requests.get(search_url, params=params, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        first_result_link = soup.select_one("a.book-grid-item")
        if not first_result_link:
            return None, None, "âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (KPIPA)"

        detail_href = first_result_link.get("href")
        detail_url = f"https://bnk.kpipa.or.kr{detail_href}"
        detail_res = requests.get(detail_url, headers=headers, timeout=15)
        detail_res.raise_for_status()
        detail_soup = BeautifulSoup(detail_res.text, "html.parser")

        pub_info_tag = detail_soup.find("dt", string="ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸")
        if not pub_info_tag:
            return None, None, "âŒ 'ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸' í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"

        dd_tag = pub_info_tag.find_next_sibling("dd")
        if dd_tag:
            full_text = dd_tag.get_text(strip=True)
            publisher_name_full = full_text
            publisher_name_part = publisher_name_full.split("/")[0].strip()
            publisher_name_norm = normalize(publisher_name_part)
            return publisher_name_full, publisher_name_norm, None

        return None, None, "âŒ 'dd' íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"

    except Exception as e:
        return None, None, f"KPIPA ì˜ˆì™¸: {e}"


# =========================
# --- Streamlit UI ë¶€ë¶„ ---
# =========================
st.title("ğŸ“š ISBN â†’ API + í¬ë¡¤ë§ â†’ KORMARC ë³€í™˜ê¸°")

isbn_input = st.text_area("ISBNì„ '/'ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”:")

if isbn_input:
    isbn_list = [re.sub(r"[^\d]", "", s) for s in isbn_input.split("/") if s.strip()]

    # êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° í•œë²ˆë§Œ ë¡œë“œ (ìºì‹œ)
    publisher_data, region_data = load_publisher_db()

    for idx, isbn in enumerate(isbn_list, start=1):
        st.markdown(f"---\n### ğŸ“˜ {idx}. ISBN: `{isbn}`")
        debug_messages = []

        # 1) Aladin APIë¡œ ë„ì„œ ì •ë³´ ì¡°íšŒ
        with st.spinner("ğŸ” ë„ì„œ ì •ë³´ ê²€ìƒ‰ ì¤‘..."):
            result, error = search_aladin_by_isbn(isbn)
        if error:
            debug_messages.append(f"âŒ Aladin API ì˜¤ë¥˜: {error}")

        # 2) í˜•íƒœì‚¬í•­(300) í¬ë¡¤ë§
        with st.spinner("ğŸ“ í˜•íƒœì‚¬í•­ í¬ë¡¤ë§ ì¤‘..."):
            field_300, err_300 = extract_physical_description_by_crawling(isbn)
        if err_300:
            debug_messages.append(f"âš ï¸ í˜•íƒœì‚¬í•­ í¬ë¡¤ë§ ê²½ê³ : {err_300}")

        if result:
            publisher = result["publisher"]
            pubyear = result["pubyear"]

            # 3) ì¶œíŒì‚¬ëª… ê´„í˜¸/ìŠ¬ë˜ì‹œ ë¶„ë¦¬ í›„ ë‘ ë²ˆ ê²€ìƒ‰ ì ìš©í•˜ì—¬ ì¶œíŒì§€ ì¡°íšŒ
            location_raw = search_publisher_location_with_alias(publisher, publisher_data)
            location_norm_for_display = normalize_publisher_location_for_display(location_raw)

            # 4) ì¶”ê°€ í¬ë¡¤ë§: **ì¶œíŒì§€ ë¯¸ìƒì¸ ê²½ìš°ì—ë§Œ** KPIPAì—ì„œ ì¶œíŒì‚¬ëª… í¬ë¡¤ë§ ì‹œë„
            if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
                debug_messages.append("ğŸ”” ì¶œíŒì§€ ë¯¸ìƒ â€” KPIPA ì¶”ê°€ ê²€ìƒ‰ ì‹¤í–‰")
                pub_full, pub_norm, crawl_err = get_publisher_name_from_isbn_kpipa(isbn)
                if crawl_err:
                    debug_messages.append(f"âŒ KPIPA í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl_err}")
                else:
                    debug_messages.append(f"ğŸ” KPIPA í¬ë¡¤ë§ ì›ë¬¸('ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸'): {pub_full}")
                    debug_messages.append(f"ğŸ§ª KPIPAì—ì„œ ì¶”ì¶œí•œ ì •ê·œí™”ëœ ì¶œíŒì‚¬ëª…: {pub_norm}")

                    # KPIPAì—ì„œ ì •ê·œí™”í•œ ì¶œíŒì‚¬ëª…ìœ¼ë¡œ ì¬ê²€ìƒ‰ (publisher_data ì‚¬ìš©)
                    new_location = get_publisher_location(pub_norm, publisher_data)
                    new_location_norm_display = normalize_publisher_location_for_display(new_location)
                    debug_messages.append(f"ğŸ™ï¸ KPIPA ê¸°ë°˜ ì¬ê²€ìƒ‰ ê²°ê³¼: {new_location} / ì •ê·œí™”: {new_location_norm_display}")

                    if new_location and new_location not in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
                        location_raw = new_location
                        location_norm_for_display = new_location_norm_display

            # 5) ë°œí–‰êµ­ ë¶€í˜¸ ì¡°íšŒ (region_data ì‚¬ìš©)
            country_code = get_country_code_by_region(location_raw, region_data)

            # â–¶ ì¶œë ¥: 008, 245, 260, 300
            with st.container():
                st.code(f"=008  \\$a{country_code}", language="text")
                st.code(result["245"], language="text")
                st.code(f"=260  \\$a{location_norm_for_display} :$b{publisher},$c{pubyear}.", language="text")
                st.code(field_300, language="text")

        else:
            debug_messages.append("âš ï¸ Aladinì—ì„œ ë„ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # â–¶ ë””ë²„ê¹… ë©”ì‹œì§€ ì¶œë ¥
        if debug_messages:
            with st.expander("ğŸ› ï¸ ë””ë²„ê¹… ë° ê²½ê³  ë©”ì‹œì§€"):
                for m in debug_messages:
                    st.write(m)
