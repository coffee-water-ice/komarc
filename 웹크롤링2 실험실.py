import streamlit as st
import requests
import re
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import io
from pymarc import Record, Field, MARCWriter, Subfield   # âœ… Subfield ì¶”ê°€


# =========================
# --- ì•Œë¼ë”˜ ìƒì„¸ í˜ì´ì§€ íŒŒì‹± (í˜•íƒœì‚¬í•­) ---
# =========================
def detect_illustrations(text: str):
    if not text:
        return False, None

    keyword_groups = {
        "ì²œì—°ìƒ‰ì‚½í™”": ["ì‚½í™”", "ì¼ëŸ¬ìŠ¤íŠ¸", "ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜", "illustration", "ê·¸ë¦¼"],
        "ì‚½í™”": ["í‘ë°± ì‚½í™”", "í‘ë°± ì¼ëŸ¬ìŠ¤íŠ¸", "í‘ë°± ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜", "í‘ë°± ê·¸ë¦¼"],
        "ì‚¬ì§„": ["ì‚¬ì§„", "í¬í† ", "photo", "í™”ë³´"],
        "ë„í‘œ": ["ë„í‘œ", "ì°¨íŠ¸", "ê·¸ë˜í”„"],
        "ì§€ë„": ["ì§€ë„", "ì§€ë„ì±…"],
    }

    found_labels = set()

    for label, keywords in keyword_groups.items():
        if any(kw in text for kw in keywords):
            found_labels.add(label)

    if found_labels:
        return True, ", ".join(sorted(found_labels))
    else:
        return False, None

def parse_aladin_physical_book_info(html):
    """
    ì•Œë¼ë”˜ ìƒì„¸ í˜ì´ì§€ HTMLì—ì„œ 300 í•„ë“œ íŒŒì‹±
    """
    soup = BeautifulSoup(html, "html.parser")

    # -------------------------------
    # ì œëª©, ë¶€ì œ, ì±…ì†Œê°œ
    # -------------------------------
    title = soup.select_one("span.Ere_bo_title")
    subtitle = soup.select_one("span.Ere_sub1_title")
    title_text = title.get_text(strip=True) if title else ""
    subtitle_text = subtitle.get_text(strip=True) if subtitle else ""

    description = None
    desc_tag = soup.select_one("div.Ere_prod_mconts_R")
    if desc_tag:
        description = desc_tag.get_text(" ", strip=True)

    # -------------------------------
    # í˜•íƒœì‚¬í•­
    # -------------------------------
    form_wrap = soup.select_one("div.conts_info_list1")
    a_part = ""
    b_part = ""
    c_part = ""
    page_value = None
    size_value = None

    if form_wrap:
        form_items = [item.strip() for item in form_wrap.stripped_strings if item.strip()]
        for item in form_items:
            if re.search(r"(ìª½|p)\s*$", item):
                page_match = re.search(r"\d+", item)
                if page_match:
                    page_value = int(page_match.group())
                    a_part = f"{page_match.group()} p."
            elif "mm" in item:
                size_match = re.search(r"(\d+)\s*[\*xÃ—X]\s*(\d+)", item)
                if size_match:
                    width = int(size_match.group(1))
                    height = int(size_match.group(2))
                    size_value = f"{width}x{height}mm"
                    if width == height or width > height or width < height / 2:
                        w_cm = round(width / 10)
                        h_cm = round(height / 10)
                        c_part = f"{w_cm}x{h_cm} cm"
                    else:
                        h_cm = round(height / 10)
                        c_part = f"{h_cm} cm"

    # -------------------------------
    # ì‚½í™” ê°ì§€ (ì œëª© + ë¶€ì œ + ì±…ì†Œê°œ ì „ì²´)
    # -------------------------------
    combined_text = " ".join(filter(None, [title_text, subtitle_text, description]))
    has_illus, illus_label = detect_illustrations(combined_text)
    if has_illus:
        b_part = f" :$b{illus_label}"

    # -------------------------------
    # 300 í•„ë“œ ì¡°í•©
    # -------------------------------
    if a_part or b_part or c_part:
        field_300 = "=300  \\$a"
        if a_part:
            field_300 += a_part
        if b_part:
            field_300 += b_part
        if c_part:
            field_300 += f" ;$c{c_part}."
        else:
            field_300 += "."
    else:
        field_300 = "=300  \\$a1ì±…."

    return {
        "300": field_300,
        "page_value": page_value,
        "size_value": size_value,
        "illustration_possibility": illus_label if illus_label else "ì—†ìŒ"
    }


def search_aladin_detail_page(link):
    try:
        res = requests.get(link, timeout=15)
        res.raise_for_status()
        return parse_aladin_physical_book_info(res.text), None
    except Exception as e:
        return {
            "300": "=300  \\$a1ì±…. [ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜]",
            "page_value": None,
            "size_value": None,
            "illustration_possibility": "ì •ë³´ ì—†ìŒ"
        }, f"Aladin ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì˜ˆì™¸: {e}"


# =========================
# --- êµ¬ê¸€ì‹œíŠ¸ ë¡œë“œ & ìºì‹œ ê´€ë¦¬ ---
# =========================
@st.cache_data(ttl=3600)
def load_publisher_db():
    creds = ServiceAccountCredentials.from_json_keyfile_dict(st.secrets["gspread"], 
                                                             ["https://spreadsheets.google.com/feeds",
                                                              "https://www.googleapis.com/auth/drive"])
    client = gspread.authorize(creds)
    sh = client.open("ì¶œíŒì‚¬ DB")
    
    # KPIPA_PUB_REG: ë²ˆí˜¸, ì¶œíŒì‚¬ëª…, ì£¼ì†Œ, ì „í™”ë²ˆí˜¸ â†’ ì¶œíŒì‚¬ëª…, ì£¼ì†Œë§Œ ì‚¬ìš©
    pub_rows = sh.worksheet("KPIPA_PUB_REG").get_all_values()[1:]
    pub_rows_filtered = [row[1:3] for row in pub_rows]  # ì¶œíŒì‚¬ëª…, ì£¼ì†Œ
    publisher_data = pd.DataFrame(pub_rows_filtered, columns=["ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ"])
    
    # 008: ë°œí–‰êµ­ ë°œí–‰êµ­ ë¶€í˜¸ â†’ ì²« 2ì—´ë§Œ
    region_rows = sh.worksheet("008").get_all_values()[1:]
    region_rows_filtered = [row[:2] for row in region_rows]
    region_data = pd.DataFrame(region_rows_filtered, columns=["ë°œí–‰êµ­", "ë°œí–‰êµ­ ë¶€í˜¸"])
    
    # IM_* ì‹œíŠ¸: ì¶œíŒì‚¬/ì„í”„ë¦°íŠ¸ í•˜ë‚˜ì˜ ì¹¼ëŸ¼
    imprint_frames = []
    for ws in sh.worksheets():
        if ws.title.startswith("IM_"):
            data = ws.get_all_values()[1:]
            imprint_frames.extend([row[0] for row in data if row])
    imprint_data = pd.DataFrame(imprint_frames, columns=["ì„í”„ë¦°íŠ¸"])
    
    return publisher_data, region_data, imprint_data

# =========================
# --- ì•Œë¼ë”˜ API ---
# =========================
def search_aladin_by_isbn(isbn):
    try:
        ttbkey = st.secrets["aladin"]["ttbkey"]
        url = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        params = {"ttbkey": ttbkey, "itemIdType": "ISBN", "ItemId": isbn, 
                  "output": "js", "Version": "20131101"}
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        data = res.json()
        if "item" not in data or not data["item"]:
            return None, None, f"ë„ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. [ì‘ë‹µ: {data}]"
        book = data["item"][0]
        title = book.get("title", "ì œëª© ì—†ìŒ")
        author = book.get("author", "")
        publisher = book.get("publisher", "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ")
        pubdate = book.get("pubDate", "")
        pubyear = pubdate[:4] if len(pubdate) >= 4 else "ë°œí–‰ë…„ë„ ì—†ìŒ"
        authors = [a.strip() for a in author.split(",")] if author else []
        creator_str = " ; ".join(authors) if authors else "ì €ì ì •ë³´ ì—†ìŒ"
        field_245 = f"=245  10$a{title} /$c{creator_str}"
        link = book.get("link")  # ìƒì„¸ í˜ì´ì§€ ë§í¬ ì¶”ì¶œ
        
        return {"title": title, "creator": creator_str, "publisher": publisher, "pubyear": pubyear, "245": field_245}, link, None
    except Exception as e:
        return None, None, f"Aladin API ì˜ˆì™¸: {e}"

# =========================
# --- ì •ê·œí™” í•¨ìˆ˜ ---
# =========================
def normalize_publisher_name(name):
    return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬", "", name).lower()

def normalize_stage2(name):
    name = re.sub(r"(ì£¼ë‹ˆì–´|JUNIOR|ì–´ë¦°ì´|í‚¤ì¦ˆ|ë¶ìŠ¤|ì•„ì´ì„¸ì›€|í”„ë ˆìŠ¤)", "", name, flags=re.IGNORECASE)
    eng_to_kor = {"springer": "ìŠ¤í”„ë§ê±°", "cambridge": "ì¼€ì„ë¸Œë¦¬ì§€", "oxford": "ì˜¥ìŠ¤í¬ë“œ"}
    for eng, kor in eng_to_kor.items():
        name = re.sub(eng, kor, name, flags=re.IGNORECASE)
    return name.strip().lower()

def split_publisher_aliases(name):
    aliases = []
    bracket_contents = re.findall(r"\((.*?)\)", name)
    for content in bracket_contents:
        parts = re.split(r"[,/]", content)
        parts = [p.strip() for p in parts if p.strip()]
        aliases.extend(parts)
    name_no_brackets = re.sub(r"\(.*?\)", "", name).strip()
    if "/" in name_no_brackets:
        parts = [p.strip() for p in name_no_brackets.split("/") if p.strip()]
        rep_name = parts[0]
        aliases.extend(parts[1:])
    else:
        rep_name = name_no_brackets
    return rep_name, aliases

def normalize_publisher_location_for_display(location_name):
    if not location_name or location_name in ("ì¶œíŒì§€ ë¯¸ìƒ", "[ì˜ˆì™¸] ë°œí–‰ì§€ë¯¸ìƒ"):
        return location_name
    location_name = location_name.strip()
    major_cities = ["ì„œìš¸", "ì¸ì²œ", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ëŒ€êµ¬", "ë¶€ì‚°", "ì„¸ì¢…"]
    for city in major_cities:
        if city in location_name:
            return location_name[:2]
    parts = location_name.split()
    loc = parts[1] if len(parts) > 1 else parts[0]
    if loc.endswith("ì‹œ"):
        loc = loc[:-1]
    return loc

# =========================
# --- KPIPA DB ê²€ìƒ‰ ë³´ì¡° í•¨ìˆ˜ ---
# =========================
def search_publisher_location_with_alias(name, publisher_data):
    debug_msgs = []
    if not name:
        return "ì¶œíŒì§€ ë¯¸ìƒ", ["âŒ ê²€ìƒ‰ ì‹¤íŒ¨: ì…ë ¥ëœ ì¶œíŒì‚¬ëª…ì´ ì—†ìŒ"]
    norm_name = normalize_publisher_name(name)
    candidates = publisher_data[publisher_data["ì¶œíŒì‚¬ëª…"].apply(lambda x: normalize_publisher_name(x)) == norm_name]
    if not candidates.empty:
        address = candidates.iloc[0]["ì£¼ì†Œ"]
        debug_msgs.append(f"âœ… KPIPA DB ë§¤ì¹­ ì„±ê³µ: {name} â†’ {address}")
        return address, debug_msgs
    else:
        debug_msgs.append(f"âŒ KPIPA DB ë§¤ì¹­ ì‹¤íŒ¨: {name}")
    return "ì¶œíŒì§€ ë¯¸ìƒ", debug_msgs

# =========================
# --- IM ì„í”„ë¦°íŠ¸ ë³´ì¡° í•¨ìˆ˜ ---
# =========================
def find_main_publisher_from_imprints(rep_name, imprint_data, publisher_data):
    """
    IM_* ì‹œíŠ¸ì—ì„œ ì„í”„ë¦°íŠ¸ëª…ì„ ê²€ìƒ‰í•˜ê³ , KPIPA DBì—ì„œ í•´ë‹¹ ì¶œíŒì‚¬ëª…ìœ¼ë¡œ ì£¼ì†Œë¥¼ ë°˜í™˜
    """
    norm_rep = normalize_publisher_name(rep_name)
    for full_text in imprint_data["ì„í”„ë¦°íŠ¸"]:
        if "/" in full_text:
            pub_part, imprint_part = [p.strip() for p in full_text.split("/", 1)]
        else:
            pub_part, imprint_part = full_text.strip(), None

        if imprint_part:
            norm_imprint = normalize_publisher_name(imprint_part)
            if norm_imprint == norm_rep:
                # KPIPA DBì—ì„œ pub_partë¥¼ ê²€ìƒ‰
                location, debug_msgs = search_publisher_location_with_alias(pub_part, publisher_data)
                return location, debug_msgs
    return None, [f"âŒ IM DB ê²€ìƒ‰ ì‹¤íŒ¨: ë§¤ì¹­ë˜ëŠ” ì„í”„ë¦°íŠ¸ ì—†ìŒ ({rep_name})"]

    
# =========================
# --- KPIPA í˜ì´ì§€ ê²€ìƒ‰ ---
# =========================
def get_publisher_name_from_isbn_kpipa(isbn):
    search_url = "https://bnk.kpipa.or.kr/home/v3/addition/search"
    params = {"ST": isbn, "PG": 1, "PG2": 1, "DSF": "Y", "SO": "weight", "DT": "A"}
    headers = {"User-Agent": "Mozilla/5.0"}
    def normalize(name):
        return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬|í”„ë ˆìŠ¤", "", name).lower()
    try:
        res = requests.get(search_url, params=params, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        first_result_link = soup.select_one("a.book-grid-item")
        if not first_result_link:
            return None, None, "âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (KPIPA)"
        detail_href = first_result_link.get("href")
        detail_url = f"https://bnk.kpipa.or.kr{detail_href}"
        detail_res = requests.get(detail_url, headers=headers, timeout=15)
        detail_res.raise_for_status()
        detail_soup = BeautifulSoup(detail_res.text, "html.parser")
        pub_info_tag = detail_soup.find("dt", string="ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸")
        if not pub_info_tag:
            return None, None, "âŒ 'ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸' í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"
        dd_tag = pub_info_tag.find_next_sibling("dd")
        if dd_tag:
            full_text = dd_tag.get_text(strip=True)
            publisher_name_full = full_text
            publisher_name_part = publisher_name_full.split("/")[0].strip()
            publisher_name_norm = normalize(publisher_name_part)
            return publisher_name_full, publisher_name_norm, None
        return None, None, "âŒ 'dd' íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"
    except Exception as e:
        return None, None, f"KPIPA ì˜ˆì™¸: {e}"

# =========================
# ----ë°œí–‰êµ­ ë¶€í˜¸ ì°¾ê¸°-----
# =========================

def get_country_code_by_region(region_name, region_data):
    """
    ì§€ì—­ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ 008 ë°œí–‰êµ­ ë¶€í˜¸ë¥¼ ì°¾ìŒ.
    region_data: DataFrame, columns=["ë°œí–‰êµ­", "ë°œí–‰êµ­ ë¶€í˜¸"]
    """
    try:
        def normalize_region_for_code(region):
            region = (region or "").strip()
            if region.startswith(("ì „ë¼", "ì¶©ì²­", "ê²½ìƒ")):
                return region[0] + (region[2] if len(region) > 2 else "")
            return region[:2]
        normalized_input = normalize_region_for_code(region_name)
        for idx, row in region_data.iterrows():
            sheet_region, country_code = row["ë°œí–‰êµ­"], row["ë°œí–‰êµ­ ë¶€í˜¸"]
            if normalize_region_for_code(sheet_region) == normalized_input:
                return country_code.strip() or "xxu"

        return "xxu"
    except Exception as e:
        st.write(f"âš ï¸ get_country_code_by_region ì˜ˆì™¸: {e}")
        return "xxu"

# =========================
# --- ë¬¸ì²´ë¶€ ê²€ìƒ‰ ---
# =========================
def get_mcst_address(publisher_name):
    url = "https://book.mcst.go.kr/html/searchList.php"
    params = {"search_area": "ì „ì²´", "search_state": "1", "search_kind": "1", 
              "search_type": "1", "search_word": publisher_name}
    debug_msgs = []
    try:
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        results = []
        for row in soup.select("table.board tbody tr"):
            cols = row.find_all("td")
            if len(cols) >= 4:
                reg_type = cols[0].get_text(strip=True)
                name = cols[1].get_text(strip=True)
                address = cols[2].get_text(strip=True)
                status = cols[3].get_text(strip=True)
                if status == "ì˜ì—…":
                    results.append((reg_type, name, address, status))
        if results:
            debug_msgs.append(f"[ë¬¸ì²´ë¶€] ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê±´")
            return results[0][2], results, debug_msgs
        else:
            debug_msgs.append("[ë¬¸ì²´ë¶€] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return "[ë¬¸ì²´ë¶€] [ë°œí–‰ì§€ë¯¸ìƒ]", [], debug_msgs
    except Exception as e:
        debug_msgs.append(f"[ë¬¸ì²´ë¶€] ì˜ˆì™¸ ë°œìƒ: {e}")
        return "ë°œìƒ [ì˜¤ë¥˜]", [], debug_msgs
        
# =========================
# --- MRC ë³€í™˜ í•¨ìˆ˜ ì¶”ê°€ ---
# =========================
def export_to_mrc(records):
    output = io.BytesIO()
    writer = MARCWriter(output)
    for rec in records:
        record = Record(force_utf8=True)
        # 008 (ë°œí–‰êµ­ ë¶€í˜¸ë§Œ ì˜ˆì‹œë¡œ ê¸°ë¡)
        record.add_field(Field(tag="008", data=rec["ë°œí–‰êµ­ ë¶€í˜¸"]))
        # 245
        record.add_field(Field(
            tag="245", indicators=["1", "0"],
            subfields=[Subfield("a", rec["ì œëª©"]), Subfield("c", rec["ì €ì"])]   
        ))
        # 260
        record.add_field(Field(
            tag="260", indicators=[" ", " "],
            subfields=[Subfield("a", rec["ì¶œíŒì§€"]), Subfield("b", rec["ì¶œíŒì‚¬"]), Subfield("c", rec["ë°œí–‰ë…„ë„"])]
        ))
        # 300
        field_300 = rec["MARC 300"].replace("=300  ", "").strip()
        record.add_field(Field(tag="300", indicators=[" ", " "], subfields=[Subfield("a", field_300)]))
        writer.write(record)

    output.seek(0)
    return output
        
# =========================
# --- Streamlit UI ---
# =========================
st.title("ğŸ“š ISBN â†’ KORMARC ë³€í™˜ê¸°")

if st.button("ğŸ”„ êµ¬ê¸€ì‹œíŠ¸ ìƒˆë¡œê³ ì¹¨"):
    st.cache_data.clear()
    st.success("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! ë‹¤ìŒ í˜¸ì¶œ ì‹œ ìµœì‹  ë°ì´í„° ë°˜ì˜ë©ë‹ˆë‹¤.")

isbn_input = st.text_area("ISBNì„ '/'ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥:")

records = []
all_mcst_results = []

if isbn_input:
    isbn_list = [re.sub(r"[^\d]", "", s) for s in isbn_input.split("/") if s.strip()]
    publisher_data, region_data, imprint_data = load_publisher_db()

    for idx, isbn in enumerate(isbn_list, start=1):
        st.markdown(f"---\n### ğŸ“˜ {idx}. ISBN: `{isbn}`")
        debug_messages = []

        # 1) Aladin API (ê¸°ë³¸ ì •ë³´ + ìƒì„¸ í˜ì´ì§€ ë§í¬)
        result, link, error = search_aladin_by_isbn(isbn)
        if error:
            st.warning(f"[Aladin API] {error}")
            continue
        publisher_api = result["publisher"]
        pubyear = result["pubyear"]
        
        # 1-1) Aladin ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (300 í•„ë“œ)
        physical_data, detail_error = search_aladin_detail_page(link)
        field_300 = physical_data.get("300", "=300  \\$a1ì±…. [íŒŒì‹± ì‹¤íŒ¨]") 
       
        if detail_error:
            debug_messages.append(f"[Aladin ìƒì„¸] {detail_error}")
        else:
            page_val = physical_data.get('page_value', 'N/A')
            size_val = physical_data.get('size_value', 'N/A')
            illus_val = physical_data.get('illustration_possibility', 'ì—†ìŒ')
            debug_messages.append(
                f"âœ… Aladin ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì„±ê³µ "
                f"(í˜ì´ì§€: {page_val}, í¬ê¸°: {size_val}, ì‚½í™”ê°ì§€: {illus_val})"
            )

        # 2) KPIPA í˜ì´ì§€ ê²€ìƒ‰
        publisher_full, publisher_norm, kpipa_error = get_publisher_name_from_isbn_kpipa(isbn)
        location_raw = "ì¶œíŒì§€ ë¯¸ìƒ"
        if publisher_norm:
            debug_messages.append(f"âœ… KPIPA í˜ì´ì§€ ê²€ìƒ‰ ì„±ê³µ: {publisher_full}")
            location_raw, debug_kpipa_db = search_publisher_location_with_alias(publisher_norm, publisher_data)
            debug_messages.extend([f"[KPIPA DB] {msg}" for msg in debug_kpipa_db])
        else:
            debug_messages.append(f"[KPIPA í˜ì´ì§€] {kpipa_error}")
            publisher_norm = publisher_api

        # 3) 1ì°¨ ì •ê·œí™” í›„ KPIPA DB
        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            rep_name, aliases = split_publisher_aliases(publisher_norm)
            location_raw, debug_stage1 = search_publisher_location_with_alias(rep_name, publisher_data)
            debug_messages.extend([f"[1ì°¨ ì •ê·œí™” KPIPA DB] {msg}" for msg in debug_stage1])
            if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
                for alias in aliases:
                    location_raw, debug_alias = search_publisher_location_with_alias(alias, publisher_data)
                    if location_raw != "ì¶œíŒì§€ ë¯¸ìƒ":
                        debug_messages.append(f"âœ… ë³„ì¹­ '{alias}' ë§¤ì¹­ ì„±ê³µ! ({location_raw})")
                        break          

        # 4) IM ê²€ìƒ‰
        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            main_pub, debug_im = find_main_publisher_from_imprints(rep_name, imprint_data, publisher_data)
            if main_pub:
                location_raw = main_pub
            debug_messages.extend([f"[IM DB] {msg}" for msg in debug_im])

        # 5) 2ì°¨ ì •ê·œí™” KPIPA DB
        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            stage2_name = normalize_stage2(publisher_norm)
            location_raw, debug_stage2 = search_publisher_location_with_alias(stage2_name, publisher_data)
            debug_messages.extend([f"[2ì°¨ ì •ê·œí™” KPIPA DB] {msg}" for msg in debug_stage2])

            # âœ… 2ì°¨ ì •ê·œí™” í›„ IM DB ê²€ìƒ‰
            if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
                main_pub_stage2, debug_im_stage2 = find_main_publisher_from_imprints(stage2_name, imprint_data, publisher_data)
                if main_pub_stage2:
                    location_raw = main_pub_stage2
                debug_messages.extend([f"[IM DB 2ì°¨ ì •ê·œí™” í›„] {msg}" for msg in debug_im_stage2])


        # 6) ë¬¸ì²´ë¶€ ê²€ìƒ‰
        mcst_address, mcst_results, debug_mcst = get_mcst_address(publisher_norm)
        debug_messages.extend(debug_mcst)
        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            if mcst_results:
                location_raw = mcst_results[0][2]
                debug_messages.append(f"[ë¬¸ì²´ë¶€] ë§¤ì¹­ ì„±ê³µ: {mcst_results}")
            else:
                location_raw = mcst_address
                debug_messages.append(f"[ë¬¸ì²´ë¶€] ë§¤ì¹­ ì‹¤íŒ¨")

        # 7) ë°œí–‰êµ­ í‘œì‹œìš© ì •ê·œí™”
        location_display = normalize_publisher_location_for_display(location_raw)

        # 8) MARC 008 ë°œí–‰êµ­ ë°œí–‰êµ­ ë¶€í˜¸
        code = get_country_code_by_region(location_raw, region_data)

        # 9) ìµœì¢… ì¶œë ¥
        with st.container():
            marc_text = (
                f"=008  \\$a{code}\n"
                f"{result['245']}\n"
                f"=260  \\$a{location_display} :$b{publisher_api},$c{pubyear}\n"
                f"{field_300}"
            )
            st.code(marc_text, language="text")
        with st.expander("ğŸ”¹ Debug / í›„ë³´ ë©”ì‹œì§€"):
            for msg in debug_messages:
                st.write(msg)
        with st.expander("ğŸ”¹ ë¬¸ì²´ë¶€ ë“±ë¡ ì¶œíŒì‚¬ ê²°ê³¼ í™•ì¸"):
            if mcst_results:
                st.table(pd.DataFrame(mcst_results, columns=["ë“±ë¡êµ¬ë¶„", "ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ", "ìƒíƒœ"]))
            else:
                st.write("âŒ ë¬¸ì²´ë¶€ ê²°ê³¼ ì—†ìŒ")
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
        record = {
            "ISBN": isbn,
            "ì œëª©": result['title'],
            "ì €ì": result['creator'],
            "ì¶œíŒì‚¬": publisher_api,
            "ë°œí–‰ë…„ë„": pubyear,
            "ì¶œíŒì§€": location_raw,
            "ë°œí–‰êµ­ ë¶€í˜¸": code,
            "MARC 245": result['245'],
            "MARC 260": f"=260  \\$a{location_display} :$b{publisher_api},$c{pubyear}",
            "MARC 300": field_300
        }
        records.append(record)

    # ëª¨ë“  ISBN ì²˜ë¦¬ í›„ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ
    if records:
        df = pd.DataFrame(records)
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='MARC_Results')
        output.seek(0)
        
        st.markdown("---")
        st.subheader("ğŸ‰ ëª¨ë“  ISBN ì²˜ë¦¬ ì™„ë£Œ!")
        st.success("ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
        st.download_button(
            label="ğŸ“¥ ê²°ê³¼ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
            data=output,
            file_name="kormarc_results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        # âœ… MRC ë‹¤ìš´ë¡œë“œ
        mrc_data = export_to_mrc(records)
        st.download_button(
            label="ğŸ“¥ ê²°ê³¼ MRC íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
            data=mrc_data,
            file_name="kormarc_results.mrc",
            mime="application/marc"
        )




