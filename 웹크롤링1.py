import streamlit as st
import requests
import re
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd

# =========================
# --- ì •ê·œí™” í•¨ìˆ˜ ---
# =========================
def normalize_publisher_name(name):
    return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬", "", name).lower()

def normalize_stage2(name):
    name = re.sub(r"(ì£¼ë‹ˆì–´|JUNIOR|ì–´ë¦°ì´|í‚¤ì¦ˆ|ë¶ìŠ¤|ì•„ì´ì„¸ì›€|í”„ë ˆìŠ¤)", "", name, flags=re.IGNORECASE)
    eng_to_kor = {"springer": "ìŠ¤í”„ë§ê±°", "cambridge": "ì¼€ì„ë¸Œë¦¬ì§€", "oxford": "ì˜¥ìŠ¤í¬ë“œ"}
    for eng, kor in eng_to_kor.items():
        name = re.sub(eng, kor, name, flags=re.IGNORECASE)
    return name.strip().lower()

def split_publisher_aliases(name):
    aliases = []
    bracket_contents = re.findall(r"\((.*?)\)", name)
    for content in bracket_contents:
        parts = re.split(r"[,/]", content)
        parts = [p.strip() for p in parts if p.strip()]
        aliases.extend(parts)
    name_no_brackets = re.sub(r"\(.*?\)", "", name).strip()
    if "/" in name_no_brackets:
        parts = [p.strip() for p in name_no_brackets.split("/") if p.strip()]
        rep_name = parts[0]
        aliases.extend(parts[1:])
    else:
        rep_name = name_no_brackets
    return rep_name, aliases

def normalize_publisher_location_for_display(location_name):
    if not location_name or location_name in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
        return location_name
    location_name = location_name.strip()
    major_cities = ["ì„œìš¸", "ì¸ì²œ", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ëŒ€êµ¬", "ë¶€ì‚°", "ì„¸ì¢…"]
    for city in major_cities:
        if city in location_name:
            return location_name[:2]
    parts = location_name.split()
    loc = parts[1] if len(parts) > 1 else parts[0]
    if loc.endswith("ì‹œ"):
        loc = loc[:-1]
    return loc

# =========================
# --- êµ¬ê¸€ì‹œíŠ¸ ë¡œë“œ & ìºì‹œ ê´€ë¦¬ ---
# =========================
@st.cache_data(ttl=3600)
def load_publisher_db():
    json_key = dict(st.secrets["gspread"])
    json_key["private_key"] = json_key["private_key"].replace('\\n', '\n')
    scope = ["https://spreadsheets.google.com/feeds",
             "https://www.googleapis.com/auth/spreadsheets",
             "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
    client = gspread.authorize(creds)

    sh = client.open("ì¶œíŒì‚¬ DB")
    publisher_data = pd.DataFrame(sh.worksheet("KPIPA_PUB_REG").get_all_values()[1:], columns=["ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ", "ì „í™”ë²ˆí˜¸"])
    region_data = pd.DataFrame(sh.worksheet("008").get_all_values()[1:], columns=["ë°œí–‰êµ­", "ë°œí–‰êµ­ì½”ë“œ"])

    imprint_frames = []
    for ws in sh.worksheets():
        if ws.title.startswith("IM_"):
            df_im = pd.DataFrame(ws.get_all_values()[1:], columns=["ì¶œíŒì‚¬/ì„í”„ë¦°íŠ¸"])
            imprint_frames.append(df_im)
    imprint_data = pd.concat(imprint_frames, ignore_index=True) if imprint_frames else pd.DataFrame(columns=["ì¶œíŒì‚¬/ì„í”„ë¦°íŠ¸"])

    return publisher_data, region_data, imprint_data

# =========================
# --- Aladin API ---
# =========================
def search_aladin_by_isbn(isbn):
    try:
        ttbkey = st.secrets["aladin"]["ttbkey"]
        url = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        params = {"ttbkey": ttbkey, "itemIdType": "ISBN", "ItemId": isbn, "output": "js", "Version": "20131101"}
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        data = res.json()
        if "item" not in data or not data["item"]:
            return None, f"ë„ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. [ì‘ë‹µ: {data}]"
        book = data["item"][0]
        title = book.get("title", "ì œëª© ì—†ìŒ")
        author = book.get("author", "")
        publisher = book.get("publisher", "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ")
        pubdate = book.get("pubDate", "")
        pubyear = pubdate[:4] if len(pubdate) >= 4 else "ë°œí–‰ë…„ë„ ì—†ìŒ"
        authors = [a.strip() for a in author.split(",")] if author else []
        creator_str = " ; ".join(authors) if authors else "ì €ì ì •ë³´ ì—†ìŒ"
        field_245 = f"=245  10$a{title} /$c{creator_str}"
        return {"title": title, "creator": creator_str, "publisher": publisher, "pubyear": pubyear, "245": field_245}, None
    except Exception as e:
        return None, f"Aladin API ì˜ˆì™¸: {e}"

# =========================
# --- KPIPA ISBN ê²€ìƒ‰ ---
# =========================
def get_publisher_name_from_isbn_kpipa(isbn):
    search_url = "https://bnk.kpipa.or.kr/home/v3/addition/search"
    params = {"ST": isbn, "PG": 1, "PG2": 1, "DSF": "Y", "SO": "weight", "DT": "A"}
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        res = requests.get(search_url, params=params, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        first_result_link = soup.select_one("a.book-grid-item")
        if not first_result_link:
            return None, None, "âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (KPIPA)"

        detail_href = first_result_link.get("href")
        detail_url = f"https://bnk.kpipa.or.kr{detail_href}"
        detail_res = requests.get(detail_url, headers=headers, timeout=15)
        detail_res.raise_for_status()
        detail_soup = BeautifulSoup(detail_res.text, "html.parser")

        pub_info_tag = detail_soup.find("dt", string="ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸")
        if not pub_info_tag:
            return None, None, "âŒ 'ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸' í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"

        dd_tag = pub_info_tag.find_next_sibling("dd")
        if dd_tag:
            full_text = dd_tag.get_text(strip=True)
            publisher_name_full = full_text
            publisher_name_part = publisher_name_full.split("/")[0].strip()
            publisher_name_norm = normalize_publisher_name(publisher_name_part)
            return publisher_name_full, publisher_name_norm, None

        return None, None, "âŒ 'dd' íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"
    except Exception as e:
        return None, None, f"KPIPA ì˜ˆì™¸: {e}"

# =========================
# --- KPIPA DB ê²€ìƒ‰ ---
# =========================
def search_publisher_location_with_alias(publisher_norm, publisher_data):
    for idx, row in publisher_data.iterrows():
        db_norm = normalize_publisher_name(str(row["ì¶œíŒì‚¬ëª…"]))
        if publisher_norm == db_norm:
            return row["ì£¼ì†Œ"], f"ğŸ’™ KPIPA DB ê²€ìƒ‰ ì„±ê³µ: {row['ì¶œíŒì‚¬ëª…']}"
    return "ì¶œíŒì§€ ë¯¸ìƒ", f"âŒ KPIPA DB ê²€ìƒ‰ ì‹¤íŒ¨: {publisher_norm}"

# =========================
# --- IM DB ê²€ìƒ‰ ---
# =========================
def find_main_publisher_from_imprints(publisher_name, imprint_data):
    publisher_name_norm = normalize_publisher_name(publisher_name)
    for idx, row in imprint_data.iterrows():
        try:
            im_str = row["ì¶œíŒì‚¬/ì„í”„ë¦°íŠ¸"]
            imprint_part = im_str.split("/")[-1].strip().lower()
            if publisher_name_norm == imprint_part:
                main_pub = im_str.split("/")[0].strip()
                return main_pub, f"ğŸŸ  IM DB ê²€ìƒ‰ ì„±ê³µ: {main_pub}"
        except:
            continue
    return None, f"âŒ IM DB ê²€ìƒ‰ ì‹¤íŒ¨: {publisher_name_norm}"

# =========================
# --- ë¬¸ì²´ë¶€ ê²€ìƒ‰ ---
# =========================
def get_mcst_address(publisher_name):
    url = "https://book.mcst.go.kr/html/searchList.php"
    params = {"search_area": "ì „ì²´", "search_state": "1", "search_kind": "1", "search_type": "1", "search_word": publisher_name}
    try:
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        results = []
        for row in soup.select("table.board tbody tr"):
            cols = row.find_all("td")
            if len(cols) >= 4:
                reg_type = cols[0].get_text(strip=True)
                name = cols[1].get_text(strip=True)
                address = cols[2].get_text(strip=True)
                status = cols[3].get_text(strip=True)
                if status == "ì˜ì—…":
                    results.append((reg_type, name, address, status))
        if results:
            return results[0][2], results
        else:
            return "ë¯¸í™•ì¸", []
    except Exception as e:
        return f"ì˜¤ë¥˜: {e}", []

# =========================
# --- Streamlit UI ---
# =========================
st.title("ğŸ“š ISBN â†’ KORMARC ë³€í™˜ê¸° (KPIPAÂ·IMÂ·2ì°¨ì •ê·œí™”Â·ë¬¸ì²´ë¶€ í†µí•©)")

if st.button("ğŸ”„ êµ¬ê¸€ì‹œíŠ¸ ìƒˆë¡œê³ ì¹¨"):
    st.cache_data.clear()
    st.success("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! ë‹¤ìŒ í˜¸ì¶œ ì‹œ ìµœì‹  ë°ì´í„° ë°˜ì˜ë©ë‹ˆë‹¤.")

isbn_input = st.text_area("ISBNì„ '/'ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥:")

all_mcst_results = []

if isbn_input:
    isbn_list = [re.sub(r"[^\d]", "", s) for s in isbn_input.split("/") if s.strip()]
    publisher_data, region_data, imprint_data = load_publisher_db()

    for idx, isbn in enumerate(isbn_list, start=1):
        st.markdown(f"---\n### ğŸ“˜ {idx}. ISBN: `{isbn}`")
        debug_messages = []

        result_api, error_api = search_aladin_by_isbn(isbn)
        if error_api:
            st.warning(error_api)
            continue

        publisher_api = result_api["publisher"]
        pubyear = result_api["pubyear"]
        field_245 = result_api["245"]

        publisher_full, publisher_norm, kpipa_error = get_publisher_name_from_isbn_kpipa(isbn)
        if publisher_norm:
            debug_messages.append(f"<span style='color:green'>âœ… KPIPA í˜ì´ì§€ ê²€ìƒ‰ ì„±ê³µ: {publisher_full}</span>")
            location_raw, debug_kpipa_db = search_publisher_location_with_alias(publisher_norm, publisher_data)
            debug_messages.append(f"<span style='color:blue'>{debug_kpipa_db}</span>")
        else:
            debug_messages.append(f"<span style='color:red'>{kpipa_error}</span>")
            publisher_norm = publisher_api
            location_raw, debug_stage1 = search_publisher_location_with_alias(publisher_norm, publisher_data)
            debug_messages.append(f"<span style='color:blue'>{debug_stage1}</span>")

        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            main_pub, debug_im = find_main_publisher_from_imprints(publisher_norm, imprint_data)
            if main_pub:
                publisher_norm = main_pub
                location_raw, debug_kpipa_db2 = search_publisher_location_with_alias(publisher_norm, publisher_data)
                debug_messages.append(f"<span style='color:orange'>{debug_im}</span>")
                debug_messages.append(f"<span style='color:blue'>{debug_kpipa_db2}</span>")

        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            publisher_norm_stage2 = normalize_stage2(publisher_norm)
            matches = []
            for idx2, row in publisher_data.iterrows():
                db_norm = normalize_stage2(str(row["ì¶œíŒì‚¬ëª…"]))
                if publisher_norm_stage2 in db_norm or db_norm in publisher_norm_stage2:
                    matches.append((row["ì¶œíŒì‚¬ëª…"], row["ì£¼ì†Œ"]))
            if matches:
                publisher_norm, location_raw = matches[0]
                st.markdown("### ğŸ” 2ì°¨ ì •ê·œí™” í›„ë³´")
                df_stage2 = pd.DataFrame(matches, columns=["ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ"])
                st.dataframe(df_stage2, use_container_width=True)
                debug_messages.append(f"<span style='color:purple'>ğŸŸ£ 2ì°¨ ì •ê·œí™” í›„ë³´: {len(matches)}ê±´</span>")

        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            addr_mcst, mcst_results = get_mcst_address(publisher_norm)
            if mcst_results:
                all_mcst_results.extend(mcst_results)
                location_raw = addr_mcst
                debug_messages.append(f"<span style='color:red'>ğŸ”´ ë¬¸ì²´ë¶€ ê²€ìƒ‰ í›„ë³´: {len(mcst_results)}ê±´</span>")

        country_code_row = region_data[region_data["ë°œí–‰êµ­"].str.contains("í•œêµ­", na=False)]
        country_code = country_code_row["ë°œí–‰êµ­ì½”ë“œ"].values[0] if not country_code_row.empty else "--"

        field_008 = f"=008  \\\\$a{country_code}"
        field_260 = f"=260  \\\\$a{normalize_publisher_location_for_display(location_raw)} :$b{publisher_norm},$c{pubyear}."

        st.code(field_008, language="text")
        st.code(field_245, language="text")
        st.code(field_260, language="text")

        if debug_messages:
            st.markdown("### ğŸ› ï¸ ê²€ìƒ‰ ë‹¨ê³„ë³„ ê²°ê³¼")
            for msg in debug_messages:
                st.markdown(msg, unsafe_allow_html=True)

if all_mcst_results:
    st.markdown("### ğŸ“Œ ë¬¸ì²´ë¶€ ê²€ìƒ‰ í›„ë³´")
    df_mcst = pd.DataFrame(all_mcst_results, columns=["ë“±ë¡ìœ í˜•", "ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ", "ìƒíƒœ"])
    st.dataframe(df_mcst, use_container_width=True)
