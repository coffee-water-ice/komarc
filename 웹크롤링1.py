import streamlit as st
import requests
import re
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import io   # âœ… ì¶”ê°€

# =========================
# --- êµ¬ê¸€ì‹œíŠ¸ ë¡œë“œ & ìºì‹œ ê´€ë¦¬ ---
# =========================
@st.cache_data(ttl=3600)
def load_publisher_db():
    json_key = dict(st.secrets["gspread"])
    json_key["private_key"] = json_key["private_key"].replace('\\n', '\n')
    scope = ["https://spreadsheets.google.com/feeds",
             "https://www.googleapis.com/auth/spreadsheets",
             "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
    client = gspread.authorize(creds)
    publisher_sheet = client.open("ì¶œíŒì‚¬ DB").worksheet("ì‹œíŠ¸3")
    region_sheet = client.open("ì¶œíŒì‚¬ DB").worksheet("Sheet2")
    publisher_data = publisher_sheet.get_all_values()[1:]
    region_data = region_sheet.get_all_values()[1:]
    return publisher_data, region_data

# =========================
# --- ì •ê·œí™” í•¨ìˆ˜ ---
# =========================
def normalize_publisher_name(name):
    return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬", "", name).lower()

def normalize_stage2(name):
    name = re.sub(r"(ì£¼ë‹ˆì–´|JUNIOR|ì–´ë¦°ì´|í‚¤ì¦ˆ|ë¶ìŠ¤|ì•„ì´ì„¸ì›€|í”„ë ˆìŠ¤)", "", name, flags=re.IGNORECASE)
    eng_to_kor = {"springer": "ìŠ¤í”„ë§ê±°", "cambridge": "ì¼€ì„ë¸Œë¦¬ì§€", "oxford": "ì˜¥ìŠ¤í¬ë“œ"}
    for eng, kor in eng_to_kor.items():
        name = re.sub(eng, kor, name, flags=re.IGNORECASE)
    return name.strip().lower()

def split_publisher_aliases(name):
    aliases = []
    bracket_contents = re.findall(r"\((.*?)\)", name)
    for content in bracket_contents:
        parts = re.split(r"[,/]", content)
        parts = [p.strip() for p in parts if p.strip()]
        aliases.extend(parts)
    name_no_brackets = re.sub(r"\(.*?\)", "", name).strip()
    if "/" in name_no_brackets:
        parts = [p.strip() for p in name_no_brackets.split("/") if p.strip()]
        rep_name = parts[0]
        aliases.extend(parts[1:])
    else:
        rep_name = name_no_brackets
    return rep_name, aliases

def normalize_publisher_location_for_display(location_name):
    if not location_name or location_name in ("ì¶œíŒì§€ ë¯¸ìƒ", "ì˜ˆì™¸ ë°œìƒ"):
        return location_name
    location_name = location_name.strip()
    major_cities = ["ì„œìš¸", "ì¸ì²œ", "ëŒ€ì „", "ê´‘ì£¼", "ìš¸ì‚°", "ëŒ€êµ¬", "ë¶€ì‚°", "ì„¸ì¢…"]
    for city in major_cities:
        if city in location_name:
            return location_name[:2]
    parts = location_name.split()
    loc = parts[1] if len(parts) > 1 else parts[0]
    if loc.endswith("ì‹œ") or loc.endswith("êµ°"):
        loc = loc[:-1]
    return loc

# =========================
# --- êµ¬ê¸€ì‹œíŠ¸ ê²€ìƒ‰ ---
# =========================
def get_publisher_location(publisher_name, publisher_data):
    try:
        target = normalize_publisher_name(publisher_name)
        for row in publisher_data:
            if len(row) < 3:
                continue
            sheet_name, region = row[1], row[2]
            if normalize_publisher_name(sheet_name) == target:
                return region.strip() or "ì¶œíŒì§€ ë¯¸ìƒ"
        for row in publisher_data:  # fallback
            if len(row) < 3:
                continue
            sheet_name, region = row[1], row[2]
            if sheet_name.strip() == publisher_name.strip():
                return region.strip() or "ì¶œíŒì§€ ë¯¸ìƒ"
        return "ì¶œíŒì§€ ë¯¸ìƒ"
    except:
        return "ì˜ˆì™¸ ë°œìƒ"

def search_publisher_location_with_alias(publisher_name, publisher_data):
    rep_name, aliases = split_publisher_aliases(publisher_name)
    debug = []
    rep_name_norm = normalize_publisher_name(rep_name)
    debug.append(f"1ì°¨ ì •ê·œí™” ëŒ€í‘œëª…: `{rep_name_norm}`")

    location = get_publisher_location(rep_name_norm, publisher_data)
    if location != "ì¶œíŒì§€ ë¯¸ìƒ":
        return location, debug

    for alias in aliases:
        alias_norm = normalize_publisher_name(alias)
        debug.append(f"ë³„ì¹­ ê²€ìƒ‰: `{alias_norm}`")
        location = get_publisher_location(alias_norm, publisher_data)
        if location != "ì¶œíŒì§€ ë¯¸ìƒ":
            return location, debug
    return "ì¶œíŒì§€ ë¯¸ìƒ", debug

def search_publisher_location_stage2_contains(publisher_name, publisher_data):
    """2ì°¨ ì •ê·œí™”ëœ ê°’ í¬í•¨ê²€ìƒ‰"""
    rep_name, aliases = split_publisher_aliases(publisher_name)
    rep_name_norm = normalize_stage2(rep_name)

    matches = []
    for row in publisher_data:
        if len(row) < 3:
            continue
        sheet_name, region = row[1], row[2]
        sheet_norm = normalize_stage2(sheet_name)
        if rep_name_norm in sheet_norm:
            matches.append((sheet_name, region))

    debug = [f"ë¶€ë¶„ì¼ì¹˜ ê²€ìƒ‰ ëŒ€í‘œëª…: `{rep_name_norm}`, ê²°ê³¼ {len(matches)}ê±´"]
    return matches, debug

# =========================
# --- ì§€ì—­ ì½”ë“œ ë³€í™˜ ---
# =========================
def get_country_code_by_region(region_name, region_data):
    def normalize_region_for_code(region):
        region = (region or "").strip()
        if region.startswith(("ì „ë¼", "ì¶©ì²­", "ê²½ìƒ")):
            if len(region) >= 3:
                return region[0] + region[2]
        return region[:2]

    normalized_input = normalize_region_for_code(region_name)
    for row in region_data:
        if len(row) < 2:
            continue
        sheet_region, country_code = row[0], row[1]
        if normalize_region_for_code(sheet_region) == normalized_input:
            return country_code.strip() or "xxu"
    return "xxu"

# =========================
# --- Aladin API ---
# =========================
def search_aladin_by_isbn(isbn):
    try:
        ttbkey = st.secrets["aladin"]["ttbkey"]
        url = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        params = {"ttbkey": ttbkey, "itemIdType": "ISBN", "ItemId": isbn, "output": "js", "Version": "20131101"}
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        data = res.json()
        if "item" not in data or not data["item"]:
            return None, f"ë„ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. [ì‘ë‹µ: {data}]"
        book = data["item"][0]
        title = book.get("title", "ì œëª© ì—†ìŒ")
        author = book.get("author", "")
        publisher = book.get("publisher", "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ")
        pubdate = book.get("pubDate", "")
        pubyear = pubdate[:4] if len(pubdate) >= 4 else "ë°œí–‰ë…„ë„ ì—†ìŒ"
        authors = [a.strip() for a in author.split(",")] if author else []
        creator_str = " ; ".join(authors) if authors else "ì €ì ì •ë³´ ì—†ìŒ"
        field_245 = f"=245  10$a{title} /$c{creator_str}"
        return {"title": title, "creator": creator_str, "publisher": publisher, "pubyear": pubyear, "245": field_245}, None
    except Exception as e:
        return None, f"Aladin API ì˜ˆì™¸: {e}"

# =========================
# --- í˜•íƒœì‚¬í•­ í¬ë¡¤ë§ ---
# =========================
def extract_physical_description_by_crawling(isbn):
    try:
        search_url = f"https://www.aladin.co.kr/search/wsearchresult.aspx?SearchWord={isbn}"
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(search_url, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        link_tag = soup.select_one("div.ss_book_box a.bo3")
        if not link_tag or not link_tag.get("href"):
            return "=300  \\$a1ì±….", "ë„ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        detail_url = link_tag["href"]
        detail_res = requests.get(detail_url, headers=headers, timeout=15)
        detail_res.raise_for_status()
        detail_soup = BeautifulSoup(detail_res.text, "html.parser")
        form_wrap = detail_soup.select_one("div.conts_info_list1")
        a_part, c_part = "", ""
        if form_wrap:
            items = [s.strip() for s in form_wrap.stripped_strings]
            for item in items:
                if re.search(r"(ìª½|p)\s*$", item):
                    m = re.search(r"(\d+)\s*(ìª½|p)?$", item)
                    if m:
                        a_part = f"{m.group(1)} p."
                elif "mm" in item:
                    size_match = re.search(r"(\d+)\s*[\*xÃ—X]\s*(\d+)\s*mm", item)
                    if size_match:
                        width = int(size_match.group(1))
                        height = int(size_match.group(2))
                        w_cm = round(width / 10)
                        h_cm = round(height / 10)
                        c_part = f"{w_cm}x{h_cm} cm"
        if a_part or c_part:
            field_300 = "=300  \\\\$a"
            if a_part:
                field_300 += a_part
            if c_part:
                if a_part:
                    field_300 += f" ;$c{c_part}."
                else:
                    field_300 += f"$c{c_part}."
        else:
            field_300 = "=300  \\$a1ì±…."
        return field_300, None
    except Exception as e:
        return "=300  \\$a1ì±….", f"í¬ë¡¤ë§ ì˜ˆì™¸: {e}"

# =========================
# --- KPIPA ---
# =========================
def get_publisher_name_from_isbn_kpipa(isbn):
    search_url = "https://bnk.kpipa.or.kr/home/v3/addition/search"
    params = {"ST": isbn, "PG": 1, "PG2": 1, "DSF": "Y", "SO": "weight", "DT": "A"}
    headers = {"User-Agent": "Mozilla/5.0"}
    def normalize(name):
        return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬|í”„ë ˆìŠ¤", "", name).lower()
    try:
        res = requests.get(search_url, params=params, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        first_result_link = soup.select_one("a.book-grid-item")
        if not first_result_link:
            return None, None, "âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (KPIPA)"
        detail_href = first_result_link.get("href")
        detail_url = f"https://bnk.kpipa.or.kr{detail_href}"
        detail_res = requests.get(detail_url, headers=headers, timeout=15)
        detail_res.raise_for_status()
        detail_soup = BeautifulSoup(detail_res.text, "html.parser")
        pub_info_tag = detail_soup.find("dt", string="ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸")
        if not pub_info_tag:
            return None, None, "âŒ 'ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸' í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"
        dd_tag = pub_info_tag.find_next_sibling("dd")
        if dd_tag:
            full_text = dd_tag.get_text(strip=True)
            publisher_name_full = full_text
            publisher_name_part = publisher_name_full.split("/")[0].strip()
            publisher_name_norm = normalize(publisher_name_part)
            return publisher_name_full, publisher_name_norm, None
        return None, None, "âŒ 'dd' íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (KPIPA)"
    except Exception as e:
        return None, None, f"KPIPA ì˜ˆì™¸: {e}"

# =========================
# --- ë¬¸ì²´ë¶€ ---
# =========================
def get_mcst_address(publisher_name):
    url = "https://book.mcst.go.kr/html/searchList.php"
    params = {"search_area": "ì „ì²´", "search_state": "1", "search_kind": "1", "search_type": "1", "search_word": publisher_name}
    try:
        res = requests.get(url, params=params, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        results = []
        for row in soup.select("table.board tbody tr"):
            cols = row.find_all("td")
            if len(cols) >= 4:
                reg_type = cols[0].get_text(strip=True)
                name = cols[1].get_text(strip=True)
                address = cols[2].get_text(strip=True)
                status = cols[3].get_text(strip=True)
                if status == "ì˜ì—…":
                    results.append((reg_type, name, address, status))
        if results:
            return results[0][2], results
        else:
            return "ë¯¸í™•ì¸", []
    except Exception as e:
        return f"ì˜¤ë¥˜: {e}", []

# =========================
# --- Streamlit UI ---
# =========================
st.title("ğŸ“š ISBN â†’ KORMARC ë³€í™˜ê¸° 2025.9.8.ìˆ˜ì •.")

if st.button("ğŸ”„ êµ¬ê¸€ì‹œíŠ¸ ìƒˆë¡œê³ ì¹¨"):
    st.cache_data.clear()
    st.success("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! ë‹¤ìŒ í˜¸ì¶œ ì‹œ ìµœì‹  ë°ì´í„° ë°˜ì˜ë©ë‹ˆë‹¤.")

isbn_input = st.text_area("ISBNì„ '/'ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥:")

records = []

if isbn_input:
    isbn_list = [re.sub(r"[^\d]", "", s) for s in isbn_input.split("/") if s.strip()]
    publisher_data, region_data = load_publisher_db()

    for idx, isbn in enumerate(isbn_list, start=1):
        st.markdown(f"---\n### ğŸ“˜ {idx}. ISBN: `{isbn}`")
        debug_messages = []

        # 1) Aladin API
        result, error = search_aladin_by_isbn(isbn)
        if error:
            debug_messages.append(f"âŒ Aladin API ì˜¤ë¥˜: {error}")

        # 2) í˜•íƒœì‚¬í•­
        field_300, err_300 = extract_physical_description_by_crawling(isbn)
        if err_300:
            debug_messages.append(f"âš ï¸ í˜•íƒœì‚¬í•­ í¬ë¡¤ë§ ê²½ê³ : {err_300}")

        if result:
            publisher = result["publisher"]
            pubyear = result["pubyear"]

            # 3) 1ì°¨ ì •ê·œí™”
            location_raw, debug1 = search_publisher_location_with_alias(publisher, publisher_data)
            debug_messages.extend(debug1)
            location_display = normalize_publisher_location_for_display(location_raw)

            # 4) ë¶€ë¶„ì¼ì¹˜ ê²€ìƒ‰ (2ì°¨ ì •ê·œí™” í¬í•¨ê²€ìƒ‰)
            if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
                matches, debug2 = search_publisher_location_stage2_contains(publisher, publisher_data)
                debug_messages.extend(debug2)
                if matches:
                    # í‘œë¡œ ê²°ê³¼ í‘œì‹œ (1ê±´ì´ë“  ë‹¤ì¤‘ì´ë“  ëª¨ë‘)
                    df = pd.DataFrame(matches, columns=["ì¶œíŒì‚¬ëª…", "ì§€ì—­"])
                    st.markdown("### ğŸ” ë¶€ë¶„ì¼ì¹˜ ê²€ìƒ‰ ê²°ê³¼")
                    st.dataframe(df, use_container_width=True)

                    # ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ìë™ ì„ íƒ
                    location_raw = matches[0][1]
                    location_display = normalize_publisher_location_for_display(location_raw)
                    debug_messages.append(f"âœ… ë¶€ë¶„ì¼ì¹˜ ê²°ê³¼ ì‚¬ìš©: {location_raw}")

            # 5) KPIPA
            if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
                pub_full, pub_norm, kpipa_err = get_publisher_name_from_isbn_kpipa(isbn)
                if kpipa_err:
                    debug_messages.append(f"âŒ KPIPA ê²€ìƒ‰ ì‹¤íŒ¨: {kpipa_err}")
                else:
                    debug_messages.append(f"ğŸ” KPIPA ì›ë¬¸: {pub_full}")
                    debug_messages.append(f"ğŸ§ª KPIPA ì •ê·œí™”: {pub_norm}")
                    kpipa_location = get_publisher_location(pub_norm, publisher_data)
                    if kpipa_location != "ì¶œíŒì§€ ë¯¸ìƒ":
                        location_raw = kpipa_location
                        location_display = normalize_publisher_location_for_display(location_raw)
                        debug_messages.append(f"ğŸ™ï¸ KPIPA ê¸°ë°˜ ì¬ê²€ìƒ‰ ê²°ê³¼: {location_raw}")

            # 6) ë¬¸ì²´ë¶€
            mcst_results = []
            if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
                addr, mcst_results = get_mcst_address(publisher)
                debug_messages.append(f"ğŸ›ï¸ ë¬¸ì²´ë¶€ ì£¼ì†Œ ê²€ìƒ‰ ê²°ê³¼: {addr}")
                if addr != "ë¯¸í™•ì¸":
                    location_raw = addr
                    location_display = normalize_publisher_location_for_display(location_raw)

            # 7) ë°œí–‰êµ­ ë¶€í˜¸
            country_code = get_country_code_by_region(location_raw, region_data)

            field_008 = f"=008  \\\\$a{country_code}"
            field_245 = result["245"]
            field_260 = f"=260  \\\\$a{location_display} :$b{publisher},$c{pubyear}."

            st.code(field_008, language="text")
            st.code(field_245, language="text")
            st.code(field_260, language="text")
            st.code(field_300, language="text")

            # âœ… ê²°ê³¼ ì €ì¥ (result ìˆì„ ë•Œë§Œ)
            records.append({
                "ISBN": isbn,
                "008": field_008,
                "245": field_245,
                "260": field_260,
                "300": field_300
            })
        else:
            # âœ… API ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ê¸°ë¡
            records.append({
                "ISBN": isbn,
                "008": "ê°’ ì—†ìŒ",
                "245": "ê°’ ì—†ìŒ",
                "260": "ê°’ ì—†ìŒ",
                "300": field_300 if 'field_300' in locals() else "ê°’ ì—†ìŒ"
            })                

            # â–¶ ë””ë²„ê¹… ë©”ì‹œì§€
            with st.expander("ğŸ› ï¸ Debugging Messages", expanded=False):
                for msg in debug_messages:
                    st.markdown(msg)
                if len(mcst_results) > 1:
                    st.markdown("### ë¬¸ì²´ë¶€ ë‹¤ì¤‘ ê²°ê³¼")
                    df = pd.DataFrame(mcst_results, columns=["ë“±ë¡ êµ¬ë¶„", "ì¶œíŒì‚¬ëª…", "ì£¼ì†Œ", "ìƒíƒœ"])
                    st.dataframe(df, use_container_width=True)

# =========================
# --- ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ---
# =========================
if records:
    # ğŸ‘‰ ì—‘ì…€ ì €ì¥ìš©: =, \, $ ë“± ì œê±°
    def clean_marc_field(value: str) -> str:
        """MARC ë¬¸ìì—´ì—ì„œ =, \, $, ì§€ì‹œê¸°í˜¸ ì œê±° â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ"""
        if not isinstance(value, str):
            return value
        cleaned = (
            value.replace("=008", "")
            .replace("=245", "")
            .replace("=260", "")
            .replace("=300", "")
            .replace("10$a", "")
            .replace("\\", "")
            .replace("$a", "")
            .replace("$b", "")
            .replace("$c", "")
            .replace("$", "")
            .strip()
        )
        return cleaned

    # ğŸ‘‰ recordsë¥¼ ë³µì‚¬í•´ì„œ "ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë²„ì „" ìƒì„±
    cleaned_records = []
    for rec in records:
        cleaned_records.append({
            "ISBN": rec["ISBN"],
            "008": clean_marc_field(rec["008"]),
            "245": clean_marc_field(rec["245"]),
            "260": clean_marc_field(rec["260"]),
            "300": clean_marc_field(rec["300"]),
        })

    df_out = pd.DataFrame(cleaned_records)
    buffer = io.BytesIO()

    # âœ… xlsxwriter ì—”ì§„ ì‚¬ìš©
    with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
        df_out.to_excel(writer, index=False, sheet_name="KORMARC ê²°ê³¼")

    buffer.seek(0)

    st.download_button(
        label="ğŸ“¥ ë³€í™˜ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (ìˆœìˆ˜ í…ìŠ¤íŠ¸)",
        data=buffer.getvalue(),
        file_name="kormarc_results.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
