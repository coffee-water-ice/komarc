import streamlit as st
import requests
import re
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd

# =========================
# --- êµ¬ê¸€ ì‹œíŠ¸ ë¡œë“œ ---
# =========================
@st.cache_data(ttl=3600)
def load_publisher_db():
    json_key = dict(st.secrets["gspread"])
    json_key["private_key"] = json_key["private_key"].replace("\\n", "\n")
    scope = ["https://spreadsheets.google.com/feeds",
             "https://www.googleapis.com/auth/spreadsheets",
             "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
    client = gspread.authorize(creds)
    sh = client.open("ì¶œíŒì‚¬ DB")

    # ì¶œíŒì‚¬-ì§€ì—­ ë§¤ì¹­
    publisher_data = pd.DataFrame(sh.worksheet("KPIPA_PUB_REG").get_all_values()[1:], 
                                  columns=["ì¶œíŒì‚¬ëª…", "ì§€ì—­"])
    # ë°œí–‰êµ­ ì½”ë“œ
    region_data = pd.DataFrame(sh.worksheet("008").get_all_values()[1:], 
                               columns=["ì§€ì—­", "ë°œí–‰êµ­", "ë°œí–‰êµ­ì½”ë“œ"])

    # IM_* ì‹œíŠ¸ í•©ì¹˜ê¸°
    imprint_frames = []
    for ws in sh.worksheets():
        if ws.title.startswith("IM_"):
            df = pd.DataFrame(ws.get_all_values()[1:], columns=["ì¶œíŒì‚¬", "ì„í”„ë¦°íŠ¸"])
            imprint_frames.append(df)
    imprint_data = pd.concat(imprint_frames, ignore_index=True) if imprint_frames else pd.DataFrame(columns=["ì¶œíŒì‚¬","ì„í”„ë¦°íŠ¸"])

    return publisher_data, region_data, imprint_data


# =========================
# --- ë³´ì¡° í•¨ìˆ˜ ---
# =========================
def normalize(name: str) -> str:
    if not name:
        return ""
    return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬|í”„ë ˆìŠ¤", "", name).lower()


def search_publisher_location_with_alias(pub_name, publisher_df):
    """KPIPA DBì—ì„œ ì¶œíŒì‚¬ ì •í™• ê²€ìƒ‰"""
    debug = []
    matches = publisher_df[publisher_df["ì¶œíŒì‚¬ëª…"].apply(normalize) == pub_name]
    if not matches.empty:
        region = matches.iloc[0]["ì§€ì—­"]
        debug.append(f"âœ… DB ì •í™•ê²€ìƒ‰ ì„±ê³µ: {pub_name} â†’ {region}")
        return region, debug
    debug.append(f"âŒ DBì—ì„œ '{pub_name}' ë¯¸ë°œê²¬")
    return "ì¶œíŒì§€ ë¯¸ìƒ", debug


def search_publisher_location_stage2_contains(pub_name, publisher_df):
    """ë¶€ë¶„ì¼ì¹˜ ê²€ìƒ‰ (2ì°¨ ì •ê·œí™”)"""
    debug = []
    matches = publisher_df[publisher_df["ì¶œíŒì‚¬ëª…"].str.contains(pub_name, case=False, na=False)]
    if not matches.empty:
        debug.append(f"ğŸ” ë¶€ë¶„ì¼ì¹˜ ê²€ìƒ‰ í›„ë³´ {len(matches)}ê±´ ë°œê²¬")
        return matches[["ì¶œíŒì‚¬ëª…","ì§€ì—­"]].values.tolist(), debug
    debug.append(f"âŒ ë¶€ë¶„ì¼ì¹˜ì—ì„œë„ '{pub_name}' ë°œê²¬ ì‹¤íŒ¨")
    return [], debug


def find_main_publisher_from_imprints(imprint_name, imprint_df):
    """IM_* ì‹œíŠ¸ì—ì„œ ì„í”„ë¦°íŠ¸ ë’·ë¶€ë¶„ìœ¼ë¡œ ì¶œíŒì‚¬ ì°¾ê¸°"""
    matches = imprint_df[imprint_df["ì„í”„ë¦°íŠ¸"].apply(normalize) == normalize(imprint_name)]
    if not matches.empty:
        return matches.iloc[0]["ì¶œíŒì‚¬"]
    return None


def get_country_code_by_region(region, region_df):
    row = region_df[region_df["ì§€ì—­"] == region]
    if not row.empty:
        return row.iloc[0]["ë°œí–‰êµ­ì½”ë“œ"]
    return "xx "


# =========================
# --- ì™¸ë¶€ ê²€ìƒ‰ í•¨ìˆ˜ ---
# =========================
def search_aladin_by_isbn(isbn):
    """ì•Œë¼ë”˜ API (ì¶œíŒì‚¬, ë°œí–‰ì—°ë„ ë“± ê°€ì ¸ì˜¤ê¸°)"""
    # TODO: ì‹¤ì œ API ì—°ë™ (ì—¬ê¸°ì„  ë”ë¯¸)
    return {"publisher": "ë¯¼ìŒì‚¬", "pubyear": "2020", "245": "=245  10$aë”ë¯¸ì œëª©"}, None


def get_publisher_name_from_isbn_kpipa(isbn):
    """KPIPA í˜ì´ì§€ ISBN ê²€ìƒ‰"""
    search_url = "https://bnk.kpipa.or.kr/home/v3/addition/search"
    params = {"ST": isbn, "PG": 1, "PG2": 1, "DSF": "Y", "SO": "weight", "DT": "A"}
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        res = requests.get(search_url, params=params, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        first_result_link = soup.select_one("a.book-grid-item")
        if not first_result_link:
            return None, None, "âŒ KPIPA ISBN ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        detail_href = first_result_link.get("href")
        detail_url = f"https://bnk.kpipa.or.kr{detail_href}"
        detail_res = requests.get(detail_url, headers=headers, timeout=15)
        detail_res.raise_for_status()
        detail_soup = BeautifulSoup(detail_res.text, "html.parser")

        pub_info_tag = detail_soup.find("dt", string="ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸")
        if not pub_info_tag:
            return None, None, "âŒ 'ì¶œíŒì‚¬ / ì„í”„ë¦°íŠ¸' í•­ëª© ì—†ìŒ"

        dd_tag = pub_info_tag.find_next_sibling("dd")
        if dd_tag:
            full_text = dd_tag.get_text(strip=True)
            publisher_name_part = full_text.split("/")[0].strip()
            return full_text, normalize(publisher_name_part), None

        return None, None, "âŒ KPIPA ìƒì„¸ì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨"
    except Exception as e:
        return None, None, f"KPIPA ì˜ˆì™¸: {e}"


def get_mcst_address(pub_name):
    """ë¬¸ì²´ë¶€ ê²€ìƒ‰ (ë”ë¯¸ êµ¬í˜„)"""
    return "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì„¸ì¢…ëŒ€ë¡œ", [{"ì¶œíŒì‚¬": pub_name, "ì£¼ì†Œ": "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ì„¸ì¢…ëŒ€ë¡œ"}]


# =========================
# --- Streamlit ë©”ì¸ ---
# =========================
st.title("ğŸ“š ISBN â†’ KORMARC ë³€í™˜ê¸° (KPIPAÂ·IMÂ·ì •ê·œí™”Â·ë¬¸ì²´ë¶€ í†µí•©)")

if st.button("ğŸ”„ êµ¬ê¸€ì‹œíŠ¸ ìƒˆë¡œê³ ì¹¨"):
    st.cache_data.clear()
    st.success("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ!")

isbn_input = st.text_area("ISBNì„ '/'ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥:")

records = []
all_mcst_results = []

if isbn_input:
    isbn_list = [re.sub(r"[^\d]", "", s) for s in isbn_input.split("/") if s.strip()]
    publisher_data, region_data, imprint_data = load_publisher_db()

    for idx, isbn in enumerate(isbn_list, start=1):
        st.markdown(f"---\n### ğŸ“˜ {idx}. ISBN: `{isbn}`")
        debug_messages = []

        # 1) Aladin API
        result, error = search_aladin_by_isbn(isbn)
        if error:
            st.warning(error)
            continue
        publisher_api = result["publisher"]
        pubyear = result["pubyear"]

        # 2) KPIPA ISBN ê²€ìƒ‰
        publisher_full, publisher_norm, kpipa_error = get_publisher_name_from_isbn_kpipa(isbn)
        if publisher_norm:
            debug_messages.append(f"âœ… KPIPA ISBN ê²€ìƒ‰ ì„±ê³µ: {publisher_full}")
            location_raw, debug_db = search_publisher_location_with_alias(publisher_norm, publisher_data)
            debug_messages.extend(debug_db)
        else:
            debug_messages.append(kpipa_error)
            publisher_norm = normalize(publisher_api)
            debug_messages.append(f"â¡ï¸ KPIPA ì‹¤íŒ¨ â†’ Aladin API ì‚¬ìš©: {publisher_norm}")
            location_raw, debug_db = search_publisher_location_with_alias(publisher_norm, publisher_data)
            debug_messages.extend(debug_db)

        # 3) 1ì°¨ ì •ê·œí™” DB ê²€ìƒ‰ ì‹¤íŒ¨
        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            main_pub = find_main_publisher_from_imprints(publisher_norm, imprint_data)
            if main_pub:
                debug_messages.append(f"âœ… IM ì‹œíŠ¸ ë§¤ì¹­ ì„±ê³µ: {publisher_norm} â†’ {main_pub}")
                location_raw, debug_db = search_publisher_location_with_alias(normalize(main_pub), publisher_data)
                debug_messages.extend(debug_db)

        # 4) 2ì°¨ ì •ê·œí™” (ë¶€ë¶„ì¼ì¹˜)
        two_stage_matches = []
        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            matches, debug_stage2 = search_publisher_location_stage2_contains(publisher_norm, publisher_data)
            debug_messages.extend(debug_stage2)
            if matches:
                two_stage_matches = matches
                location_raw = matches[0][1]  # ì²« í›„ë³´ì§€ì—­

        # 5) ë¬¸ì²´ë¶€ ê²€ìƒ‰
        if location_raw == "ì¶œíŒì§€ ë¯¸ìƒ":
            addr, mcst_results = get_mcst_address(publisher_norm)
            if mcst_results:
                all_mcst_results.extend(mcst_results)
                location_raw = "ë¬¸ì²´ë¶€ í™•ì¸ í•„ìš”"

        # 6) ìµœì¢… ë°œí–‰êµ­ ì½”ë“œ
        location_display = location_raw if location_raw != "ì¶œíŒì§€ ë¯¸ìƒ" else "[ë°œí–‰ì§€ë¶ˆëª…]"
        country_code = get_country_code_by_region(location_raw, region_data)

        # KORMARC ì¶œë ¥
        field_008 = f"=008  \\\\$a{country_code}"
        field_245 = result["245"]
        field_260 = f"=260  \\\\$a{location_display} :$b{publisher_api},$c{pubyear}."

        st.code(field_008, language="text")
        st.code(field_245, language="text")
        st.code(field_260, language="text")

        records.append({"ISBN": isbn, "008": field_008, "245": field_245, "260": field_260})

        # ë””ë²„ê·¸ ë©”ì‹œì§€
        if debug_messages:
            st.markdown("### ğŸ› ï¸ ê²€ìƒ‰ ë””ë²„ê·¸")
            for msg in debug_messages:
                st.text(msg)

        # 2ì°¨ ì •ê·œí™” í›„ë³´ ì¶œë ¥
        if two_stage_matches:
            st.markdown("### ğŸ” 2ì°¨ ì •ê·œí™” í›„ë³´")
            df_stage2 = pd.DataFrame(two_stage_matches, columns=["ì¶œíŒì‚¬ëª…","ì§€ì—­"])
            st.dataframe(df_stage2, use_container_width=True)

    # ë¬¸ì²´ë¶€ ê²°ê³¼ëŠ” ë§ˆì§€ë§‰ì— ì¶œë ¥
    if all_mcst_results:
        st.markdown("---\n## ğŸ›ï¸ ë¬¸ì²´ë¶€ ê²€ìƒ‰ ê²°ê³¼")
        df_mcst = pd.DataFrame(all_mcst_results)
        st.dataframe(df_mcst, use_container_width=True)
