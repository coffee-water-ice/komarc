import streamlit as st
import requests
import pandas as pd
import openai
import xml.etree.ElementTree as ET
import re
import io
import xml.etree.ElementTree as ET
import re, datetime
from collections import Counter
from bs4 import BeautifulSoup
from openai import OpenAI
from requests.adapters import HTTPAdapter, Retry
from concurrent.futures import ThreadPoolExecutor

# â”€â”€ í•œ ë²ˆë§Œ ìƒì„±: êµ­ì¤‘APIìš© ì„¸ì…˜ & ì¬ì‹œë„ ì„¤ì •
_nlk_session = requests.Session()
_nlk_session.mount(
    "https://",
    HTTPAdapter(
        max_retries=Retry(
            total=1,                # ì¬ì‹œë„ 1íšŒ
            backoff_factor=0.5,     # 0.5ì´ˆ ê°„ê²©
            status_forcelist=[429,500,502,503,504]
        )
    )
)

# âœ… API í‚¤ (secrets.tomlì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
openai_key = st.secrets["api_keys"]["openai_key"]
aladin_key = st.secrets["api_keys"]["aladin_key"]
nlk_key = st.secrets["api_keys"]["nlk_key"]

gpt_client = OpenAI(api_key=openai_key)

# 008 ë³¸ë¬¸(40ì) ì¡°ë¦½ê¸°: ë‹¨í–‰ë³¸ ê¸°ì¤€, type_of_date ê¸°ë³¸ 's'
def build_008_kormarc_bk(
    date_entered,          # 00-05 YYMMDD
    date1,                 # 07-10 4ìë¦¬(ì˜ˆ: '2025' / '19uu' í—ˆìš©)
    country3,              # 15-17 3ìë¦¬ (ì§€ê¸ˆì€ 'HST' ê³ ì •)
    lang3,                 # 35-37 3ìë¦¬ (ì§€ê¸ˆì€ 'MRT' ê³ ì •)
    date2="",              # 11-14
    illus4="",             # 18-21 ìµœëŒ€ 4ì (ì˜ˆ: 'a', 'ad', 'ado'â€¦)
    has_index="0",         # 31 '0' ì—†ìŒ / '1' ìˆìŒ
    lit_form=" ",          # 33 ë¬¸í•™í˜•ì‹ (p ì‹œ, f ì†Œì„¤, e ìˆ˜í•„, i ì„œê°„ë¬¸í•™, m ê¸°í–‰/ì¼ê¸°/ìˆ˜ê¸°)
    bio=" ",               # 34 ì „ê¸° (a ìì„œì „, b ì „ê¸°, d ë¶€ë¶„ì  ì „ê¸°)
    type_of_date="s",      # 06 ê¸°ë³¸ 's'
    modified_record=" ",   # 28 ê¸°ë³¸ ê³µë°±
    cataloging_src=" ",    # 32 ê¸°ë³¸ ê³µë°±
):
    def pad(s, n, fill=" "):
        s = "" if s is None else str(s)
        return (s[:n] + fill * n)[:n]

    if len(date_entered) != 6 or not date_entered.isdigit():
        raise ValueError("date_enteredëŠ” YYMMDD 6ìë¦¬ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
    if len(date1) != 4:
        raise ValueError("date1ì€ 4ìë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: '2025', '19uu'")

    body = "".join([
        date_entered,               # 00-05
        pad(type_of_date,1),        # 06
        date1,                      # 07-10
        pad(date2,4),               # 11-14
        pad(country3,3),            # 15-17
        pad(illus4,4),              # 18-21
        " " * 4,                    # 22-25 (ì´ìš©ëŒ€ìƒ/ìë£Œí˜•íƒœ/ë‚´ìš©í˜•ì‹) ê³µë°±
        " " * 2,                    # 26-27 ê³µë°±
        pad(modified_record,1),     # 28 ê³µë°±
        " ",                        # 29 íšŒì˜ê°„í–‰ë¬¼ ê³µë°±
        " ",                        # 30 ê¸°ë…ë…¼ë¬¸ì§‘ ê³µë°±
        has_index if has_index in ("0","1") else "0", # 31 ìƒ‰ì¸
        pad(cataloging_src,1),      # 32 ê³µë°±
        pad(lit_form,1),            # 33 ë¬¸í•™í˜•ì‹
        pad(bio,1),                 # 34 ì „ê¸°
        pad(lang3,3),               # 35-37 ì–¸ì–´
        " " * 2                     # 38-39 ê³µë°±
    ])
    if len(body) != 40:
        raise AssertionError(f"008 length != 40: {len(body)}")
    return body

# ì•Œë¼ë”˜ pubDate ë¬¸ìì—´ì—ì„œ ì—°ë„ë§Œ ì¶”ì¶œ
def extract_year_from_aladin_pubdate(pubdate_str: str) -> str:
    m = re.search(r"(19|20)\d{2}", pubdate_str or "")
    return m.group(0) if m else "19uu"

# ì‚½í™” ê°ì§€: a(ì‚½í™”/ì¼ëŸ¬ìŠ¤íŠ¸), d(ë„í‘œ/ê·¸ë˜í”„), o(ì‚¬ì§„/í™”ë³´)
def detect_illus4(text: str) -> str:
    keys = []
    if re.search(r"ì‚½í™”|ì¼ëŸ¬ìŠ¤íŠ¸|ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜|ê·¸ë¦¼|illustration", text, re.I): keys.append("a")
    if re.search(r"ë„í‘œ|ì°¨íŠ¸|ê·¸ë˜í”„", text, re.I):                              keys.append("d")
    if re.search(r"ì‚¬ì§„|í¬í† |í™”ë³´|photo", text, re.I):                           keys.append("o")
    out=[]; [out.append(k) for k in keys if k not in out]
    return "".join(out)[:4]

# ìƒ‰ì¸ ê°ì§€: 'ìƒ‰ì¸', 'ì°¾ì•„ë³´ê¸°', 'index'
def detect_index(text: str) -> str:
    return "1" if re.search(r"ìƒ‰ì¸|ì°¾ì•„ë³´ê¸°|index", text, re.I) else "0"

# ë¬¸í•™í˜•ì‹ ê°ì§€: p ì‹œ / f ì†Œì„¤ / e ìˆ˜í•„ / i ì„œê°„ë¬¸í•™ / m ê¸°í–‰Â·ì¼ê¸°Â·ìˆ˜ê¸°
def detect_lit_form(title: str, category: str, kdc: str = None) -> str:
    blob = f"{title} {category}"
    if re.search(r"ì„œê°„ì§‘|í¸ì§€|ì„œê°„ë¬¸|letters?", blob, re.I): return "i"
    if re.search(r"ê¸°í–‰|ì—¬í–‰ê¸°|ì¼ê¸°|ìˆ˜ê¸°|diary|travel", blob, re.I): return "m"
    if re.search(r"ì‹œì§‘|poem|poetry", blob, re.I): return "p"
    if re.search(r"ì†Œì„¤|novel|fiction", blob, re.I): return "f"
    if re.search(r"ì—ì„¸ì´|ìˆ˜í•„|essay", blob, re.I): return "e"
    return " "  # ë¹„ë¬¸í•™ ë˜ëŠ” ë¯¸ë¶„ë¥˜

# ì „ê¸° ê°ì§€: a ìì„œì „ / b ì „ê¸°Â·í‰ì „(íƒ€ì¸) / d ë¶€ë¶„ì  ì „ê¸°(íšŒê³ /ì¼ê¸° ë“± ì•”ì‹œ)
def detect_bio(text: str) -> str:
    t = text or ""
    if re.search(r"ìì„œì „|autobiograph", t, re.I): return "a"
    if re.search(r"ì „ê¸°|í‰ì „|biograph", t, re.I):  return "b"
    if re.search(r"ì „ê¸°ì |íšŒê³ |íšŒìƒ", t):         return "d"
    return " "

# ISBN í•˜ë‚˜ë¡œ 008 ìƒì„± (ìš”ì²­ì‚¬í•­ ë°˜ì˜: country/lang ì„ì‹œ ê³ ì •ê°’)
COUNTRY_FIXED = "HST"  # TODO: 300 ëª¨ë“ˆ ì™„ì„± í›„ override
LANG_FIXED    = "MRT"  # TODO: 041 ëª¨ë“ˆ ì™„ì„± í›„ override

def build_008_from_isbn(
    isbn: str,
    *,
    aladin_pubdate: str = "",
    aladin_title: str = "",
    aladin_category: str = "",
    aladin_desc: str = "",
    override_country3: str = None,  # ë‚˜ì¤‘ì— 300ì—ì„œ ì±„ì›Œ ë„£ê¸°
    override_lang3: str = None,     # ë‚˜ì¤‘ì— 041ì—ì„œ ì±„ì›Œ ë„£ê¸°
):
    today  = datetime.datetime.now().strftime("%y%m%d")  # YYMMDD
    date1  = extract_year_from_aladin_pubdate(aladin_pubdate)
    country3 = (override_country3 or COUNTRY_FIXED)
    lang3    = (override_lang3    or LANG_FIXED)

    bigtext   = " ".join([aladin_title or "", aladin_desc or ""])
    illus4    = detect_illus4(bigtext)
    has_index = detect_index(bigtext)
    lit_form  = detect_lit_form(aladin_title or "", aladin_category or "")
    bio       = detect_bio(bigtext)

    return build_008_kormarc_bk(
        date_entered=today,
        date1=date1,
        country3=country3,
        lang3=lang3,
        illus4=illus4,
        has_index=has_index,
        lit_form=lit_form,
        bio=bio
    )
# ========= 008 ìƒì„± ë¸”ë¡: ë¶™ì—¬ë„£ê¸° ë =========

# ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ (konlpy ì—†ì´)
def extract_keywords_from_text(text, top_n=7):
    words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
    filtered = [w for w in words if len(w) > 1]
    freq = Counter(filtered)
    return [kw for kw, _ in freq.most_common(top_n)]

def clean_keywords(words):
    stopwords = {"ì•„ì£¼", "ê°€ì§€", "í•„ìš”í•œ", "ë“±", "ìœ„í•´", "ê²ƒ", "ìˆ˜", "ë”", "ì´ëŸ°", "ìˆë‹¤", "ëœë‹¤", "í•œë‹¤"}
    return [w for w in words if w not in stopwords and len(w) > 1]

# ğŸ“š ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_category_keywords(category_str):
    keywords = set()
    lines = category_str.strip().splitlines()
    for line in lines:
        parts = [x.strip() for x in line.split('>') if x.strip()]
        if parts:
            keywords.add(parts[-1])
    return list(keywords)

# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ
# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ (OpenAI 1.6.0+ ë°©ì‹ìœ¼ë¡œ ë¦¬íŒ©í† ë§)
def recommend_kdc(title, author, api_key):
    try:
        # ğŸ”‘ ë¹„ë°€ì˜ ì—´ì‡ ë¡œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ê¹¨ì›ë‹ˆë‹¤
        client = OpenAI(api_key=api_key)

        # ğŸ“œ ì£¼ë¬¸ë¬¸ì„ ì¤€ë¹„í•˜ê³ 
        prompt = (
            f"ë„ì„œ ì œëª©: {title}\n"
            f"ì €ì: {author}\n"
            "ì´ ì±…ì˜ ì£¼ì œë¥¼ ê³ ë ¤í•˜ì—¬ í•œêµ­ì‹­ì§„ë¶„ë¥˜(KDC) ë²ˆí˜¸ í•˜ë‚˜ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\n"
            "ì •í™•í•œ ìˆ«ìë§Œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì‘ë‹µí•´ ì£¼ì„¸ìš”:\n"
            "KDC: 813.7"
        )

        # ğŸ§  GPTì˜ ì§€í˜œë¥¼ ì†Œí™˜
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )

        # âœ‚ï¸ â€œKDC:â€ ë’¤ì˜ ìˆ«ìë§Œ êº¼ë‚´ì„œ ëŒë ¤ë“œë¦½ë‹ˆë‹¤
        for line in response.choices[0].message.content.splitlines():
            if "KDC:" in line:
                return line.split("KDC:")[1].strip()

    except Exception as e:
        st.warning(f"ğŸ§  GPT ì˜¤ë¥˜: {e}")

    # ğŸ›¡ï¸ ë§Œì•½ ì‹¤íŒ¨í•˜ë©´ ë””í´íŠ¸ â€œ000â€
    return "000"


# ğŸ“¡ ë¶€ê°€ê¸°í˜¸ ì¶”ì¶œ (êµ­ë¦½ì¤‘ì•™ë„ì„œê´€)
@st.cache_data(ttl=24*3600)
def fetch_additional_code_from_nlk(isbn: str) -> str:
    url = (
        f"https://www.nl.go.kr/seoji/SearchApi.do?"
        f"cert_key={nlk_key}&result_style=xml"
        f"&page_no=1&page_size=1&isbn={isbn}"
    )
    try:
        res = _nlk_session.get(url, timeout=3)  # 3ì´ˆë§Œ ê¸°ë‹¤ë¦¬ê³ 
        res.raise_for_status()
        root = ET.fromstring(res.text)
        doc  = root.find('.//docs/e')
        return (doc.findtext('EA_ADD_CODE') or "").strip() if doc is not None else ""
    except Exception:
        st.warning("âš ï¸ êµ­ì¤‘API ì§€ì—°, ë¶€ê°€ê¸°í˜¸ëŠ” ìƒëµí•©ë‹ˆë‹¤.")
        return ""


# ğŸ”¤ ì–¸ì–´ ê°ì§€ ë° 041, 546 ìƒì„±
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}

def detect_language(text):
    text = re.sub(r'[\s\W_]+', '', text)
    if not text:
        return 'und'
    first_char = text[0]
    if '\uac00' <= first_char <= '\ud7a3':
        return 'kor'
    elif '\u3040' <= first_char <= '\u30ff':
        return 'jpn'
    elif '\u4e00' <= first_char <= '\u9fff':
        return 'chi'
    elif '\u0400' <= first_char <= '\u04FF':
        return 'rus'
    elif 'a' <= first_char.lower() <= 'z':
        return 'eng'
    else:
        return 'und'

def generate_546_from_041_kormarc(marc_041: str) -> str:
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"):
            a_codes.append(part[2:])
        elif part.startswith("$h"):
            h_code = part[2:]
    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{a_lang}ë¡œ ì”€, ì›ì €ëŠ” {h_lang}ì„"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"
    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"

def crawl_aladin_original_and_price(isbn13):
    url = f"https://www.aladin.co.kr/shop/wproduct.aspx?ISBN={isbn13}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        original = soup.select_one("div.info_original")
        price = soup.select_one("span.price2")
        return {
            "original_title": original.text.strip() if original else "",
            "price": price.text.strip().replace("ì •ê°€ : ", "").replace("ì›", "").replace(",", "").strip() if price else ""
        }
    except:
        return {}

# ğŸ“„ 653 í•„ë“œ í‚¤ì›Œë“œ ìƒì„±
# â‘¡ ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„° í˜¸ì¶œ í•¨ìˆ˜
def fetch_aladin_metadata(isbn):
    url = (
        "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        f"?ttbkey={aladin_key}"
        "&ItemIdType=ISBN"
        f"&ItemId={isbn}"
        "&output=js"
        "&Version=20131101"
        "&OptResult=Toc" 
    )
    data = requests.get(url).json()
    item = data["item"][0]
    return {
        "category": item.get("categoryName", ""),
        "title": item.get("title", ""),
        "description": item.get("description", ""),
        "toc": item.get("toc", ""),
    }


# â‘¢ GPT-4 ê¸°ë°˜ 653 ìƒì„± í•¨ìˆ˜
def generate_653_with_gpt(category, title, description, toc, max_keywords=7):
    parts = [p.strip() for p in category.split(">") if p.strip()]
    cat_kw = parts[-1] if parts else ""
    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì±…ì˜ ë¶„ë¥˜, ì œëª©, ì„¤ëª…, ëª©ì°¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
            "MARC 653 í•„ë“œìš© ì£¼ì œì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."
        )
    }
    user_msg = {
        "role": "user",
        "content": (
            f"ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ìµœëŒ€ {max_keywords}ê°œì˜ MARC 653 ì£¼ì œì–´ë¥¼ í•œ ì¤„ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”:\n\n"
            f"- ë¶„ë¥˜: \"{cat_kw}\"\n"
            f"- ì œëª©: \"{title}\"\n"
            f"- ì„¤ëª…: \"{description}\"\n"
            f"- ëª©ì°¨: \"{toc}\"\n\n"
             "â€» â€œì œëª©â€ì— ì‚¬ìš©ëœ ë‹¨ì–´ëŠ” ì œì™¸í•˜ê³ , ìˆœìˆ˜í•˜ê²Œ ë¶„ë¥˜Â·ì„¤ëª…Â·ëª©ì°¨ì—ì„œ ì¶”ì¶œëœ ì£¼ì œì–´ë§Œ ë½‘ì•„ì£¼ì„¸ìš”.\n"
            "ì¶œë ¥ í˜•ì‹: $aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 â€¦"
        )
    }
    try:
        resp = gpt_client.chat.completions.create(
            model="gpt-4",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=150,
        )
        # 1) ì›ë³¸ ì‘ë‹µì„ rawì— ë‹´ìŠµë‹ˆë‹¤
        raw = resp.choices[0].message.content.strip()

        # 2) $a â€¦ ë‹¤ìŒ $a ë˜ëŠ” ëê¹Œì§€ ìº¡ì²˜ (non-greedy)
        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]

        # 3) ê° í‚¤ì›Œë“œ ë‚´ë¶€ ê³µë°± ì œê±°
        kws = [kw.replace(" ", "") for kw in kws]

        # 4) ë‹¤ì‹œ "$aí‚¤ì›Œë“œ" í˜•íƒœë¡œ ì¡°ë¦½
        return "".join(f"$a{kw}" for kw in kws)

    except Exception as e:
        st.warning(f"âš ï¸ 653 ì£¼ì œì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    


# ğŸ“š MARC ìƒì„±
@st.cache_data(show_spinner=False)
def fetch_book_data_from_aladin(isbn, reg_mark="", reg_no="", copy_symbol=""):
    import re
    from concurrent.futures import ThreadPoolExecutor

    # 1) ì•Œë¼ë”˜ + (ì˜µì…˜) êµ­ì¤‘ ë¶€ê°€ê¸°í˜¸ ë™ì‹œ ìš”ì²­
    url = (
        f"https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?"
        f"ttbkey={aladin_key}&itemIdType=ISBN&ItemId={isbn}"
        f"&output=js&Version=20131101"
    )
    with ThreadPoolExecutor(max_workers=2) as ex:
        future_aladin = ex.submit(lambda: requests.get(url, verify=False, timeout=5))
        future_nlk    = ex.submit(fetch_additional_code_from_nlk, isbn)

        try:
            resp = future_aladin.result()
            resp.raise_for_status()
            data = resp.json().get("item", [{}])[0]
        except Exception as e:
            st.error(f"ğŸš¨ ì•Œë¼ë”˜API ì˜¤ë¥˜: {e}")
            return ""

        add_code = future_nlk.result()  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´

    # 2) ë©”íƒ€ë°ì´í„° (ì•Œë¼ë”˜)
    title       = data.get("title",       "ì œëª©ì—†ìŒ")
    author      = data.get("author",      "ì €ìë¯¸ìƒ")
    publisher   = data.get("publisher",   "ì¶œíŒì‚¬ë¯¸ìƒ")
    pubdate     = data.get("pubDate",     "2025")  # 'YYYY' ë˜ëŠ” 'YYYY-MM-DD'
    category    = data.get("categoryName", "")
    description = data.get("description", "")
    toc         = data.get("subInfo", {}).get("toc", "")
    price       = str(data.get("priceStandard", ""))  # 020/950 ìš©

    # 3) =008 ìƒì„± (ISBNë§Œìœ¼ë¡œ ìë™, country/langì€ ì„ì‹œ ê³ ì •ê°’ â†’ ì¶”í›„ override)
    tag_008 = "=008  " + build_008_from_isbn(
        isbn,
        aladin_pubdate=pubdate,
        aladin_title=title,
        aladin_category=category,
        aladin_desc=description,
        # override_country3="ulk",  # 300 ëª¨ë“ˆ ì™„ì„± ì‹œ ì‚¬ìš©
        # override_lang3="kor",     # 041 ëª¨ë“ˆ ì™„ì„± ì‹œ ì‚¬ìš©
    )

    # 4) 041/546 (ê°„ì´ ê°ì§€: ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    lang_a  = detect_language(title)
    lang_h  = detect_language(data.get("title", ""))
    tag_041 = f"=041  \\$a{lang_a}" + (f"$h{lang_h}" if lang_h != "und" else "")
    tag_546 = f"=546  \\$a{generate_546_from_041_kormarc(tag_041)}"

    # 5) 020 (ë¶€ê°€ê¸°í˜¸ ìˆìœ¼ë©´ $g ì¶”ê°€)
    tag_020 = f"=020  \\$a{isbn}:$c{price}"
    if add_code:
        tag_020 += f"$g{add_code}"

    # 6) 653/KDC â€” âœ… ì—¬ê¸°ì„œë§Œ ìƒì„± (GPTAPI ìµœì‹  í•¨ìˆ˜ë¡œ í†µì¼)
    kdc     = recommend_kdc(title, author, api_key=openai_key)
    gpt_653 = generate_653_with_gpt(category, title, description, toc, max_keywords=7)
    tag_653 = f"=653  \\{gpt_653.replace(' ', '')}" if gpt_653 else ""

    # 7) ê¸°ë³¸ MARC ë¼ì¸
    marc_lines = [
        tag_008,
        "=007  ta",
        f"=245  00$a{title} /$c{author}",
        f"=260  \\$aì„œìš¸ :$b{publisher},$c{pubdate[:4]}.",
    ]

    # 8) 490Â·830 (ì´ì„œ)
    series = data.get("seriesInfo", {})
    name = (series.get("seriesName") or "").strip()
    vol  = (series.get("volume")    or "").strip()
    if name:
        marc_lines.append(f"=490  \\$a{name};$v{vol}")
        marc_lines.append(f"=830  \\$a{name};$v{vol}")

    # 9) ê¸°íƒ€ í•„ë“œ
    marc_lines.append(tag_020)
    marc_lines.append(tag_041)
    marc_lines.append(tag_546)
    if kdc and kdc != "000":
        marc_lines.append(f"=056  \\$a{kdc}$26")
    if tag_653:
        marc_lines.append(tag_653)
    marc_lines.append(f"=950  0\\$b{price}")

    # 10) 049: ì†Œì¥ê¸°í˜¸(ì…ë ¥ëœ ê²½ìš°ë§Œ)
    if reg_mark or reg_no or copy_symbol:
        line = f"=049  0\\$I{reg_mark}{reg_no}"
        if copy_symbol:
            line += f"$f{copy_symbol}"
        marc_lines.append(line)

    # 11) ë²ˆí˜¸ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ í›„ ì¶œë ¥
    marc_lines.sort(key=lambda L: int(re.match(r"=(\d+)", L).group(1)))
    return "\n".join(marc_lines)




# ğŸ›ï¸ Streamlit UI
st.title("ğŸ“š ISBN to MARC ë³€í™˜ê¸° (í†µí•©ë²„ì „)")

isbn_list = []
single_isbn = st.text_input("ğŸ”¹ ë‹¨ì¼ ISBN ì…ë ¥", placeholder="ì˜ˆ: 9788936434267")
if single_isbn.strip():
    isbn_list = [[single_isbn.strip(), "", "", ""]]

uploaded_file = st.file_uploader("ğŸ“ CSV ì—…ë¡œë“œ (ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)", type="csv")
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    if {'ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸'}.issubset(df.columns):
        isbn_list = df[['ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸']].dropna(subset=['ISBN']).values.tolist()
    else:
        st.error("âŒ í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸")

if isbn_list:
    st.subheader("ğŸ“„ MARC ì¶œë ¥")
    marc_results = []
    for row in isbn_list:
        isbn, reg_mark, reg_no, copy_symbol = row
        marc = fetch_book_data_from_aladin(isbn, reg_mark, reg_no, copy_symbol)
        if marc:
            st.code(marc, language="text")
            marc_results.append(marc)

    full_text = "\n\n".join(marc_results)
    st.download_button("ğŸ“¦ ëª¨ë“  MARC ë‹¤ìš´ë¡œë“œ", data=full_text, file_name="marc_output.txt", mime="text/plain")

# ğŸ“„ í…œí”Œë¦¿ ì˜ˆì‹œ ë‹¤ìš´ë¡œë“œ
example_csv = "ISBN,ë“±ë¡ê¸°í˜¸,ë“±ë¡ë²ˆí˜¸,ë³„ì¹˜ê¸°í˜¸\n9791173473968,JUT,12345,TCH\n"
buffer = io.BytesIO()
buffer.write(example_csv.encode("utf-8-sig"))
buffer.seek(0)
st.download_button("ğŸ“„ ì„œì‹ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", data=buffer, file_name="isbn_template.csv", mime="text/csv")

# â¬‡ï¸ í•˜ë‹¨ ë§ˆí¬
st.markdown("""
<div style='text-align: center; font-size: 14px; color: gray;'>
ğŸ“š <strong>ë„ì„œ DB ì œê³µ</strong> : <a href='https://www.aladin.co.kr' target='_blank'>ì•Œë¼ë”˜ ì¸í„°ë„·ì„œì (www.aladin.co.kr)</a>
</div>
""", unsafe_allow_html=True)
